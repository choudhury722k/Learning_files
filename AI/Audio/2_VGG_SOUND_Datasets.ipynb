{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"4_VGG_SOUND_Datasets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPZivCg2L4bJtfA/tvE7SV"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **VGG-SOUND Datasets**"],"metadata":{"id":"cqkKohsbcKOa"}},{"cell_type":"markdown","source":["VGG-SOUND Datasets is Developed by VGG, Department of Engineering Science, University of Oxford, UK Audio VGGSound  Dataset has set a benchmark for audio recognition with visuals. It contains more than  210 k videos with visual and audio. The dataset contains over 310 categorie and 550 hours of video. It is available to download for commercial/research purposes. The VGGSound dataset consists of each video and audio segment being 10 seconds long."],"metadata":{"id":"92Er3o77pA80"}},{"cell_type":"markdown","source":["## **Download Dataset**"],"metadata":{"id":"8BpLZnPhpIU1"}},{"cell_type":"markdown","source":["The dataset is available as a CSV file which contains the ‘youtube URL’ of the audio and video,  click [here](https://www.robots.ox.ac.uk/~vgg/data/vggsound/vggsound.csv) to download locally on your computer.\n","\n","Download Size: 8 MB\n","\n","Now, Let’s do some coding to know about the dataset and their category ratio of training and testing data. "],"metadata":{"id":"dUADMRWcpMXQ"}},{"cell_type":"code","execution_count":null,"source":["!wget https://www.robots.ox.ac.uk/~vgg/data/vggsound/vggsound.csv"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVjsvUtYbvED","executionInfo":{"status":"ok","timestamp":1623223290585,"user_tz":-330,"elapsed":1767,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"e408fc8b-e6ea-490b-8471-55db7ba851e2"}},{"cell_type":"markdown","source":["## **Visualization of Data**"],"metadata":{"id":"yLwBasr1pafV"}},{"cell_type":"markdown","source":["Import all these library pandas, matplotlib and seaborn and load the dataset using the code."],"metadata":{"id":"jDUBufmqpdaZ"}},{"cell_type":"code","execution_count":null,"source":["\n","!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image pillow librosa torchaudio apache_beam --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","df = pd.read_csv(\"vggsound.csv\")"],"outputs":[],"metadata":{"id":"L1PO4lGupe9s","executionInfo":{"status":"ok","timestamp":1623223305551,"user_tz":-330,"elapsed":1072,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["The dataset contains more than 310 classes which is broadly categorised as :\n","\n","1. People\n","2. Animals\n","3. Music\n","4. Sports\n","5. Nature\n","6. Vehicle\n","7. Tools\n","8. Instruments\n","9. Mammals\n","10. Others"],"metadata":{"id":"3RcujeNQpiG-"}},{"cell_type":"markdown","source":["If you plot a pie chart of it then it looks messy like this."],"metadata":{"id":"nweJA4mYptPA"}},{"cell_type":"code","execution_count":null,"source":["df.groupby('people marching').size().plot(kind='pie', autopct='%.2f')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"Wdvzp-M_pvEk","executionInfo":{"status":"ok","timestamp":1623223371949,"user_tz":-330,"elapsed":4544,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"d8ee0be0-0f2c-4eda-cc1c-f21aec2d21f6"}},{"cell_type":"markdown","source":["Now visualize the train and test data."],"metadata":{"id":"oMQUJF6wpwub"}},{"cell_type":"code","execution_count":null,"source":["sns.catplot(x=\"1\", y=\"test\", data=df)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"P322d2Wrpyhz","executionInfo":{"status":"ok","timestamp":1623223384359,"user_tz":-330,"elapsed":2937,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"02ea8225-80f3-4e62-b600-c231e1ffc1a6"}},{"cell_type":"markdown","source":["Plot the ratio of training and Test set:"],"metadata":{"id":"x27ce7fWp0Ei"}},{"cell_type":"code","execution_count":null,"source":["df.groupby('people marching').size().plot(kind='pie', autopct='%.2f')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"6GxqPkqSp1lW","executionInfo":{"status":"ok","timestamp":1623223398846,"user_tz":-330,"elapsed":4564,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"c8bc605d-1fc8-4e84-8d2e-c60123d377c6"}},{"cell_type":"markdown","source":["The dataset contains 92.25 per cent of training data and 7.75 per cent of test data as shown in the pie chart."],"metadata":{"id":"Oe-b-XKyp34E"}},{"cell_type":"markdown","source":["## **Implementation of VGG-Sound**"],"metadata":{"id":"SnML-Jukp5Ww"}},{"cell_type":"markdown","source":["**Using PyTorch:**"],"metadata":{"id":"gEeOyvNKp7ca"}},{"cell_type":"code","execution_count":null,"source":["import os\n","import cv2\n","import json\n","import torch\n","import csv\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import time\n","from PIL import Image\n","import glob\n","import sys\n","from scipy import signal\n","import random\n","import soundfile as sf\n","class GetAudioVideoDataset(Dataset):\n","    def __init__(self, args, mode='train', transforms=None):\n","        data2path = {}\n","        classes = []\n","        classes_ = []\n","        data = []\n","        data2class = {}\n","        with open(args.csv_path + 'stat.csv') as f:\n","            csv_reader = csv.reader(f)\n","            for row in csv_reader:\n","                classes.append(row[0])\n","        with open(args.csv_path  + args.test) as f:\n","            csv_reader = csv.reader(f)\n","            for item in csv_reader:\n","                if item[1] in classes and os.path.exists(args.data_path + item[0][:-3] + 'wav'):\n","                    data.append(item[0])\n","                    data2class[item[0]] = item[1]\n","        self.audio_path = args.data_path \n","        self.mode = mode\n","        self.transforms = transforms\n","        self.classes = sorted(classes)\n","        self.data2class = data2class\n","        # initialize audio transform\n","        self._init_atransform()\n","        #  Retrieve list of audio and video files\n","        self.video_files = []\n","        for item in data:\n","            self.video_files.append(item)\n","        print('# of audio files = %d ' % len(self.video_files))\n","        print('# of classes = %d' % len(self.classes))\n","    def _init_atransform(self):\n","        self.aid_transform = transforms.Compose([transforms.ToTensor()])\n","    def __len__(self):\n","        return len(self.video_files)  \n","    def __getitem__(self, idx):\n","        wav_file = self.video_files[idx]\n","        # Audio\n","        samples, samplerate = sf.read(self.audio_path + wav_file[:-3]+'wav')\n","        # repeat in case audio is too short\n","        resamples = np.tile(samples,10)[:160000]\n","        resamples[resamples > 1.] = 1.\n","        resamples[resamples < -1.] = -1.\n","        frequencies, times, spectrogram = signal.spectrogram(resamples, samplerate, nperseg=512,noverlap=353)\n","        spectrogram = np.log(spectrogram+ 1e-7)\n","        mean = np.mean(spectrogram)\n","        std = np.std(spectrogram)\n","        spectrogram = np.divide(spectrogram-mean,std+1e-9)\n","        return spectrogram, resamples,self.classes.index(self.data2class[wav_file]),wav_file"],"outputs":[],"metadata":{"id":"emp-Omf8p-tN","executionInfo":{"status":"ok","timestamp":1623223447867,"user_tz":-330,"elapsed":3867,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [VGG Sound Datasets](https://analyticsindiamag.com/guide-to-vgg-sound-datasets-for-visual-audio-recognition/)\n","\n","> * [Voxceleb Datasets](https://analyticsindiamag.com/guide-to-voxceleb-datasets-for-visual-audio-of-human-speech/)\n","\n","> * [FreeSound Datasets](https://analyticsindiamag.com/datasets-freesound-pytorch-research/)\n","\n","> * [LibriSpeech Datasets](https://analyticsindiamag.com/librispeech-datasets/)\n","\n","> * [Simple Transformers](https://analyticsindiamag.com/speech-classification-in-3-minutes/)"],"metadata":{"id":"agPHjISycAxz"}}]}