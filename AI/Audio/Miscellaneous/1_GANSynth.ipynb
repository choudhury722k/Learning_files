{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_GANSynth.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM1XpSgsygqtq00FanfZ75g"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **GANSynth**"],"metadata":{"id":"UpgDLgLu7B8G"}},{"cell_type":"markdown","source":["GANSynth is a state-of-the-art method for synthesizing high-fidelity and locally coherent audio using Generative Adversarial Networks (GANs). Hence the name GANSynth (GAN used for audio Synthesis)."],"metadata":{"id":"FVWljBmI78FU"}},{"cell_type":"markdown","source":["Autoregressive models like WaveNets generate audio sequentially. On the contrary, GANSynth creates the whole sequence in parallel, synthesizing audio much faster on GPU runtime than real-time synthesis. It generates the entire audio clip from a single latent vector, allowing for easier release of global features like pitch and timbre (tone quality). It uses progressive GAN architecture. It eliminates the drawback of traditional GANs which struggle to synthesize locally coherent audio waveforms though they use global latent conditioning and efficient parallel sampling.\n","\n","Are you interested in understanding the detailed workings of GANSynth? Refer to this page before proceeding!"],"metadata":{"id":"rzQJl3-T8Ety"}},{"cell_type":"markdown","source":["https://analyticsindiamag.com/hands-on-guide-to-gansynth-an-adversarial-neural-audio-synthesis-technique/"],"metadata":{"id":"HfWhDx_D7A8N"}},{"cell_type":"markdown","source":["## **Practical Implementation of GANSynth**"],"metadata":{"id":"9_LFVhc_-Vvf"}},{"cell_type":"markdown","source":["Here’s a demonstration of how GANSynth learns to produce musical notes of individual instruments as contained in the NSynth dataset (a large-sized qualitative dataset having annotated notes). The GAN learns to use its latent space for representing various instrument timbres. It synthesizes audio from MIDI files and interpolates between different instruments."],"metadata":{"id":"bMOmAfGTGtLJ"}},{"cell_type":"markdown","source":["Step-wise explanation of the code is as follows:"],"metadata":{"id":"t1QmyVMFGwoP"}},{"cell_type":"markdown","source":["Install Magenta (an open-source Python library, powered by Tensorflow)"],"metadata":{"id":"2yNk6iQ0Gyz8"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"BVapPeWk6yTi"}},{"cell_type":"code","execution_count":null,"source":["\n","# Install Magenta\n","print('Copying data from GCS...')\n","!rm -r /content/gansynth &>/dev/null\n","!mkdir /content/gansynth\n","!mkdir /content/gansynth/midi\n","!mkdir /content/gansynth/samples\n","\n","# Get default MIDI (Bach Prelude)\n","#-o option provided with the curl command saves the downloaded file on your local machine with the name specified as the parameter.\n","!curl -o /content/gansynth/midi/bach.mid http://www.jsbach.net/midi/cs1-1pre.mid\n","MIDI_SONG_DEFAULT = '/content/gansynth/midi/bach.mid'\n","!curl -o /content/gansynth/midi/riff-default.mid http://storage.googleapis.com/magentadata/papers/gansynth/midi/arp.mid\n","MIDI_RIFF_DEFAULT = '/content/gansynth/midi/riff-default.mid'\n","\n","!pip install -q -U magenta"],"outputs":[],"metadata":{"id":"t6WUlphBrGeC"}},{"cell_type":"markdown","source":["Import required libraries and classes "],"metadata":{"id":"gbm8mc1BHJwP"}},{"cell_type":"code","execution_count":null,"source":["\n","import os\n","import librosa\n","from magenta.models.nsynth.utils import load_audio\n","from magenta.models.gansynth.lib import flags as lib_flags\n","from magenta.models.gansynth.lib import generate_util as gu\n","from magenta.models.gansynth.lib import model as lib_model\n","from magenta.models.gansynth.lib import util\n","import matplotlib.pyplot as plt\n","import note_seq\n","from note_seq.notebook_utils import colab_play as play\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","\n","tf.disable_v2_behavior()"],"outputs":[],"metadata":{"id":"hhHJTayOHHZV"}},{"cell_type":"markdown","source":["Define a function for uploading .wav file"],"metadata":{"id":"ZvZ6pSR0HV-_"}},{"cell_type":"code","execution_count":null,"source":["# File IO\n","#download = files.download\n","\n","def upload():\n","  '''Upload a .wav file.'''\n","  filemap = files.upload()\n","  file_list = []\n","  for key, value in filemap.iteritems():\n","    fname = os.path.join('/content/gansynth/midi', key)\n","    with open(fname, 'w') as f:\n","      f.write(value)\n","      print('Writing {}'.format(fname))\n","    file_list.append(fname)\n","  return file_list"],"outputs":[],"metadata":{"id":"1xohLM7jHTst"}},{"cell_type":"markdown","source":["Define global variables"],"metadata":{"id":"_aWtE8tzHkGM"}},{"cell_type":"code","execution_count":null,"source":["# GLOBALS\n","CKPT_DIR = 'gs://magentadata/models/gansynth/acoustic_only'\n","output_dir = '/content/gansynth/samples'\n","BATCH_SIZE = 16\n","SR = 16000"],"outputs":[],"metadata":{"id":"OC4qEX4wHnzd"}},{"cell_type":"markdown","source":["Create an output directory if it does not exist"],"metadata":{"id":"rHUaEHi_Hpng"}},{"cell_type":"code","execution_count":null,"source":["# Make an output directory if it doesn't exist\n","OUTPUT_DIR = util.expand_path(output_dir)\n","if not tf.gfile.Exists(OUTPUT_DIR):\n","  tf.gfile.MakeDirs(OUTPUT_DIR)"],"outputs":[],"metadata":{"id":"QLkRuc6YHtWp"}},{"cell_type":"markdown","source":["Load the model"],"metadata":{"id":"O-Q162W0H11f"}},{"cell_type":"code","execution_count":null,"source":["# Load the model\n","tf.reset_default_graph()\n","flags = lib_flags.Flags({\n","    'batch_size_schedule': [BATCH_SIZE],\n","    'tfds_data_dir': \"gs://tfds-data/datasets\",\n","})\n","model = lib_model.Model.load_from_path(CKPT_DIR, flags)"],"outputs":[],"metadata":{"id":"bVNhgEUkH4wc"}},{"cell_type":"markdown","source":["Define a function for loading MIDI file as a notesequence"],"metadata":{"id":"blWNoKQ1IPdC"}},{"cell_type":"code","execution_count":null,"source":["\n","# Helper functions\n","def load_midi(midi_path, min_pitch=36, max_pitch=84):\n","  \"\"\"Load midi as a notesequence.\"\"\"\n","  midi_path = util.expand_path(midi_path)\n","  ns = note_seq.midi_file_to_sequence_proto(midi_path)\n","  pitches = np.array([n.pitch for n in ns.notes])\n","  velocities = np.array([n.velocity for n in ns.notes])\n","  start_times = np.array([n.start_time for n in ns.notes])\n","  end_times = np.array([n.end_time for n in ns.notes])\n","  valid = np.logical_and(pitches >= min_pitch, pitches <= max_pitch)\n","  notes = {'pitches': pitches[valid],\n","           'velocities': velocities[valid],\n","           'start_times': start_times[valid],\n","           'end_times': end_times[valid]}\n","  return ns, notes"],"outputs":[],"metadata":{"id":"SJXhVDCfIJTB"}},{"cell_type":"markdown","source":["Create an attack, sustain and release amplitude envelope (these are the stages of envelope generator)\n","\n","‘Attack’ is part of the envelope which represents time taken by the amplitude to reach its peak.’Sustain’ is the duration for which sound is held before it fades out.’Release’ is the final reduction in amplitude over time."],"metadata":{"id":"T63z8D9oIS3z"}},{"cell_type":"code","execution_count":null,"source":["def get_envelope(t_note_length, t_attack=0.010, t_release=0.3, sr=16000):\n","  \"\"\"Create an attack sustain release amplitude envelope.\"\"\"\n","  t_note_length = min(t_note_length, 3.0)\n","  i_attack = int(sr * t_attack)\n","  i_sustain = int(sr * t_note_length)\n","  i_release = int(sr * t_release)\n","  i_tot = i_sustain + i_release  # attack envelope doesn't add to sound length\n","  envelope = np.ones(i_tot)\n","  # Linear attack\n","  envelope[:i_attack] = np.linspace(0.0, 1.0, i_attack)\n","  # Linear release\n","  envelope[i_sustain:i_tot] = np.linspace(1.0, 0.0, i_release)\n","  return envelope"],"outputs":[],"metadata":{"id":"UfcyLKDiIWPU"}},{"cell_type":"markdown","source":["Define a function to combine multiple notes from a single audio clip."],"metadata":{"id":"fC67KjNhIZ-m"}},{"cell_type":"code","execution_count":null,"source":["def combine_notes(audio_notes, start_times, end_times, velocities, sr=16000):\n","  \"\"\"Combine audio from multiple notes into a single audio clip.\n","\n","  Args:\n","    audio_notes: Array of audio [n_notes, audio_samples].\n","    start_times: Array of note starts in seconds [n_notes].\n","    end_times: Array of note ends in seconds [n_notes].\n","    sr: Integer, sample rate.\n","\n","  Returns:\n","    audio_clip: Array of combined audio clip [audio_samples]\n","  \"\"\"\n","  n_notes = len(audio_notes)\n","  clip_length = end_times.max() + 3.0\n","  audio_clip = np.zeros(int(clip_length) * sr)\n","\n","  for t_start, t_end, vel, i in zip(start_times, end_times, velocities, range(n_notes)):\n","    # Generate an amplitude envelope\n","    t_note_length = t_end - t_start\n","    envelope = get_envelope(t_note_length)\n","    length = len(envelope)\n","    audio_note = audio_notes[i, :length] * envelope\n","    # Normalize\n","    audio_note /= audio_note.max()\n","    audio_note *= (vel / 127.0)\n","    # Add to clip buffer\n","    clip_start = int(t_start * sr)\n","    clip_end = clip_start + length\n","    audio_clip[clip_start:clip_end] += audio_note\n","\n","  # Normalize\n","  audio_clip /= audio_clip.max()\n","  audio_clip /= 2.0\n","  return audio_clip"],"outputs":[],"metadata":{"id":"Bu7LGz7hIeue"}},{"cell_type":"markdown","source":["Define a function to plot spectrogram"],"metadata":{"id":"Re6swoB7IjpC"}},{"cell_type":"code","execution_count":null,"source":["# Plotting tools\n","def specplot(audio_clip):\n","  p_min = np.min(36)\n","  p_max = np.max(84)\n","  f_min = librosa.midi_to_hz(p_min)\n","  f_max = 2 * librosa.midi_to_hz(p_max)\n","  octaves = int(np.ceil(np.log2(f_max) - np.log2(f_min)))\n","  bins_per_octave = 36\n","  n_bins = int(bins_per_octave * octaves)\n","  C = librosa.cqt(audio_clip, sr=SR, hop_length=2048, fmin=f_min, n_bins=n_bins, bins_per_octave=bins_per_octave)\n","  power = 10 * np.log10(np.abs(C)**2 + 1e-6)\n","  plt.matshow(power[::-1, 2:-2], aspect='auto', cmap=plt.cm.magma)\n","  plt.yticks([])\n","  plt.xticks([])\n","\n","print('And...... Done!')"],"outputs":[],"metadata":{"id":"F71w2NTuIhbj"}},{"cell_type":"markdown","source":["Choose the Interpolation\n","\n","These cells allow you to choose two latent vectors and interpolate between them over a MIDI clip."],"metadata":{"id":"zX_T22HDuU7M"}},{"cell_type":"markdown","source":["Choose the MIDI file"],"metadata":{"id":"kjOCkLt3Iqv4"}},{"cell_type":"markdown","source":["This will allow you to choose the default uploaded MIDI file or upload a file of your choice as follows:"],"metadata":{"id":"DdHGPQP-IyU2"}},{"cell_type":"code","execution_count":null,"source":["midi_file = \"Arpeggio (Default)\"\n","\n","midi_path = MIDI_RIFF_DEFAULT\n","if midi_file == \"Upload your own\":\n","  try:\n","    file_list = upload()\n","    midi_path = file_list[0]\n","    ns, notes_2 = load_midi(midi_path)\n","  except Exception as e:\n","    print('Upload Cancelled')\n","else:\n","  # Load Default, but slow it down 30%\n","  ns, notes_2 = load_midi(midi_path)\n","  notes_2['start_times'] *= 1.3\n","  notes_2['end_times'] *= 1.3\n","\n","\n","print('Loaded {}'.format(midi_path))\n","note_seq.plot_sequence(ns)"],"outputs":[],"metadata":{"id":"QvaMGVIzuVy1"}},{"cell_type":"markdown","source":["Choose some random instruments to generate custom interpolation. \n","\n","Audio ‘interpolation’ means making the audio sound better."],"metadata":{"id":"i4sURTEmI3Ke"}},{"cell_type":"code","execution_count":null,"source":["number_of_random_instruments = 10\n","pitch_preview = 60\n","n_preview = number_of_random_instruments\n","\n","pitches_preview = [pitch_preview] * n_preview\n","z_preview = model.generate_z(n_preview)\n","\n","audio_notes = model.generate_samples_from_z(z_preview, pitches_preview)\n","for i, audio_note in enumerate(audio_notes):\n","  print(\"Instrument: {}\".format(i))\n","  play(audio_note, sample_rate=16000)\n"],"outputs":[],"metadata":{"id":"Tbe66_ZTub_b"}},{"cell_type":"markdown","source":["Create a list of instruments to interpolate between"],"metadata":{"id":"wY--ZI4hJE59"}},{"cell_type":"code","execution_count":null,"source":["instruments = [0, 2, 4, 0]"],"outputs":[],"metadata":{"id":"8QGptcoKJk-F"}},{"cell_type":"markdown","source":["Place each instrument at a specific point of time (from 0 to 1.0)"],"metadata":{"id":"Ypr9mXA0Jj92"}},{"cell_type":"code","execution_count":null,"source":["times = [0, 0.3, 0.6, 1.0]"],"outputs":[],"metadata":{"id":"xxPZvc3TJoZr"}},{"cell_type":"markdown","source":["Start and end times of synthesized audio"],"metadata":{"id":"fxcc-MBqJqO3"}},{"cell_type":"code","execution_count":null,"source":["# Force endpoints\n","times[0] = -0.001\n","times[-1] = 1.0"],"outputs":[],"metadata":{"id":"si-4gDk7Jsoq"}},{"cell_type":"markdown","source":["Latent vectors of selected instruments"],"metadata":{"id":"bZZMZQYRJv74"}},{"cell_type":"code","execution_count":null,"source":["z_instruments = np.array([z_preview[i] for i in instruments])"],"outputs":[],"metadata":{"id":"1MHrQalQJzCn"}},{"cell_type":"markdown","source":["End times for selected instruments"],"metadata":{"id":"PKmHoPqKJyiK"}},{"cell_type":"code","execution_count":null,"source":["t_instruments = np.array([notes_2['end_times'][-1] * t for t in times])"],"outputs":[],"metadata":{"id":"qmxPCGQTJ2aU"}},{"cell_type":"markdown","source":[" Get interpolated latent vectors for each note"],"metadata":{"id":"0kYPUzRzJ4VU"}},{"cell_type":"code","execution_count":null,"source":["z_notes = gu.get_z_notes(notes_2['start_times'], z_instruments, t_instruments)\n"],"outputs":[],"metadata":{"id":"W4OVOC2SJ5Ad"}},{"cell_type":"markdown","source":["Generate audio for each note"],"metadata":{"id":"yJN62AwMJ72-"}},{"cell_type":"code","execution_count":null,"source":["# Generate audio for each note\n","print('Generating {} samples...'.format(len(z_notes)))\n","audio_notes = model.generate_samples_from_z(z_notes, notes_2['pitches'])"],"outputs":[],"metadata":{"id":"cVFllmBqJ-41"}},{"cell_type":"markdown","source":["Combine the audio samples of all instruments into a single audio clip"],"metadata":{"id":"epjvlyULKBZY"}},{"cell_type":"code","execution_count":null,"source":["# Make a single audio clip\n","audio_clip = combine_notes(audio_notes,\n","                           notes_2['start_times'],\n","                           notes_2['end_times'],\n","                           notes_2['velocities'])"],"outputs":[],"metadata":{"id":"Ccr-52lIKDKA"}},{"cell_type":"markdown","source":["Play the synthesized audio"],"metadata":{"id":"b8HfI6WlKE7V"}},{"cell_type":"code","execution_count":null,"source":["# Play the audio\n","print('\\nAudio:')\n","play(audio_clip, sample_rate=SR)"],"outputs":[],"metadata":{"id":"IyvkrsU1uiqh"}},{"cell_type":"markdown","source":["Plot the spectrogram using spectrogram()"],"metadata":{"id":"mFWjEEZBKIqz"}},{"cell_type":"code","execution_count":null,"source":["print('CQT Spectrogram:')\n","specplot(audio_clip) "],"outputs":[],"metadata":{"id":"oCovaEeZQ5OW"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [GANSynth](https://analyticsindiamag.com/hands-on-guide-to-gansynth-an-adversarial-neural-audio-synthesis-technique/)\n","\n","> * [Audio Visualizaton](https://analyticsindiamag.com/step-by-step-guide-to-audio-visualization-in-python/)\n","\n","> * [VGG Sound Datasets](https://analyticsindiamag.com/guide-to-vgg-sound-datasets-for-visual-audio-recognition/)\n","\n","> * [Voxceleb Datasets](https://analyticsindiamag.com/guide-to-voxceleb-datasets-for-visual-audio-of-human-speech/)\n","\n","> * [FreeSound Datasets](https://analyticsindiamag.com/datasets-freesound-pytorch-research/)"],"metadata":{"id":"agPHjISycAxz"}}]}