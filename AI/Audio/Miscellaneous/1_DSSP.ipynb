{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_DSSP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMuf1pB2kyaEhhodG0D5Iq7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Differentiable Digital Signal Processing (DDSP) Library**"],"metadata":{"id":"jT9U23__R6KQ"}},{"cell_type":"markdown","source":["Differentiable Digital Signal Processing (DDSP) is an audio generation library that uses classical interpretable DSP elements (like oscillators, filters, synthesizers) with deep learning models. "],"metadata":{"id":"-5mwkmd7R9zC"}},{"cell_type":"markdown","source":["DDSP library creates complex realistic audio signals by controlling parameters of simple interpretable DSP, e.g. by tuning the frequencies and responses of sinusoidal oscillators and linear filters; it can synthesize the sound of a realistic instrument such as violin, flute etc."],"metadata":{"id":"ywZhs07WSCGk"}},{"cell_type":"markdown","source":["To know about the working of DSSP, please refer [this](https://analyticsindiamag.com/guide-to-differentiable-digital-signal-processing-ddsp-library-with-python-code/) article."],"metadata":{"id":"uTXcOZWHSC1w"}},{"cell_type":"markdown","source":["# **Practical implementation of DDSP**"],"metadata":{"id":"WKoMNVv5SLHR"}},{"cell_type":"markdown","source":["## Install DDSP library"],"metadata":{"id":"zjirpx5TSO35"}},{"cell_type":"code","execution_count":null,"source":["\n","!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image pillow librosa torchaudio ddsp --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Import required libraries and modules."],"metadata":{"id":"lFGWU9xTSVA6"}},{"cell_type":"code","execution_count":null,"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import copy\n","import os  #for interacting with the operating system\n","import time \n","import crepe\n","import ddsp\n","import ddsp.training\n","from ddsp.colab import colab_utils\n","from ddsp.colab.colab_utils import (\n","    auto_tune, detect_notes, fit_quantile_transform, \n","    get_tuning_factor, download, play, record, \n","    specplot, upload, DEFAULT_SAMPLE_RATE)\n","import gin\n","from google.colab import files\n","import librosa\n","import matplotlib.pyplot as pl\n","import numpy as np\n","import pickle\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets as tfds "],"outputs":[],"metadata":{"id":"cmBaV3GESWc1"}},{"cell_type":"markdown","source":["Initialize signal sampling rate (default sampling rate of 16000 defined in ddsp.spectral_ops has been used here)"],"metadata":{"id":"m4M5psVbSptW"}},{"cell_type":"code","execution_count":null,"source":["sample_rate = DEFAULT_SAMPLE_RATE   "],"outputs":[],"metadata":{"id":"xw5vVM7ASsYG"}},{"cell_type":"markdown","source":["Display options for the user to record an input audio signal or upload one. If recorded, provide an option of selecting the number of seconds for which recording is to be done."],"metadata":{"id":"oeWxL4F5SuMZ"}},{"cell_type":"code","execution_count":null,"source":["record_or_upload = \"Upload (.mp3 or .wav)\" \n","\"\"\"\n","Input for recording’s duration can range from 1 to 10  seconds; it can be changed in step of 1 seconds\n","\"\"\"\n","record_seconds = 20"],"outputs":[],"metadata":{"id":"ZjTwY8bYSxSZ"}},{"cell_type":"markdown","source":["Define actions to be performed based on the user’s selection of recording or uploading the audio."],"metadata":{"id":"SnvmXLsIS-vi"}},{"cell_type":"code","execution_count":null,"source":["#If user selects ‘Record’ option, record audio from browser using record() method defined here\n","if record_or_upload == \"Record\":\n","  audio = record(seconds=record_seconds)\n","\n","#If user selects ‘Upload’ option, allow loading a .wav or .mp3 audio file from disk into the colab notebook using upload() method defined here\n","else:\n","  filenames, audios = upload()\n","\"\"\"\n","upload() returns names of the files uploaded and their respective audio sound. If user uploads multiple files, select the first one from the ‘audios’ array\n","\"\"\"\n","audio = audios[0]\n","audio = audio[np.newaxis, :]\n","print('\\nExtracting audio features...') "],"outputs":[],"metadata":{"id":"AOCCXIohTBaz"}},{"cell_type":"markdown","source":["Plot the spectrum of the audio signal using specplot() method"],"metadata":{"id":"HOa-gUYPTUJJ"}},{"cell_type":"code","execution_count":null,"source":["specplot(audio)"],"outputs":[],"metadata":{"id":"xhOiSMNYTYBn"}},{"cell_type":"markdown","source":["Create an HTML5 audio widget using play() method to play the audio file"],"metadata":{"id":"FrCDBRi0TZq9"}},{"cell_type":"code","execution_count":null,"source":["play(audio)"],"outputs":[],"metadata":{"id":"pEQYO9ImTX8M"}},{"cell_type":"markdown","source":["Reset CREPE’s global state for re-building the model"],"metadata":{"id":"Lh1q1RmvTc1P"}},{"cell_type":"code","execution_count":null,"source":["ddsp.spectral_ops.reset_crepe()"],"outputs":[],"metadata":{"id":"HUsg4p8xTddH"}},{"cell_type":"markdown","source":["Record the start time of audio"],"metadata":{"id":"EhMF6yAITgDF"}},{"cell_type":"code","execution_count":null,"source":["start_time = time.time()"],"outputs":[],"metadata":{"id":"wrRYvoZaTiPt"}},{"cell_type":"markdown","source":["Compute audio features"],"metadata":{"id":"S__lusWITjvb"}},{"cell_type":"code","execution_count":null,"source":["audio_features = ddsp.training.metrics.compute_audio_features(audio)"],"outputs":[],"metadata":{"id":"QkgppGDLTllC"}},{"cell_type":"markdown","source":["Store the loudness (in decibels) of the audio "],"metadata":{"id":"0QzbaGP-SrqX"}},{"cell_type":"code","execution_count":null,"source":["audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n","audio_features_mod = None "],"outputs":[],"metadata":{"id":"49_qQ4WSTqOu"}},{"cell_type":"markdown","source":["Compute the time taken for calculating audio features by subtracting start time from the current time"],"metadata":{"id":"BZz3zot1Ttfb"}},{"cell_type":"code","execution_count":null,"source":["print('Audio features took %.1f seconds' % (time.time() - start_time))"],"outputs":[],"metadata":{"id":"pQc9DUWVTu91"}},{"cell_type":"markdown","source":["Plot the computed features"],"metadata":{"id":"-P2_uAFHTvZ1"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","TRIM = -15\n","fig, ax = plt.subplots(nrows=3, \n","                      ncols=1, \n","                      sharex=True,\n","                      figsize=(6, 8))\n","#Plot the loudness of audio\n","ax[0].plot(audio_features['loudness_db'][:-15])\n","ax[0].set_ylabel('loudness_db')\n","#Plot the frequency of MIDI notes\n","ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n","ax[1].set_ylabel('f0 [midi]')\n","#Plot the confidence of audio signal\n","ax[2].plot(audio_features['f0_confidence'][:TRIM])\n","ax[2].set_ylabel('f0 confidence')\n","_ = ax[2].set_xlabel('Time step [frame]') "],"outputs":[],"metadata":{"id":"qNKK9h7hTy2P"}},{"cell_type":"markdown","source":["Select the pretrained model of an instrument to be used."],"metadata":{"id":"011sbms5T1Wn"}},{"cell_type":"code","execution_count":null,"source":["model = 'Violin'\n","MODEL = model "],"outputs":[],"metadata":{"id":"DeckYR0UT6T4"}},{"cell_type":"markdown","source":["Define a function to find the selected model"],"metadata":{"id":"KdfiYtS1T-aV"}},{"cell_type":"code","execution_count":null,"source":["def find_model_dir(dir_name):\n","  # Iterate through directories until model directory is found\n","  for root, dirs, filenames in os.walk(dir_name):\n","    for filename in filenames:\n","      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n","        model_dir = root\n","        break\n","  return model_dir "],"outputs":[],"metadata":{"id":"jbfvq9LnUAVE"}},{"cell_type":"markdown","source":["Select the model to be used."],"metadata":{"id":"0KlZ9PfhUDqi"}},{"cell_type":"code","execution_count":null,"source":["if model in ('Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone'):\n","    # Pretrained models.\n","   PRETRAINED_DIR = '/content/pretrained'\n","   # Copy over from gs:// for faster loading.\n","   !rm -r $PRETRAINED_DIR &> /dev/null\n","   !mkdir $PRETRAINED_DIR &> /dev/null\n","   GCS_CKPT_DIR = 'gs://ddsp/models/timbre_transfer_colab/2021-01-06'\n","   model_dir = os.path.join(GCS_CKPT_DIR, 'solo_%s_ckpt' % model.lower())\n","   !gsutil cp $model_dir/* $PRETRAINED_DIR &> /dev/null\n","   model_dir = PRETRAINED_DIR\n","   gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n","else:\n","    # User models.\n","    UPLOAD_DIR = '/content/uploaded'\n","    !mkdir $UPLOAD_DIR\n","    uploaded_files = files.upload()\n","    for fnames in uploaded_files.keys():\n","     print(\"Unzipping... {}\".format(fnames))\n","    !unzip -o \"/content/$fnames\" -d $UPLOAD_DIR &> /dev/null\n","    model_dir = find_model_dir(UPLOAD_DIR)\n","    gin_file = os.path.join(model_dir, 'operative_config-0.gin') "],"outputs":[],"metadata":{"id":"xHJnsgnUUDDR"}},{"cell_type":"markdown","source":["Load the dataset statistics file"],"metadata":{"id":"3n8zpq1CUS6p"}},{"cell_type":"code","execution_count":null,"source":["DATASET_STATS = None\n","dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n","print(f'Loading dataset statistics from {dataset_stats_file}')\n","try:\n","#Load the dataset statistics file if it exists\n","  if tf.io.gfile.exists(dataset_stats_file):\n","    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n","      DATASET_STATS = pickle.load(f)\n","#throw exception if loading of the file fails\n","except Exception as err:\n","  print('Loading dataset statistics from pickle failed: {}.'.format(err))"],"outputs":[],"metadata":{"id":"GY4FJOj9UVCQ"}},{"cell_type":"markdown","source":["Define a method to parse gin config"],"metadata":{"id":"eft6w-uOUXgw"}},{"cell_type":"code","execution_count":null,"source":["#First, unlock the config temporarily using a context manager\n","with gin.unlock_config():\n","#Parse the file using parse_config_file() defined here\n","  gin.parse_config_file(gin_file, skip_unknown=True) "],"outputs":[],"metadata":{"id":"qRt4ZoKVUZaM"}},{"cell_type":"markdown","source":["Store the checkpoint files\n"],"metadata":{"id":"sRvhZsPXVCrY"}},{"cell_type":"code","execution_count":null,"source":["\"\"\"\n","For each file in the list containing files of the model directory, add it to the ‘ckpt_files’ if it has checkpoint\n","\"\"\"\n","# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n","ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n","ckpt_name = ckpt_files[0].split('.')[0]\n","ckpt = os.path.join(model_dir, ckpt_name)"],"outputs":[],"metadata":{"id":"Hj_aS0-mVGYF"}},{"cell_type":"markdown","source":["Check that dimensions and sampling rates are equal"],"metadata":{"id":"iXI-DPnVVLZM"}},{"cell_type":"code","execution_count":null,"source":["\"\"\"\n","gin.query_parameter() returns the value currently bound to the binding key specified as \n","its parameter. Binding is the parameter whose value we need to query \n","\"\"\"\n","#Time steps for training process\n","time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n","#Number of training samples\n","n_samples_train = gin.query_parameter('Harmonic.n_samples')\n","#Compute number of samples between successive frames (called ‘hop size’)\n","hop_size = int(n_samples_train / time_steps_train)\n","#Compute total time steps and number of samples\n","time_steps = int(audio.shape[1] / hop_size)\n","n_samples = time_steps * hop_size "],"outputs":[],"metadata":{"id":"Z6MbaZq7VNX4"}},{"cell_type":"markdown","source":["Create a list of gin parameters "],"metadata":{"id":"xo9OhjtyVR1I"}},{"cell_type":"code","execution_count":null,"source":["gin_params = [\n","    'Harmonic.n_samples = {}'.format(n_samples),\n","    'FilteredNoise.n_samples = {}'.format(n_samples),\n","    'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n","    'oscillator_bank.use_angular_cumsum = True', ]\n","#Parse the above gin parameters\n","#First, unlock the config \n","with gin.unlock_config():\n","#Parse the list of parameter bindings using parse_config()\n","  gin.parse_config(gin_params) "],"outputs":[],"metadata":{"id":"CcS7tLCfVT-s"}},{"cell_type":"markdown","source":["Trim the input vectors to correct lengths "],"metadata":{"id":"2dwZB4YfU9rQ"}},{"cell_type":"code","execution_count":null,"source":["#Trip each of the frequency, confidence and loudness to its time step length\n","for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n","  audio_features[key] = audio_features[key][:time_steps]\n","#Trip ‘audio’ vector to the length equal to the total number os samples\n","audio_features['audio'] = audio_features['audio'][:, :n_samples] "],"outputs":[],"metadata":{"id":"5lyoEcCJU9cu"}},{"cell_type":"markdown","source":["Initialize the model just to predict audio"],"metadata":{"id":"AnUrDNCBVcEW"}},{"cell_type":"code","execution_count":null,"source":["model = ddsp.training.models.Autoencoder()"],"outputs":[],"metadata":{"id":"aMjfT4qbVfNG"}},{"cell_type":"markdown","source":["\n","Restore the model checkpoints\n","\n"],"metadata":{"id":"0t0FOqB5VggK"}},{"cell_type":"code","execution_count":null,"source":["model.restore(ckpt)"],"outputs":[],"metadata":{"id":"Qg7ALFcGViB7"}},{"cell_type":"markdown","source":["Build a model by running a batch of audio features through it."],"metadata":{"id":"jED8P6CtVkw9"}},{"cell_type":"code","execution_count":null,"source":["#Record start time of the audio\n","start_time = time.time()\n","#Build the model using computed features\n","_ = model(audio_features, training=False)\n","\"\"\"\n","Display the time taken for model building by computing difference between current time and start time of audio\n","\"\"\"\n","print('Restoring model took %.1f seconds' % (time.time() - start_time)) "],"outputs":[],"metadata":{"id":"5Z7yhBltVm0j"}},{"cell_type":"markdown","source":["  The pretrained models (Violin, Flute etc.) were not explicitly trained to perform timbre transfer, so they may sound unnatural if the input audio frequencies and loudness are very different from the training data (which will be true most of the time).\n","\n","Create sliders for model conditioning"],"metadata":{"id":"Y8P_QK85UCzU"}},{"cell_type":"code","execution_count":null,"source":["#You can leave this at 1.0 for most cases\n","threshold = 1 \n","ADJUST = True \n","#Quiet parts without notes detected (dB)\n","quiet = 20 #\n","#Force pitch to nearest note (amount)\n","autotune = 0 #\n","# Shift the pitch (octaves)\n","pitch_shift =  0\n","# Adjust the overall loudness (dB)\n","loudness_shift = 0 \n","audio_features_mod = {k: v.copy() for k, v in audio_features.items()} "],"outputs":[],"metadata":{"id":"i52qc0tXX8M6"}},{"cell_type":"markdown","source":["Define a method to shift loudness "],"metadata":{"id":"UoEmjkFZYTZS"}},{"cell_type":"code","execution_count":null,"source":["def shift_ld(audio_features, ld_shift=0.0):\n","#Increment the loudness by ld_shift\n","  audio_features['loudness_db'] += ld_shift\n","#Return modified audio features\n","  return audio_features"],"outputs":[],"metadata":{"id":"GmJ8pFg3YWF1"}},{"cell_type":"markdown","source":["Define a method to shift frequency by a number of octaves"],"metadata":{"id":"wxMKhTxjT5iB"}},{"cell_type":"code","execution_count":null,"source":["def shift_f0(audio_features, pitch_shift=0.0):\n","#Multiply the frequency by 2^pitch_shift\n","  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n","  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'],0.0,\n","      librosa.midi_to_hz(110.0))\n","  return audio_features "],"outputs":[],"metadata":{"id":"6gBJJZ61YbBY"}},{"cell_type":"markdown","source":["Detect the sections of audio which are ‘on’"],"metadata":{"id":"sFrCoA8qYeR6"}},{"cell_type":"code","execution_count":null,"source":["if ADJUST and DATASET_STATS is not None:\n","#Store the loudness, confidence and notes of ‘on’ sections\n","  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n","  audio_features['f0_confidence'],threshold)"],"outputs":[],"metadata":{"id":"dwr5MoJOYf8k"}},{"cell_type":"markdown","source":["Quantile shift the parts with ‘on’ section"],"metadata":{"id":"wc2kukkuYh5e"}},{"cell_type":"code","execution_count":null,"source":["_, loudness_norm = colab_utils.fit_quantile_transform(\n","audio_features['loudness_db'],mask_on,\n","inv_quantile=DATASET_STATS['quantile_transform']) "],"outputs":[],"metadata":{"id":"dTgxKR0BYhvz"}},{"cell_type":"markdown","source":["Turn down the parts of audio with ‘off’ notes."],"metadata":{"id":"63hL3SGZYmgS"}},{"cell_type":"code","execution_count":null,"source":["## Helper functions.\n","def shift_ld(audio_features, ld_shift=0.0):\n","  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n","  audio_features['loudness_db'] += ld_shift\n","  return audio_features\n","\n","\n","def shift_f0(audio_features, pitch_shift=0.0):\n","  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n","  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n","  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n","                                    0.0, \n","                                    librosa.midi_to_hz(110.0))\n","  return audio_features\n","\n","\n","mask_on = None\n","\n","if ADJUST and DATASET_STATS is not None:\n","  # Detect sections that are \"on\".\n","  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n","                                        audio_features['f0_confidence'],\n","                                        threshold)\n","\n","  if np.any(mask_on):\n","    # Shift the pitch register.\n","    target_mean_pitch = DATASET_STATS['mean_pitch']\n","    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n","    mean_pitch = np.mean(pitch[mask_on])\n","    p_diff = target_mean_pitch - mean_pitch\n","    p_diff_octave = p_diff / 12.0\n","    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n","    p_diff_octave = round_fn(p_diff_octave)\n","    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n","\n","\n","    # Quantile shift the note_on parts.\n","    _, loudness_norm = colab_utils.fit_quantile_transform(\n","        audio_features['loudness_db'],\n","        mask_on,\n","        inv_quantile=DATASET_STATS['quantile_transform'])\n","\n","    # Turn down the note_off parts.\n","    mask_off = np.logical_not(mask_on)\n","    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n","    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n","    \n","    audio_features_mod['loudness_db'] = loudness_norm \n","\n","    # Auto-tune.\n","    if autotune:\n","      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n","      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n","      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n","      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n","\n","  else:\n","    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n","\n","else:\n","  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n"],"outputs":[],"metadata":{"id":"Bp-h25V9YpCo"}},{"cell_type":"markdown","source":["Perform manual shifts of loudness and frequency using methods"],"metadata":{"id":"JMQpTGWcY0z3"}},{"cell_type":"code","execution_count":null,"source":["audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n","audio_features_mod = shift_f0(audio_features_mod, pitch_shift) "],"outputs":[],"metadata":{"id":"-KH_vdV1Y21R"}},{"cell_type":"markdown","source":["Plot the features"],"metadata":{"id":"dJ32Kg65Y4kQ"}},{"cell_type":"code","execution_count":null,"source":["#Check if ‘on’ notes has mask\n","has_mask = int(mask_on is not None)\n","#3 subplots if ‘has_mask’ is 1(True), else only 2 subplots of loudness and frequency\n","n_plots = 3 if has_mask else 2 \n","#Initialize the figure and axes parameters\n","fig, axes = plt.subplots(nrows=n_plots, \n","                      ncols=1, \n","                      sharex=True,\n","                      figsize=(2*n_plots, 8))\n","#Plot the mask of ‘on’ notes, if exists\n","if has_mask:\n","  ax = axes[0]\n","  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n","  ax.plot(note_on_value[:TRIM])\n","  ax.plot(mask_on[:TRIM])\n","  ax.set_ylabel('Note-on Mask')\n","  ax.set_xlabel('Time step [frame]')\n","  ax.legend(['Threshold', 'Likelihood','Mask'])\n","\n","#Plot the original and adjusted loudness\n","ax = axes[0 + has_mask]\n","ax.plot(audio_features['loudness_db'][:TRIM])\n","ax.plot(audio_features_mod['loudness_db'][:TRIM])\n","ax.set_ylabel('loudness_db')\n","ax.legend(['Original','Adjusted'])\n","#Plot the original and adjusted frequencies\n","ax = axes[1 + has_mask]\n","ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n","ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n","ax.set_ylabel('f0 [midi]')\n","_ = ax.legend(['Original','Adjusted']) "],"outputs":[],"metadata":{"id":"9ZgB-K67Y64D"}},{"cell_type":"markdown","source":["Resynthesize the audio "],"metadata":{"id":"Bb8eNKpGY86-"}},{"cell_type":"markdown","source":["Store the computed audio features first"],"metadata":{"id":"khQJYa1KY4dp"}},{"cell_type":"code","execution_count":null,"source":["af = audio_features if audio_features_mod is None else audio_features_mod"],"outputs":[],"metadata":{"id":"cMCVMeRCZBNH"}},{"cell_type":"markdown","source":["Run a batch of predictions"],"metadata":{"id":"Kk-yE2aDZC72"}},{"cell_type":"code","execution_count":null,"source":["#Record the start time of audio\n","start_time = time.time()\n","#Apply the model defined in step (17) using the computed audio feature\n","outputs = model(af, training=False) "],"outputs":[],"metadata":{"id":"GU392XwiZEfZ"}},{"cell_type":"markdown","source":["Extract audio output from outputs’ dictionary"],"metadata":{"id":"U0V-OUuXZGOh"}},{"cell_type":"code","execution_count":null,"source":["audio_gen = model.get_audio_from_outputs(outputs)"],"outputs":[],"metadata":{"id":"4QGEe8KNZIAG"}},{"cell_type":"markdown","source":["Display the time taken for making predictions by computing difference between current time and start time of input audio"],"metadata":{"id":"RdbVqqrSZJhd"}},{"cell_type":"code","execution_count":null,"source":["print('Prediction took %.1f seconds' % (time.time() - start_time))"],"outputs":[],"metadata":{"id":"WH6druSoZLQG"}},{"cell_type":"markdown","source":["Plot the HTML5 widget for playing the original and resynthesized audios as well as spectrum of both the signals"],"metadata":{"id":"EIXY9_GpZNlX"}},{"cell_type":"code","execution_count":null,"source":["print('Original')\n","play(audio)\n","print('Resynthesis')\n","play(audio_gen)\n","specplot(audio)\n","plt.title(\"Original\")\n","specplot(audio_gen)\n","_ = plt.title(\"Resynthesis\") "],"outputs":[],"metadata":{"id":"0c7uU_8CZMzZ"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [DSSP Library](https://analyticsindiamag.com/guide-to-differentiable-digital-signal-processing-ddsp-library-with-python-code/)\n","\n","> * [YAMNet](https://analyticsindiamag.com/guide-to-yamnet-sound-event-classifier/)\n","\n","> * [GANSynth](https://analyticsindiamag.com/hands-on-guide-to-gansynth-an-adversarial-neural-audio-synthesis-technique/)\n","\n","> * [Audio Visualizaton](https://analyticsindiamag.com/step-by-step-guide-to-audio-visualization-in-python/)\n","\n","> * [VGG Sound Datasets](https://analyticsindiamag.com/guide-to-vgg-sound-datasets-for-visual-audio-recognition/)\n"],"metadata":{"id":"agPHjISycAxz"}}]}