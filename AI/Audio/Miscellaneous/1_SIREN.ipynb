{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_SIREN.ipynb","provenance":[],"authorship_tag":"ABX9TyNZWuX63OmPg2mYfghSr0h+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vD4Igloldod_"},"source":["# **Sinusoidal Representation Networks for Implicit Neural Representations**"]},{"cell_type":"markdown","metadata":{"id":"J4cTue7Jdsq7"},"source":["Implicit Neural Representations yield memory-efficient shape or object or appearance or scene reconstructions for various machine learning problems, including 2D/3D images, videos, audio and wave problems. However, present implicit neural representations employ non-periodic activation functions such as ReLU, tanh, sigmoid and softplus. ReLU is linear, continuous and differentiable to first-order, but it cannot be differentiated twice. On the other hand, a few variants of ReLU, tanh, sigmoid and softplus are twice-differentiable and continuous. But, these functions are unable to handle a physical signalâ€™s spatial and temporal derivatives. Therefore, these functions fail to yield satisfactory reconstructions for complex and higher-order problems. "]},{"cell_type":"code","metadata":{"id":"ljWze0R5dlzj"},"source":[""],"execution_count":null,"outputs":[]}]}