{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_YAMNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO8MJMNkekdmZ8qQD11rtuW"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **YAMNet**"],"metadata":{"id":"7WB6Dh3TLQd5"}},{"cell_type":"markdown","source":["Transfer Learning is a well-liked and popular machine learning technique in which one can train a model by reusing information learned from a previously existing model. You must have heard and read about common applications of transfer learning in the vision domain – training models to accurately classify images and do object detection or text-domain – sentiment analysis or question answering, and the list goes on …"],"metadata":{"id":"aUyJXJUDLYZ3"}},{"cell_type":"markdown","source":["With YAMNet, we can easily create a sound classifier in a few simple and easy steps!\n","\n","YAMNet (Yet Another Mobile Network) – Yes, that is the full form, is a pretrained acoustic detection model trained by Dan Ellis on the AudioSet dataset which contains labelled data from more than 2 million Youtube videos. It employs the MobileNet_v1 depth-wise-separable convolution architecture. This pretrained model is readily available in Tensorflow Hub, which includes TFLite(lite model for mobile) and TF.js(running on the web) versions."],"metadata":{"id":"KFiuz2_6Lbjc"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/guide-to-yamnet-sound-event-classifier/) article."],"metadata":{"id":"1HruzrWvLcM4"}},{"cell_type":"markdown","source":["## **Code Implementation**"],"metadata":{"id":"0pKzQPcxLiXC"}},{"cell_type":"markdown","source":["### Importing the Dependencies "],"metadata":{"id":"ET6rjQ48LmXx"}},{"cell_type":"markdown","source":["Importing tensorflow-hub for leveraging the pre-trained model, wavfile for storing an audio file.\n","\n","IPython.display lets us play audio right here in the notebook."],"metadata":{"id":"jKQQG3M-Lpvq"}},{"cell_type":"code","execution_count":null,"source":["\n","!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image pillow librosa torchaudio tensorflow_hub apache_beam --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","import csv\n","\n","import matplotlib.pyplot as plt\n","from IPython.display import Audio\n","from scipy.io import wavfile"],"outputs":[],"metadata":{"id":"Bteu7pfkpt_f","executionInfo":{"status":"ok","timestamp":1623232269770,"user_tz":-330,"elapsed":2118,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["### Loading the Model"],"metadata":{"id":"7oI_WyTxLrvG"}},{"cell_type":"markdown","source":["Instantiating the pre-trained model to a variable using hub.model method for usage in the below cells. The labels file will also be loaded from model assets and is present at model.class_map_path(). We require this to load it on the class_names variable later on."],"metadata":{"id":"rGOKyMZELvKN"}},{"cell_type":"code","execution_count":null,"source":["# Load the model.\n","model = hub.load('https://tfhub.dev/google/yamnet/1')"],"outputs":[],"metadata":{"id":"VX8Vzs6EpwMo","executionInfo":{"status":"ok","timestamp":1623232286447,"user_tz":-330,"elapsed":5562,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["### Helper_Function_1\n","\n","This helper function is created to find the name of the class with the top score when mean-aggregated across frames."],"metadata":{"id":"qt64nbBPLxRU"}},{"cell_type":"code","execution_count":null,"source":["# Find the name of the class with the top score when mean-aggregated across frames.\n","def class_names_from_csv(class_map_csv_text):\n","  \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n","  class_names = []\n","  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:\n","    reader = csv.DictReader(csvfile)\n","    for row in reader:\n","      class_names.append(row['display_name'])\n","\n","  return class_names\n","\n","class_map_path = model.class_map_path().numpy()\n","class_names = class_names_from_csv(class_map_path)"],"outputs":[],"metadata":{"id":"EHSToAW--o4U","executionInfo":{"status":"ok","timestamp":1623232301870,"user_tz":-330,"elapsed":484,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["### Helper_Function_2\n","\n","We need to add this method to verify and convert the loaded audio to the proper sample_rate which is 16K. This is mentioned in the YAMNet paper by the authors, as it can adversely affect our model results."],"metadata":{"id":"Fywg-o-KL08r"}},{"cell_type":"code","execution_count":null,"source":["def ensure_sample_rate(original_sample_rate, waveform,\n","                       desired_sample_rate=16000):\n","  \"\"\"Resample waveform if required.\"\"\"\n","  if original_sample_rate != desired_sample_rate:\n","    desired_length = int(round(float(len(waveform)) /\n","                               original_sample_rate * desired_sample_rate))\n","    waveform = scipy.signal.resample(waveform, desired_length)\n","  return desired_sample_rate, waveform"],"outputs":[],"metadata":{"id":"LizGwWjc5w6A","executionInfo":{"status":"ok","timestamp":1623232312159,"user_tz":-330,"elapsed":597,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["### Downloading and Preparing the sound file "],"metadata":{"id":"ZVXHaqBJL7CB"}},{"cell_type":"code","execution_count":null,"source":["# !curl -O https://storage.googleapis.com/audioset/speech_whistling2.wav"],"outputs":[],"metadata":{"id":"WzZHvyTtsJrc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623232313115,"user_tz":-330,"elapsed":465,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"34bc589f-0ab9-4b36-ef82-9104bcacafe2"}},{"cell_type":"code","execution_count":null,"source":["# !curl -O https://storage.googleapis.com/audioset/miaow_16k.wav"],"outputs":[],"metadata":{"id":"D8LKmqvGzZzr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623232314262,"user_tz":-330,"elapsed":606,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"801ddae8-8868-4fc1-a27f-402a838e3a7b"}},{"cell_type":"markdown","source":["We can also listen to a sample audio file from the downloaded data set and check its properties by applying the following snippet. As shown below, we can play the sample audio file and look at some information about this particular audio file.  "],"metadata":{"id":"Ex4D2mMFL9ij"}},{"cell_type":"code","execution_count":null,"source":["# wav_file_name = 'speech_whistling2.wav'\n","wav_file_name = 'miaow_16k.wav'\n","sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')\n","sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n","\n","# Show some basic information about the audio.\n","duration = len(wav_data)/sample_rate\n","print(f'Sample rate: {sample_rate} Hz')\n","print(f'Total duration: {duration:.2f}s')\n","print(f'Size of the input: {len(wav_data)}')\n","\n","# Listening to the wav file.\n","Audio(wav_data, rate=sample_rate)"],"outputs":[],"metadata":{"id":"Wo9KJb-5zuz1","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1623232371192,"user_tz":-330,"elapsed":1864,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"1a1bba47-fcca-4381-820f-b1f659d035a0"}},{"cell_type":"markdown","source":["### Running the Model "],"metadata":{"id":"UoZfePWBMA0c"}},{"cell_type":"markdown","source":["We are converting the wave data into numbers to feed to the pre-trained model. The model will give us scores , embeddings, and spectrograms as output that we can later display. Helper_Function_1 gives us the output as “Animal” which means that this is the label with the maximum number of audio files in our dataset."],"metadata":{"id":"4S772mxSMDV8"}},{"cell_type":"code","execution_count":null,"source":["waveform = wav_data / tf.int16.max"],"outputs":[],"metadata":{"id":"bKr78aCBsQo3","executionInfo":{"status":"ok","timestamp":1623232371643,"user_tz":-330,"elapsed":4,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["# Run the model, check the output.\n","scores, embeddings, spectrogram = model(waveform)"],"outputs":[],"metadata":{"id":"BJGP6r-At_Jc","executionInfo":{"status":"ok","timestamp":1623232372105,"user_tz":-330,"elapsed":465,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["scores_np = scores.numpy()\n","spectrogram_np = spectrogram.numpy()\n","infered_class = class_names[scores_np.mean(axis=0).argmax()]\n","print(f'The main sound is: {infered_class}')"],"outputs":[],"metadata":{"id":"Vmo7griQprDk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623232381644,"user_tz":-330,"elapsed":448,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"aca2ef6f-ecb8-455b-92bd-61ac497ce324"}},{"cell_type":"markdown","source":["### Plotting the Output\n","\n","Plotting the three outputs we got from running the model, namely scores, embeddings and spectrograms. "],"metadata":{"id":"SSE2BttaMIrf"}},{"cell_type":"markdown","source":["After running the below snippets, it will be displayed as below.\n"],"metadata":{"id":"6_beXMI1MOoB"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(10, 6))\n","\n","# Plot the waveform.\n","plt.subplot(3, 1, 1)\n","plt.plot(waveform)\n","plt.xlim([0, len(waveform)])\n","\n","# Plot the log-mel spectrogram (returned by the model).\n","plt.subplot(3, 1, 2)\n","plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n","\n","# Plot and label the model output scores for the top-scoring classes.\n","mean_scores = np.mean(scores, axis=0)\n","top_n = 10\n","top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n","plt.subplot(3, 1, 3)\n","plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n","\n","# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n","# values from the model documentation\n","patch_padding = (0.025 / 2) / 0.01\n","plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n","# Label the top_N classes.\n","yticks = range(0, top_n, 1)\n","plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n","_ = plt.ylim(-0.5 + np.array([top_n, 0]))"],"outputs":[],"metadata":{"id":"_QSTkmv7wr2M","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"ok","timestamp":1623232392696,"user_tz":-330,"elapsed":2130,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"c49752c9-8453-4f1f-f28d-a3961ecd7d53"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [YAMNet](https://analyticsindiamag.com/guide-to-yamnet-sound-event-classifier/)\n","\n","> * [GANSynth](https://analyticsindiamag.com/hands-on-guide-to-gansynth-an-adversarial-neural-audio-synthesis-technique/)\n","\n","> * [Audio Visualizaton](https://analyticsindiamag.com/step-by-step-guide-to-audio-visualization-in-python/)\n","\n","> * [VGG Sound Datasets](https://analyticsindiamag.com/guide-to-vgg-sound-datasets-for-visual-audio-recognition/)\n","\n","> * [Voxceleb Datasets](https://analyticsindiamag.com/guide-to-voxceleb-datasets-for-visual-audio-of-human-speech/)\n","\n","> * [FreeSound Datasets](https://analyticsindiamag.com/datasets-freesound-pytorch-research/)"],"metadata":{"id":"agPHjISycAxz"}}]}