{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a82cb1b8906c50effbd49afa215f89f814898a1"
   },
   "source": [
    "## 3D Convolutions : Understanding + Use Case - Drug Discovery\n",
    "\n",
    "<br>\n",
    "In one of my previous [kernel](https://www.kaggle.com/shivamb/a-very-comprehensive-tutorial-nn-cnn), I have shared the working of convolutional neural networks for images. In this kernel, I have explained 3D convolutions and their implementation on 3D MNIST dataset. Later in this kernel, I have shown how to use 3D convolution layers on one of the breakthrough and important area of Healthcare : Drug Discovery\n",
    "\n",
    "### What are Convolutions? \n",
    "\n",
    "- Mathematically, A convolution is an integration function that expresses the amount of overlap of one function g as it is shifted over another function f.   \n",
    "- Intutively, A convolution acts as a blender that mixes one function with another to give reduced data space while preserving the information.    \n",
    "\n",
    "In terms of Neural Networks and Deep Learning:\n",
    "- Convolutions are filter (matrix / vectors) with learnable parameters that are used to extract low-dimentional features from an input data.     \n",
    "- They have the property to preserve the spatial or positional relationships between input data points  \n",
    "- Convolutional neural networks exploits the spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers.    \n",
    "- Intutively, convolution is the step of applying the concept of sliding window (a filter with learnable weights) over the input and producing a weighted sum (of weights and input) as the output. The weighted sum is the feature space which is used as the input for the next layers.  \n",
    "\n",
    "For example, in Face Recognization problem, first few convolution layers learns the pressence of key points in the input image, next convolution layers learns the edges and shapes, and final convolution layers learns the face.  In this example, the input space is first reduced to lower dimentional space (representing information about points / pixels), then this space is reduced to another space containing (edges / shapes) and finally it is reduced to classify faces in the images. Convolutions can be applied in N dimentions.  \n",
    "\n",
    "### Types of Convolutions : \n",
    "\n",
    "Let's discuss what are different types of convolutions \n",
    "\n",
    "### 1D Convolutions\n",
    "\n",
    "Most simplistic convolutions are 1D convolutionals are generally used on sequence datasets (but can be used for other use-cases as well). They can be used for extracting local 1D subsequences from the input sequences and identify local patterns within the window of convolution. The following image shows how a 1 D convolution filter is applied to a sequence to obtain new features. Other common uses of 1D convolutions are seen in the area of NLP where every sentence is represented as a sequence of words.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "![](https://i.imgur.com/5UQz1zI.jpg)\n",
    "\n",
    "\n",
    "### 2D Convolutions \n",
    "\n",
    "On image datasets, mostly 2D Convolutional filters are used in CNN architectures. The main idea of 2D convolutions is that the the convolutional filter moves in 2-directions (x,y) to calculate low dimentional features from the image data. The output shape is also a 2 dimentional matrix. \n",
    "\n",
    "![](https://www.andrewszot.com/static/img/ml/cnn/pooling.png)\n",
    "\n",
    "### 3D Convolutions\n",
    "\n",
    "3D convolutions applies a 3 dimentional filter to the dataset and the filter moves 3-direction (x, y, z) to calcuate the low level feature representations. Their output shape is a 3 dimentional volume space such as cube or cuboid. They are helpful in event detection in videos, 3D medical images etc. They are not limited to 3d space but can also be applied to 2d space inputs such as images.  \n",
    "\n",
    "![](https://i.imgur.com/jriyCTU.png?1)\n",
    "\n",
    "Lets implement the 3D CNN on 3D Mnist dataset. First, lets import the key libraries. \n",
    "\n",
    "Additionally, there are other types of convolutions as well: \n",
    "\n",
    "### Dilated Convolutions  \n",
    "\n",
    "Dilated or Altrous convolutions defines the spacing between the values in a kernel. In this type of convolution, the receptive view of the kernels increases due to the spacing, For example a 3x3 kernel with a dilation rate of 2 will have the same field of view as a 5x5 kernel. The complexity remains the same but different features are generated in this case. \n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-d9025e88d7d792e26f4040b767b25819)\n",
    "\n",
    "Let's now create a 3D convolutional neural network architecture on 3D mnist dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "import plotly.graph_objs as go\n",
    "from matplotlib.pyplot import cm\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import keras\n",
    "import h5py\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2fbce5b6945c7d2871825684cdf6a8d591e7d1e"
   },
   "source": [
    "The 3D MNIST data is given in the .h5 format, lets load the complete dataset into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('../input/full_dataset_vectors.h5', 'r') as dataset:\n",
    "    x_train = dataset[\"X_train\"][:]\n",
    "    x_test = dataset[\"X_test\"][:]\n",
    "    y_train = dataset[\"y_train\"][:]\n",
    "    y_test = dataset[\"y_test\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04b87609b915480c9d030377c298c633ed516e16"
   },
   "source": [
    "Lets look at the dataset dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a36ce4a1304ca30fe57cf9d4225f61dfb3864b97",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"x_train shape: \", x_train.shape)\n",
    "print (\"y_train shape: \", y_train.shape)\n",
    "\n",
    "print (\"x_test shape:  \", x_test.shape)\n",
    "print (\"y_test shape:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3491c058bf820e796a63bb835968e9ad17f3689"
   },
   "source": [
    "This dataset is a flat one dimentional data, but the author of this dataset has also shared the original x,y,z in a separate data file. Lets plot a digits in 3D space. Rotate this 3D digit to view it properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5872c4f2256ba290aa80a372204d2067c1edddc9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"../input/train_point_clouds.h5\", \"r\") as points_dataset:        \n",
    "    digits = []\n",
    "    for i in range(10):\n",
    "        digit = (points_dataset[str(i)][\"img\"][:], \n",
    "                 points_dataset[str(i)][\"points\"][:], \n",
    "                 points_dataset[str(i)].attrs[\"label\"]) \n",
    "        digits.append(digit)\n",
    "        \n",
    "x_c = [r[0] for r in digits[0][1]]\n",
    "y_c = [r[1] for r in digits[0][1]]\n",
    "z_c = [r[2] for r in digits[0][1]]\n",
    "trace1 = go.Scatter3d(x=x_c, y=y_c, z=z_c, mode='markers', \n",
    "                      marker=dict(size=12, color=z_c, colorscale='Viridis', opacity=0.7))\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(height=500, width=600, title= \"Digit: \"+str(digits[0][2]) + \" in 3D space\")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f680ae2832f1e177cdb7868c7ac62f6c454f018a"
   },
   "source": [
    "Now, lets implement a 3D convolutional Neural network on this dataset.  To use 2D convolutions, we first convert every image into a 3D shape : width, height, channels. Channels represents the slices of Red, Green, and Blue layers. So it is set as 3. In the similar manner, we will convert the input dataset into 4D shape in order to use 3D convolution for : length, breadth, height, channel (r/g/b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe9172aef54ee37a733595037e8355d4dd47570e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Introduce the channel dimention in the input dataset \n",
    "xtrain = np.ndarray((x_train.shape[0], 4096, 3))\n",
    "xtest = np.ndarray((x_test.shape[0], 4096, 3))\n",
    "\n",
    "## iterate in train and test, add the rgb dimention \n",
    "def add_rgb_dimention(array):\n",
    "    scaler_map = cm.ScalarMappable(cmap=\"Oranges\")\n",
    "    array = scaler_map.to_rgba(array)[:, : -1]\n",
    "    return array\n",
    "for i in range(x_train.shape[0]):\n",
    "    xtrain[i] = add_rgb_dimention(x_train[i])\n",
    "for i in range(x_test.shape[0]):\n",
    "    xtest[i] = add_rgb_dimention(x_test[i])\n",
    "\n",
    "## convert to 1 + 4D space (1st argument represents number of rows in the dataset)\n",
    "xtrain = xtrain.reshape(x_train.shape[0], 16, 16, 16, 3)\n",
    "xtest = xtest.reshape(x_test.shape[0], 16, 16, 16, 3)\n",
    "\n",
    "## convert target variable into one-hot\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cc7592508b09cd3694a172348c731c4a5dce4e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19dc523490fbf9c0e2ed5192e311e5a19a56e87c"
   },
   "source": [
    "Lets create the model architecture. The architecture is described below: \n",
    "\n",
    "Input and Output layers: \n",
    "- One Input layer with dimentions 16, 16, 16, 3  \n",
    "- Output layer with dimentions 10  \n",
    "\n",
    "Convolutions : \n",
    "- Apply 4 Convolutional layer with increasing order of filter size (standard size : 8, 16, 32, 64) and fixed kernel size = (3, 3, 3)\n",
    "- Apply 2 Max Pooling layers, one after 2nd convolutional layer and one after fourth convolutional layer.  \n",
    "\n",
    "MLP architecture: \n",
    "- Batch normalization on convolutiona architecture  \n",
    "- Dense layers with 2 layers followed by dropout to avoid overfitting  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6995f22169235efa3d99e403df08e2e12dbb682c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## input layer\n",
    "input_layer = Input((16, 16, 16, 3))\n",
    "\n",
    "## convolutional layers\n",
    "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
    "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
    "\n",
    "## add max pooling to obtain the most imformatic features\n",
    "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
    "\n",
    "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
    "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
    "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
    "\n",
    "## perform batch normalization on the convolution outputs before feeding it to MLP architecture\n",
    "pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
    "flatten_layer = Flatten()(pooling_layer2)\n",
    "\n",
    "## create an MLP architecture with dense layers : 4096 -> 512 -> 10\n",
    "## add dropouts to avoid overfitting / perform regularization\n",
    "dense_layer1 = Dense(units=2048, activation='relu')(flatten_layer)\n",
    "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "dense_layer2 = Dense(units=512, activation='relu')(dense_layer1)\n",
    "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "output_layer = Dense(units=10, activation='softmax')(dense_layer2)\n",
    "\n",
    "## define the model with input layer and output layer\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3a57d0bd679d28e088be80bb3c933dc3682b9d8"
   },
   "source": [
    "Compile the model and start training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "796e8799c72a2c883e3820d709a9a38bac138373",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, optimizer=Adadelta(lr=0.1), metrics=['acc'])\n",
    "model.fit(x=xtrain, y=y_train, batch_size=128, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a15f507a96c58870f4037dbdeb8a51bc1084739"
   },
   "source": [
    "In the model training, we can observe that the accuracy on validation set is fluctuating which suggests that the network can be improved further. Let's predict and measure the accuracy of current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6baf8a37fd5669f80b314ac935e99ca0645f5c7a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(xtest)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da6afd196d26c057eff415495b7debb2326c9731",
    "collapsed": true
   },
   "source": [
    "The model is not accurate at this point of time, But it can be improved further with architectural changes and hyperparameter tuning. \n",
    "\n",
    "## Use Case : Drug Discovery using 3D CNN\n",
    "\n",
    "> In Progress\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
