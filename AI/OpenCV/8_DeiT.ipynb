{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_DeiT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNQehc78D6ApGhh5YKcg7k7"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# DieT"],"metadata":{"id":"MUIvHfRZVsfu"}},{"cell_type":"markdown","source":["Vision transformers (ViT) have been able to achieve state-of-the-art performance on ImageNet without using convolution. Even ViT was only able to achieve this when trained with a large private labelled image dataset using extensive computing resources. In their paper, “Training data-efficient image transformers & distillation through attention”, Hugo Touvron, Matthieu Cord, et al. proposed a convolution-free transformer network, DeiT, that achieves top-1 accuracy of 83.1% on ImageNet with no external data. DeiT introduces a new teacher-student strategy specific to transformers that relies on a distillation token, similar to the class token already employed in transformer networks. "],"metadata":{"id":"tlm6HgSJVquQ"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/introducing-deit-data-efficient-image-transformers/) article."],"metadata":{"id":"xMitR0fAVuhL"}},{"cell_type":"markdown","source":["# Image Classification with a pre-trained DeiT model"],"metadata":{"id":"LKIAg68XV0lF"}},{"cell_type":"markdown","source":["Install PyTorch Image Models (timm) "],"metadata":{"id":"PQ3dqhaBWmg2"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras opencv-python pillow scikit-image torch torchvision \\\n","     tqdm --user -q --no-warn-script-location\n","\n","!python -m pip install timm==0.3.2 --user -q\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"Ssz9YmCpWmQa"}},{"cell_type":"markdown","source":["Download ImageNet class labels and create a list."],"metadata":{"id":"hzKtz0ZpWqa8"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n","# Read the ImageNet categories\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    imagenet_categories = [s.strip() for s in f.readlines()] "],"outputs":[],"metadata":{"id":"NanbClycVhlh"}},{"cell_type":"markdown","source":["Import necessary libraries and classes"],"metadata":{"id":"7A4KrbXpWyqE"}},{"cell_type":"code","execution_count":null,"source":["from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format = 'retina'\n","import torch\n","import timm\n","import torchvision\n","import torchvision.transforms as T\n","from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","torch.set_grad_enabled(False); "],"outputs":[],"metadata":{"id":"uXP4-B02Wtbh"}},{"cell_type":"markdown","source":["Create the data transform expected by DeiT"],"metadata":{"id":"PPXyxSkYW31W"}},{"cell_type":"code","execution_count":null,"source":["transform = T.Compose([\n","  T.Resize(256, interpolation=3),\n","  T.CenterCrop(224),\n","  T.ToTensor(),\n","  T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n","]) "],"outputs":[],"metadata":{"id":"NZCf5Oj8W1FP"}},{"cell_type":"markdown","source":["Load the pre-trained model from TorchHub and get an image to perform inference on."],"metadata":{"id":"3F6OFINyW72z"}},{"cell_type":"code","execution_count":null,"source":["model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n","model.eval();\n","url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n","im = Image.open(requests.get(url, stream=True).raw)\n","# display the image \n","im"],"outputs":[],"metadata":{"id":"Nx0dDRbjW6GH"}},{"cell_type":"markdown","source":["  Transform the image and perform inference."],"metadata":{"id":"SbVWH8VCXAAY"}},{"cell_type":"code","execution_count":null,"source":["# transform the original image and add a batch dimension\n","img = transform(im).unsqueeze(0)\n","\n","# compute the predictions\n","out = model(img)\n","\n","# and convert them into probabilities\n","scores = torch.nn.functional.softmax(out, dim=-1)[0]\n","\n","# get the index of the prediction with highest score\n","topk_scores, topk_label = torch.topk(scores, k=5, dim=-1)\n","for i in range(5):\n","  pred_name = imagenet_categories[topk_label[i]]\n","  print(f\"Prediction index {i}: {pred_name:<25}, score: {topk_scores[i].item():.3f}\")"],"outputs":[],"metadata":{"id":"4wWQuyXxW-PZ"}}]}