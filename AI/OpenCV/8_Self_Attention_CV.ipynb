{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Self_Attention_CV.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+S7q2itK47ZHBIppNPoij"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Self-Attention Computer Vision**\n","\n","Self-Attention Computer Vision, known technically as self_attention_cv, is a PyTorch based library providing a one-stop solution for all of the self-attention based requirements. It includes varieties of self-attention based layers and pre-trained models that can be simply employed in any custom architecture. Rather than building the self-attention layers or blocks from scratch, this library helps its users perform model building in no-time. On the other hand, the pre-trained heavy models such as TransUNet, ViT can be incorporated into custom models and can finish training in minimal time even in a CPU environment!  According to its contributors Adaloglou Nicolas and Sergios Karagiannakos, the library is still under development by updating the latest models and architectures.\n","\n","To read about it more, please refer [this](https://analyticsindiamag.com/pytorch-code-for-self-attention-computer-vision/) article.\n","\n","This notebook has reference from the following sources and papers\n","\n","https://github.com/The-AI-Summer/self-attention-cv\n","https://arxiv.org/pdf/1706.03762.pdf\n","https://analyticsindiamag.com/going-beyond-cnn-stand-alone-self-attention/\n","https://arxiv.org/pdf/2101.11605"],"metadata":{"id":"DXybxfSTjhYS"}},{"cell_type":"markdown","source":["# **Code Implementation**"],"metadata":{"id":"wBXfK4o7gn6e"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras opencv-python pillow scikit-image torch torchvision \\\n","    self_attention_cv --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Multi-head Self Attention"],"metadata":{"id":"mXNuftZNtnZY"}},{"cell_type":"markdown","source":["According to the authors of the paper, Attention Is All You Need,\n","\n","\"\"\"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\"\"\""],"metadata":{"id":"nr5PaMi6gxCF"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","from self_attention_cv import MultiHeadSelfAttention\n","\n","model = MultiHeadSelfAttention(dim=64)\n","x = torch.rand(16, 10, 64)  # [batch, tokens, dim]\n","mask = torch.zeros(10, 10)  # tokens X tokens\n","mask[5:8, 5:8] = 1\n","y = model(x, mask)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first token/patch in the first batch \\n')\n","print(y.detach().numpy()[0][0])\n"],"outputs":[],"metadata":{"id":"aBIJkMtzkDN7"}},{"cell_type":"markdown","source":["## Axial Attention"],"metadata":{"id":"s2Mw4OMatsT-"}},{"cell_type":"markdown","source":["Axial attention is a special kind of self-attention layers collection incorporated in autoregressive models such as Axial Transformers that take high-dimensional data as input such as high-resolution images. The following codes demonstrate Axial attention block implementation with randomly generated image data of size 64 by 64"],"metadata":{"id":"YD-0OvfPg3jm"}},{"cell_type":"code","execution_count":null,"source":["# Axial Attention\n","from self_attention_cv import AxialAttentionBlock\n","\n","model = AxialAttentionBlock(in_channels=256, dim=64, heads=8)\n","x = torch.rand(1, 256, 64, 64)  # [batch, tokens, dim, dim]\n","y = model(x)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first token/patch in the first batch \\n')\n","print(y.detach().numpy()[0][0])"],"outputs":[],"metadata":{"id":"0TljeBmWkGnP"}},{"cell_type":"markdown","source":["## Bottleneck Attention"],"metadata":{"id":"JtOkveV2txCA"}},{"cell_type":"markdown","source":["Bottleneck Transformers employ multi-head self-attention layers in multiple computer vision tasks. The whole transformer block is available as a module in our library. The Bottleneck block is demonstrated in the following codes with randomly generated images of size 32 by 32."],"metadata":{"id":"kFtQfuHcg6qX"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv.bottleneck_transformer import BottleneckBlock\n","x = torch.rand(1, 512, 32, 32)\n","bottleneck_block = BottleneckBlock(in_channels=512, fmap_size=(32, 32), heads=4, out_channels=1024, pooling=True)\n","y = bottleneck_block(x)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first patch in the first head, first batch \\n')\n","print(y.detach().numpy()[0][0][0])"],"outputs":[],"metadata":{"id":"OLXdM48Bs7sX"}},{"cell_type":"markdown","source":["## Transformer Encoder\n"],"metadata":{"id":"QvBX6luZt6mF"}},{"cell_type":"markdown","source":["The encoder part of base Transformer architecture can be simply obtained using the module TransformerEncoder. The following codes demonstrate usage of this module with randomly generated tokens of dimension 64. "],"metadata":{"id":"0BqULMU5g9qM"}},{"cell_type":"code","execution_count":null,"source":["# Transformer Encoder\n","from self_attention_cv import TransformerEncoder\n","\n","model = TransformerEncoder(dim=64,blocks=6,heads=8)\n","x = torch.rand(16, 10, 64)  # [batch, tokens, dim]\n","mask = torch.zeros(10, 10)  # tokens X tokens\n","mask[5:8, 5:8] = 1\n","y = model(x,mask)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first token/patch in the first batch \\n')\n","print(y.detach().numpy()[0][0])"],"outputs":[],"metadata":{"id":"oRWIv_yqryHO"}},{"cell_type":"markdown","source":["## Vision Transformer "],"metadata":{"id":"Guu4CBXat-5_"}},{"cell_type":"markdown","source":["Vision Transformer (ViT) became popular with all kinds of computer vision tasks, achieving state-of-the-art performance in many applications at its publication time. Though few other latest architectures outperform ViT, most of them are built on top of it. The basic ViT is available as a module so that it can be simply used in any custom architecture. The following codes demonstrate the module’s usage with randomly generated 3-channel colored images of ize 256 by 256 in a 10-class classification problem. "],"metadata":{"id":"S8Jq0ozDhAXG"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv import ViT\n","\n","model = ViT(img_dim=256, in_channels=3, patch_dim=16, num_classes=10,dim=512)\n","x = torch.rand(2, 3, 256, 256)\n","y = model(x) # [2,10]\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first image \\n')\n","print(y.detach().numpy()[0])"],"outputs":[],"metadata":{"id":"MwV5vwsDsRR-"}},{"cell_type":"markdown","source":["## Vision Transformer with ResNet50"],"metadata":{"id":"ZX0DmeXVuH3f"}},{"cell_type":"markdown","source":["The Vision Transformer backed with ResNet performs greatly with many of the computer vision tasks. The following codes demonstrate the corresponding module’s usage with randomly generated 3-channel colored images of size 256 by 256 in a 10-class classification problem. "],"metadata":{"id":"EWlZGGhVhDGC"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv import ResNet50ViT\n","\n","model = ResNet50ViT(img_dim=256, pretrained_resnet=False, \n","                        blocks=6, num_classes=10, \n","                        dim_linear_block=256, dim=256)\n","x = torch.rand(2, 3, 256, 256)\n","y = model(x) # [2,10]\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first image \\n')\n","print(y.detach().numpy()[0])"],"outputs":[],"metadata":{"id":"B98nDL3bsU81"}},{"cell_type":"markdown","source":["## TransUNet "],"metadata":{"id":"JeRk_Nw1uMUY"}},{"cell_type":"markdown","source":["TransUNet is the present state-of-the-art architecture in Medical Image Segmentation tasks. This architecture is available as a module in the self_attention_cv library. The following codes demonstrate the module’s usage with randomly generated 3-channel colored images of dimensions 128 by 128. The output of the model built with this module corresponds to the dimensions of the input images."],"metadata":{"id":"MhvwUURwhGPM"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv.transunet import TransUnet\n","x = torch.rand(2, 3, 128, 128)\n","model = TransUnet(in_channels=3, img_dim=128, vit_blocks=8,\n","vit_dim_linear_mhsa_block=512, classes=5)\n","y = model(x) # [2, 5, 128, 128]\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first image \\n')\n","print(y.detach().numpy()[0][0])"],"outputs":[],"metadata":{"id":"46U88N95ssVz"}},{"cell_type":"markdown","source":["## 1D Absolute Positional Embedding"],"metadata":{"id":"cdIe6dDJudyK"}},{"cell_type":"markdown","source":["Two forms of positional embeddings are fed into a self-attention layer to denote memory vectors’ position, namely, absolute positioning and relative positioning. Position-aware self-attention models exhibit memory efficiency and improved performance. Self-attention Computer Vision library has separate modules for absolute and relative position embeddings for 1D and 2D sequential data. The following codes demonstrate application of 1-dimensional absolute positional embedding of tokens of dimension 64 with the corresponding module."],"metadata":{"id":"--tNidkxhIR6"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv.pos_embeddings import AbsPosEmb1D\n","\n","model = AbsPosEmb1D(tokens=20, dim_head=64)\n","# batch heads tokens dim_head\n","x = torch.rand(2, 3, 20, 64)\n","y = model(x)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first token in the first head, first batch \\n')\n","print(y.detach().numpy()[0][0][0])"],"outputs":[],"metadata":{"id":"HrY4aR90s9Bl"}},{"cell_type":"markdown","source":["## 1D Relative Positional Embedding"],"metadata":{"id":"lE0cNWoruasQ"}},{"cell_type":"markdown","source":["Relative positional embedding helps greater performance in Neural Machine Translation compared to absolute positional embedding. The following codes demonstrate the application of 1-dimensional relative positional embedding of tokens of dimension 64 with the corresponding module."],"metadata":{"id":"JTqDgOJghLl3"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv.pos_embeddings import RelPosEmb1D\n","\n","model = RelPosEmb1D(tokens=20, dim_head=64, heads=3)\n","x = torch.rand(2, 3, 20, 64)\n","y = model(x)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first token in the first head, first batch \\n')\n","print(y.detach().numpy()[0][0][0])"],"outputs":[],"metadata":{"id":"ZvBJDQbbtP34"}},{"cell_type":"markdown","source":["## 2D Relative Positional Embedding"],"metadata":{"id":"EkaDlPfPusR7"}},{"cell_type":"markdown","source":["The following codes demonstrate the 2-dimensional relative positional embedding module usage with input feature map patches of dimension 32 by 32."],"metadata":{"id":"0uSBZ906hN_M"}},{"cell_type":"code","execution_count":null,"source":["from self_attention_cv.pos_embeddings import RelPosEmb2D\n","dim = 32  # spatial dim of the feat map\n","model = RelPosEmb2D(\n","    feat_map_size=(dim, dim),\n","    dim_head=128)\n","\n","x = torch.rand(2, 4, dim*dim, 128)\n","y = model(x)\n","\n","print('Shape of output is: ', y.shape)\n","print('-'*70)\n","print('Output corresponding to the first patch in the first head, first batch \\n')\n","print(y.detach().numpy()[0][0][0])"],"outputs":[],"metadata":{"id":"1uw5PnFbtV1T"}}]}