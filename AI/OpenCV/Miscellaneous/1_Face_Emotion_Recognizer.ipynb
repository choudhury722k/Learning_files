{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Face_Emotion_Recognizer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMO1DFwqKlADn4XmL5wnFkv"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **Face Emotion Recognizer**"],"metadata":{"id":"84g0fFDOjhjn"}},{"cell_type":"markdown","source":["Whatever we feel at heart is understood by our facial expressions. Facial expressions are a vital mode of communication. It is said that any person’s behaviour is controlled by his/her face. Social Media to video chat applications our emotions are tracked everywhere. Medical research has also used them widely. Mainly to understand mental health by analyzing emotions. AI has made this possible. Computer Vision has reached new heights from recognising only faces to decoding face gestures. Face recognition has been in this field for ages but have you ever wondered how interesting it would be to decode facial expressions mainly happy, sad, fear, anger, surprise, neutral, disgust. \n","\n","In this session, We’ll be discussing how to create a face emotion recognizer using ‘FER’ library from python."],"metadata":{"id":"E4HSHYImjnWb"}},{"cell_type":"markdown","source":["## **FER library**"],"metadata":{"id":"KHoVOUWejtsd"}},{"cell_type":"markdown","source":["Facial Expression Recognition with a deep neural network using Tensorflow and Keras libraries implemented in python. Dataset used is from Kaggle competition Challenges in Representation Learning: Facial Expression Recognition Challenge. "],"metadata":{"id":"bLe-SImVjw94"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image fer --user -q --no-warn-script-location\n","\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## **Predicting emotions over the static image**"],"metadata":{"id":"q0bVFrBwkA3j"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://www.happierhuman.com/wp-content/uploads/2018/10/how-to-be-happy.jpg"],"outputs":[],"metadata":{"id":"Q1grewkwqc6x"}},{"cell_type":"code","execution_count":null,"source":["from fer import FER\n","import matplotlib.pyplot as plt \n","img = plt.imread(\"how-to-be-happy.jpg\")\n","detector = FER(mtcnn=True)\n","print(detector.detect_emotions(img))\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"Hi4GiKpGnDbs"}},{"cell_type":"markdown","source":["Detector returns a list containing the Ordered dictionary of bounding box notations where the face is detected and all the 7 emotions in decimals values from 0 to 1. The FER contains the Keras model built with convolutional neural networks and weights saved in HDF5 model. Alternatively, there’s Peltarion API which could be used in the backend in place of Keras model. MTCNN stands for multi cascade convolutional network. It is an advanced technique for detecting faces. If mtcnn=False then by default OpenCV Haar Cascade Classifier is used. Lastly, the detect_emotions() function is called to classify the emotion into ‘happy’, ’sad’, ‘disgust’, ‘anger’, ‘fear’, ‘neutral’ with values for each.\n","\n","If we wish to only want the emotion with the highest score we can directly do that with top_emotion() function."],"metadata":{"id":"XkwbA1n_7i1c"}},{"cell_type":"code","execution_count":null,"source":["emotion, score = detector.top_emotion(img)\n","print(emotion,score)"],"outputs":[],"metadata":{"id":"Ygi1u4kY6jzY"}},{"cell_type":"markdown","source":["Lets look at other emotions:"],"metadata":{"id":"DpPj6BTo7k8Q"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://cdn.pixabay.com/photo/2017/07/17/21/59/sad-2514026_1280.jpg"],"outputs":[],"metadata":{"id":"Zi2poIN-8sj6"}},{"cell_type":"code","execution_count":null,"source":["img = plt.imread(\"sad-2514026_1280.jpg\")\n","detector = FER()\n","print(detector.top_emotion(img))\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"okv5xb0M8jym"}},{"cell_type":"markdown","source":["Similarly, the rest of the emotions are shown."],"metadata":{"id":"kDctYAoo9ENs"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://thenakedphysio.files.wordpress.com/2018/10/afraid_of_the_dark_image_via_shutterstock.jpg"],"outputs":[],"metadata":{"id":"SbmtyxK49I6Y"}},{"cell_type":"code","execution_count":null,"source":["img = plt.imread(\"afraid_of_the_dark_image_via_shutterstock.jpg\")\n","detector = FER()\n","print(detector.top_emotion(img))\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"10s-TGdv9Geb"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://i0.wp.com/grahamstoney.com/wp-content/uploads/Angry-Man.jpeg"],"outputs":[],"metadata":{"id":"jYdURyvV9flq"}},{"cell_type":"code","execution_count":null,"source":["img = plt.imread(\"Angry-Man.jpeg\")\n","detector = FER()\n","print(detector.top_emotion(img))\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"FJGZhk1K9iDr"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://image.shutterstock.com/image-photo/close-headshot-young-caucasian-man-260nw-1487254088.jpg"],"outputs":[],"metadata":{"id":"_BLYTMhT9zzO"}},{"cell_type":"code","execution_count":null,"source":["img = plt.imread(\"close-headshot-young-caucasian-man-260nw-1487254088.jpg\")\n","detector = FER()\n","print(detector.top_emotion(img))\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"E7AsvJvn92oq"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://image.shutterstock.com/image-photo/surprised-pretty-young-woman-260nw-531877915.jpg"],"outputs":[],"metadata":{"id":"Xjd8lpNr9-a1"}},{"cell_type":"code","execution_count":null,"source":["img = plt.imread(\"surprised-pretty-young-woman-260nw-531877915.jpg\")\n","detector = FER()\n","print(detector.top_emotion(img))\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"HrEoy-JD-AaD"}},{"cell_type":"markdown","source":["## **Video Analysis**"],"metadata":{"id":"MoJfSUJ9vbs1"}},{"cell_type":"markdown","source":["The ‘fer’ library has a separate module for analysis of facial emotions in videos. This extracts frames and performs emotion analysis using video.analyze() function over detected faces."],"metadata":{"id":"V95HPqMp-NhC"}},{"cell_type":"code","execution_count":null,"source":["# %matplotlib inline\n","# from fer import Video\n","# from fer import FER\n","# import matplotlib.pyplot as plt\n","# import os\n","# import sys\n","# # fn contains the video file name (Ensure that you only upload one file)\n","# videofile = \"test.mp4\"\n","# # Face detection\n","# detector = FER(mtcnn=True)\n","# # Video predictions\n","# video = Video(videofile)\n","# # Output list of dictionaries\n","# raw_data = video.analyze(detector, display=False)"],"outputs":[],"metadata":{"id":"JQuiuoBlvgHe"}},{"cell_type":"code","execution_count":null,"source":["# # Convert to pandas for analysis\n","# df = video.to_pandas(raw_data)\n","# df = video.get_first_face(df)\n","# df = video.get_emotions(df)\n","\n","# # Plot emotions\n","# fig = df.plot(figsize=(20, 16), fontsize=26).get_figure()\n","# # Filename for plot\n","# fig.savefig('my_figure.png')"],"outputs":[],"metadata":{"id":"pA9li7p1w49c"}},{"cell_type":"markdown","source":["## **Storing the emotion and detector face**"],"metadata":{"id":"7rzjesep-5is"}},{"cell_type":"markdown","source":["For detected face bounding boxes is drawn and the emotion having the highest value is shown with brighter text colour than others. "],"metadata":{"id":"0uQmFBel-8ok"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://wp.technologyreview.com/wp-content/uploads/2017/08/andrew-ng-7.jpg"],"outputs":[],"metadata":{"id":"qR7FPWVLCldx"}},{"cell_type":"code","execution_count":null,"source":["import cv2\n","from fer import FER\n","detector = FER(mtcnn=True) \n","image = cv2.imread(\"andrew-ng-7.jpg\")\n","result = detector.detect_emotions(image)"],"outputs":[],"metadata":{"id":"TcRfCmPC--T7"}},{"cell_type":"markdown","source":["As discussed above the result list will return bounding box and emotion values which are stored in separate arrays."],"metadata":{"id":"VCed0r3g_APV"}},{"cell_type":"code","execution_count":null,"source":["bounding_box = result[0][\"box\"]\n","emotions = result[0][\"emotions\"]\n","#Bounding around face is drawn\n","cv2.rectangle(image,(bounding_box[0], bounding_box[1]),\n","(bounding_box[0] + bounding_box[2], bounding_box[1] + bounding_box[3]),(0, 155, 255), 2,)"],"outputs":[],"metadata":{"id":"Oyjz_GrT_C-G"}},{"cell_type":"markdown","source":["Now the emotion along with scores are highlighted"],"metadata":{"id":"mKElk5_7_LyO"}},{"cell_type":"code","execution_count":null,"source":["for idx, (emotion, score) in enumerate(emotions.items()):\n","    color = (211, 211, 211) if score < 0.01 else (255, 0, 0)\n","    emotion_score = \"{}: {}\".format(\n","          emotion, \"{:.2f}\".format(score) if score > 0.01 else \"\"\n","        )\n","    cv2.putText(image,emotion_score,\n","            (bounding_box[0], bounding_box[1] + bounding_box[3] + 30 + idx * 15),cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA,)\n","# cv2.imwrite(\"emotion.jpg\", image)"],"outputs":[],"metadata":{"id":"WvuNeUKj_NeN"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(20,18), dpi=80)\n","img = plt.imread(\"emotion.jpg\")\n","plt.imshow(img)"],"outputs":[],"metadata":{"id":"qDWgNWd5Czsq"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Face Emotion Recognizer](https://analyticsindiamag.com/face-emotion-recognizer-in-6-lines-of-code/)\n","\n","> * [Sign Language Classification using CNN](https://analyticsindiamag.com/hands-on-guide-to-sign-language-classification-using-cnn/)\n","\n","> * [Transfer Learning for multi class classification](https://analyticsindiamag.com/transfer-learning-for-multi-class-image-classification-using-deep-convolutional-neural-network/)\n","\n","> * [FastAI with PyTorch for multiclass Image Classification](https://analyticsindiamag.com/fastai-with-tpu-in-pytorch-for-multiclass-image-classification/)\n","\n","> * [SuffleNet V1 for Multiclass Image Classification](https://analyticsindiamag.com/complete-guide-to-shufflenet-v1-with-implementation-in-multiclass-image-classification/)\n","\n","> * [Image Compression using K-Means Clustering](https://analyticsindiamag.com/beginners-guide-to-image-compression-using-k-means-clustering/)\n"],"metadata":{"id":"1ZtPNx78qe5T"}}]}