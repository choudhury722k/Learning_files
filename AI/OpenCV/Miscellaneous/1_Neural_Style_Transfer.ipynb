{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Neural_Style_Transfer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyML2nQk6Y3Ce2PgbH9wKAdU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Neural Style Transfer**"],"metadata":{"id":"kbqCUhxdaDOG"}},{"cell_type":"markdown","source":["Neural Style Transfer is one of the interesting applications of computer vision using deep learning. In this method, two images named as original content images and the style reference images are blended together by the algorithms. This blending is done in such a way that the resulting image looks like the original content image but painted in the style of the style reference image. This style transfer task is performed by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using the feature extraction capabilities of the convolutional neural network."],"metadata":{"id":"gNuPZwZPaGv3"}},{"cell_type":"markdown","source":["There are several deep learning methods for the neural style transfer in images. But they require a heavy execution environment and consume much computational resources and time. The TensorFlow Hub module provides a class of pre-trained machine learning modules in order to save computational resources and time. It also provides the pre-trained deep convolutional neural network for style transfer in images. \n","\n","In this practice, we present a very fast and effective way to neural style transfer in images using the TensorFlow Hub module. The TF-Hub module provides the pre-trained VGG Deep Convolutional Neural Network for style transfer. This approach takes less than four seconds to transfer style to a content image."],"metadata":{"id":"_W_74KRGayZ8"}},{"cell_type":"markdown","source":["# **Implementation**"],"metadata":{"id":"HyHJjBnha7ky"}},{"cell_type":"markdown","source":["First of all, we will import the required libraries. To use the TensorFlow hub, we will import it as hub from tensorflow_hub. The functool library is used for higher-order functions that act on or return other functions. Make sure to install the if you are working on the local system. "],"metadata":{"id":"6ehXudgKbAiG"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image pillow face_recognition --user -q --no-warn-script-location\n","\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import functools\n","import os\n","from matplotlib import gridspec\n","import matplotlib.pylab as plt\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_hub as hub"],"outputs":[],"metadata":{"id":"81GzJXikXO87"}},{"cell_type":"markdown","source":["After importing, we will check the versions. tf.executing_eagerly() checks whether the current thread has eager execution enabled."],"metadata":{"id":"JYPNxkPFbK1F"}},{"cell_type":"code","execution_count":null,"source":["print(\"TF Version: \", tf.__version__)\n","print(\"TF-Hub version: \", hub.__version__)\n","print(\"Eager mode enabled: \", tf.executing_eagerly())\n","#Check if GPU available\n","print(\"GPU available: \", tf.test.is_gpu_available())"],"outputs":[],"metadata":{"id":"bqRw0bYbbMdf"}},{"cell_type":"markdown","source":["Using the below lines of codes the functions to load, format, preprocess and visualize images are defined. content_image, style_image, and stylized_image are expected to be 4-D Tensors with shapes [batch_size, image_height, image_width, 3]"],"metadata":{"id":"TccokFonbWmv"}},{"cell_type":"code","execution_count":null,"source":["def crop_center(image):\n","  \"\"\"Returns a cropped square image.\"\"\"\n","  shape = image.shape\n","  new_shape = min(shape[1], shape[2])\n","  offset_y = max(shape[1] - shape[2], 0) // 2\n","  offset_x = max(shape[2] - shape[1], 0) // 2\n","  image = tf.image.crop_to_bounding_box(\n","      image, offset_y, offset_x, new_shape, new_shape)\n","  return image\n","\n","\n","@functools.lru_cache(maxsize=None)\n","def load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):\n","  \"\"\"Loads and preprocesses images.\"\"\"\n","  # Cache image file locally.\n","  image_path = tf.keras.utils.get_file(os.path.basename(image_url)[-128:], image_url)\n","  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n","  img = plt.imread(image_path).astype(np.float32)[np.newaxis, ...]\n","  if img.max() > 1.0:\n","    img = img / 255.\n","  if len(img.shape) == 3:\n","    img = tf.stack([img, img, img], axis=-1)\n","  img = crop_center(img)\n","  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n","  return img\n","\n","\n","def show_n(images, titles=('',)):\n","  n = len(images)\n","  image_sizes = [image.shape[1] for image in images]\n","  w = (image_sizes[0] * 6) // 320\n","  plt.figure(figsize=(w  * n, w))\n","  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n","  for i in range(n):\n","    plt.subplot(gs[i])\n","    plt.imshow(images[i][0], aspect='equal')\n","    plt.axis('off')\n","    plt.title(titles[i] if len(titles) > i else '')\n","  plt.show()"],"outputs":[],"metadata":{"id":"QfDWb9VfbZR_"}},{"cell_type":"markdown","source":["Now, we will define the required sizes for the content images and style images. "],"metadata":{"id":"vTyerezIbbzU"}},{"cell_type":"code","execution_count":null,"source":["output_image_size = 384 \n","content_img_size = (output_image_size, output_image_size)\n","style_img_size = (256, 256)"],"outputs":[],"metadata":{"id":"5HEUDsRMbeVG"}},{"cell_type":"markdown","source":["In the next step, we will load the content and the style images and visualize them."],"metadata":{"id":"sh8R74uhbgCg"}},{"cell_type":"code","execution_count":null,"source":["content_image_url = 'https://d16yj43vx3i1f6.cloudfront.net/uploads/2019/10/GettyImages-803849852.jpg'\n","style_image_url = 'https://vertexpages.com/wp-content/uploads/2019/10/farm.jpg'\n","\n","content_image = load_image(content_image_url, content_img_size)\n","style_image = load_image(style_image_url, style_img_size)"],"outputs":[],"metadata":{"id":"ocGiv-OCbjDL"}},{"cell_type":"code","execution_count":null,"source":["style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n","show_n([content_image, style_image], ['Content image', 'Style image'])"],"outputs":[],"metadata":{"id":"UWUmGYcJblXE"}},{"cell_type":"markdown","source":["The below lines of codes will load the TF-HUB module. "],"metadata":{"id":"SPpIMM23bnpD"}},{"cell_type":"code","execution_count":null,"source":["import time\n","start_time = time.time()\n","\n","hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n","hub_module = hub.load(hub_handle)"],"outputs":[],"metadata":{"id":"9xYxPLzmbreO"}},{"cell_type":"markdown","source":["\n","## **Neural Style Transfer**\n","\n","Using the blow lines of codes, the style is transferred using the HUB module and the output image is generated."],"metadata":{"id":"I8ef9jlsbveh"}},{"cell_type":"code","execution_count":null,"source":["outputs = hub_module(content_image, style_image)\n","stylized_image = outputs[0]\n","\n","# Stylize content image with a given style image.\n","# This is pretty fast within a few milliseconds on a GPU.\n","\n","outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n","stylized_image = outputs[0]"],"outputs":[],"metadata":{"id":"ck6qYrdnb06B"}},{"cell_type":"markdown","source":["Once the style is transferred to the original image, we will visualize it all together using the below lines of codes."],"metadata":{"id":"CFkUobUob25s"}},{"cell_type":"code","execution_count":null,"source":["# Visualize input images and the generated stylized image.\n","\n","show_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])"],"outputs":[],"metadata":{"id":"nYLqELvIb5Me"}},{"cell_type":"code","execution_count":null,"source":["end_time = time.time()\n","print('Time Taken = ', end_time-start_time)"],"outputs":[],"metadata":{"id":"7-SoJepUb7xV"}},{"cell_type":"markdown","source":["The above steps will be repeated once more for another style transfer example.\n"],"metadata":{"id":"FRMFCL8Pb-Hg"}},{"cell_type":"code","execution_count":null,"source":["content_image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Taj_Mahal_%28Edited%29.jpeg/1920px-Taj_Mahal_%28Edited%29.jpeg'\n","style_image_url = 'https://joeburciaga.files.wordpress.com/2013/02/tsunami-2698.jpg'\n","\n","content_image = load_image(content_image_url, content_img_size)\n","style_image = load_image(style_image_url, style_img_size)"],"outputs":[],"metadata":{"id":"y_fZENVhcABx"}},{"cell_type":"code","execution_count":null,"source":["style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n","show_n([content_image, style_image], ['Content image', 'Style image'])\n"],"outputs":[],"metadata":{"id":"AqlkahKbcB8U"}},{"cell_type":"code","execution_count":null,"source":["start_time = time.time()\n","\n","hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n","hub_module = hub.load(hub_handle)\n","\n","outputs = hub_module(content_image, style_image)\n","stylized_image = outputs[0]\n","\n","# Stylize content image with a given style image.\n","# This is pretty fast within a few milliseconds on a GPU.\n","\n","outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n","stylized_image = outputs[0]\n","\n","# Visualize input images and the generated stylized image.\n","show_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])\n"],"outputs":[],"metadata":{"id":"CyjETshgcEf9"}},{"cell_type":"code","execution_count":null,"source":["end_time = time.time()\n","print('Time Taken = ', end_time-start_time)"],"outputs":[],"metadata":{"id":"7-X997rocNFm"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Neural Style Transfer](https://analyticsindiamag.com/hands-on-guide-to-neural-style-transfer-using-tensorflow-hub-module/)\n","\n","> * [ResNet50 in PyTorch](https://analyticsindiamag.com/hands-on-guide-to-implement-resnet50-in-pytorch-with-tpu/)\n","\n","> * [CNN Model â€“ To Count Fingers](https://analyticsindiamag.com/how-to-implement-cnn-model-to-count-fingers-and-distinguish-between-left-and-right-hand/)\n","\n","> * [Emotion Detection](https://analyticsindiamag.com/my-first-cnn-project-emotion-detection-using-convolutional-neural-network-with-tpu/)\n","\n","> * [Roboflow](https://analyticsindiamag.com/step-by-step-guide-to-object-detection-using-roboflow/)\n","\n","> * [Capsule Network](https://analyticsindiamag.com/understanding-capsule-net-with-its-implementation-in-computer-vision/)\n","\n","> * [Face Attendance System](https://analyticsindiamag.com/a-complete-guide-on-building-a-face-attendance-system/)"],"metadata":{"id":"1ZtPNx78qe5T"}}]}