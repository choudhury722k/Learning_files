{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_HiSD_Image_to_Image_translation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOITHPJ2ggdU2DxPvI9+Nwz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Image-to-image translation process via Hierarchical Style Disentanglement (HiSD)**"],"metadata":{"id":"Nwul_Jl2yTGt"}},{"cell_type":"markdown","source":["The image-to-Image translation is a field in the computer vision domain that deals with generating a modified image from the original input image based on certain conditions. The conditions can be multi-labels or multi-styles, or both. In recent successful methods, translation of the input image is performed based on the multi-labels and the generation of output image out of the translated feature map is performed based on the multi-styles. The labels and styles are fed to the models via texts or reference images. The translation sometimes takes unnecessary manipulations and alterations in identity attributes that are difficult to control in a semi-supervised setting."],"metadata":{"id":"KcoIhgDjygqe"}},{"cell_type":"markdown","source":["To know about it more, please refer [here](https://analyticsindiamag.com/hisd-python-implementation-of-image-to-image-translation/)."],"metadata":{"id":"YoMcit5Dyhs5"}},{"cell_type":"markdown","source":["## **Python implementation of HiSD**"],"metadata":{"id":"GJMLP1jSyptD"}},{"cell_type":"markdown","source":["HiSD needs a Python environment and PyTorch framework to set up and run. Usage of a GPU runtime is optional. Pre-trained HiSD can be loaded and inference may be performed with a CPU runtime itself. Install dependencies using the following command."],"metadata":{"id":"qpyqUcdhytPr"}},{"cell_type":"markdown","source":["# HiSD\n","\n","### Hierarchical Style Disentanglement\n","\n","References:\n","\n","https://github.com/imlixinyang/HiSD\n","\n","https://arxiv.org/abs/2103.01456\n","\n","According to the authors of this research paper, the code is meant only for academic and research purpose. "],"metadata":{"id":"idGjxUg8GmFY"}},{"cell_type":"markdown","source":["## Load pre-trained model checkpoint from offcial Google Drive page\n","\n","checkpoint_256_celeba-hq.pt"],"metadata":{"id":"SUGip-body95"}},{"cell_type":"markdown","source":["Download the checkpoint parquet file from the official page using the following command."],"metadata":{"id":"6NA9BSBCJcFw"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image --user -q --no-warn-script-location\n","\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# id = '1KDrNWLejpo02fcalUOrAJOl1hGoccBKl'\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1KDrNWLejpo02fcalUOrAJOl1hGoccBKl' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1KDrNWLejpo02fcalUOrAJOl1hGoccBKl\" -O checkpoint_256_celeba-hq.pt && rm -rf /tmp/cookies.txt"],"outputs":[],"metadata":{"id":"W3y4aHnxcSIx"}},{"cell_type":"markdown","source":["Move the checkpoint parquet file to the /HiSD directory using the following commands."],"metadata":{"id":"HF5apYsFJgGV"}},{"cell_type":"code","execution_count":null,"source":["checkpoint_name = 'checkpoint_256_celeba-hq.pt'"],"outputs":[],"metadata":{"id":"LORxflUFeHs1"}},{"cell_type":"code","execution_count":null,"source":["!ls"],"outputs":[],"metadata":{"id":"-la_0eAbePJ_"}},{"cell_type":"markdown","source":["## Clone source code and install dependencies"],"metadata":{"id":"8oU7HfMyqehR"}},{"cell_type":"code","execution_count":null,"source":["!pip install tensorboardx"],"outputs":[],"metadata":{"id":"iEjWyICjqdi0"}},{"cell_type":"markdown","source":["The following command downloads the source codes from the official repository to the local machine. "],"metadata":{"id":"IHByav8M9jh9"}},{"cell_type":"code","execution_count":null,"source":["!git clone https://github.com/imlixinyang/HiSD.git"],"outputs":[],"metadata":{"id":"lXzqCBaGGh5w"}},{"cell_type":"code","execution_count":null,"source":["!mv checkpoint_256_celeba-hq.pt HiSD/"],"outputs":[],"metadata":{"id":"nkRiZavoqQMV"}},{"cell_type":"code","execution_count":null,"source":["%cd HiSD/"],"outputs":[],"metadata":{"id":"UOGK3yXMG4py"}},{"cell_type":"code","execution_count":null,"source":["!ls"],"outputs":[],"metadata":{"id":"7Iz4dS5ZL4sU"}},{"cell_type":"markdown","source":["## Prepare the environment"],"metadata":{"id":"WT51uaKMqvoJ"}},{"cell_type":"code","execution_count":null,"source":["%cd HiSD/\n","from core.utils import get_config\n","from core.trainer import HiSD_Trainer\n","import argparse\n","import torchvision.utils as vutils\n","import sys\n","import torch\n","import os\n","from torchvision import transforms\n","from PIL import Image\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"id":"qWqJBsMXRTn4"}},{"cell_type":"code","execution_count":null,"source":["## Load model and its checkpoint"],"outputs":[],"metadata":{"id":"Gcg0gSevq1aG"}},{"cell_type":"markdown","source":["Load the checkpoint and prepare the model for inference using the following codes."],"metadata":{"id":"bRN0S6YhJixU"}},{"cell_type":"code","execution_count":null,"source":["device = 'cpu'\n","# load checkpoint\n","config = get_config('configs/celeba-hq_256.yaml')\n","noise_dim = config['noise_dim']\n","image_size = config['new_size']\n","checkpoint = 'checkpoint_256_celeba-hq.pt'\n","trainer = HiSD_Trainer(config)\n","# map_location=torch.device('cpu')\n","state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))\n","trainer.models.gen.load_state_dict(state_dict['gen_test'])\n","trainer.models.gen.to(device)\n","\n","E = trainer.models.gen.encode\n","T = trainer.models.gen.translate\n","G = trainer.models.gen.decode\n","M = trainer.models.gen.map\n","F = trainer.models.gen.extract\n","\n","transform = transforms.Compose([transforms.Resize(image_size),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"],"outputs":[],"metadata":{"id":"8nLKagcsSM7v"}},{"cell_type":"markdown","source":["## Define a helper function to perform inference"],"metadata":{"id":"Te5kgO_Iq699"}},{"cell_type":"markdown","source":["Define a function to perform the image-to-image translation."],"metadata":{"id":"697aRnQBJnMb"}},{"cell_type":"code","execution_count":null,"source":["def translate(input, steps):\n","    x = transform(Image.open(input).convert('RGB')).unsqueeze(0).to(device)\n","    c = E(x)\n","    c_trg = c\n","    for j in range(len(steps)):\n","        step = steps[j]\n","        if step['type'] == 'latent-guided':\n","            if step['seed'] is not None:\n","                torch.manual_seed(step['seed'])\n","                torch.cuda.manual_seed(step['seed']) \n","\n","            z = torch.randn(1, noise_dim).to(device)\n","            s_trg = M(z, step['tag'], step['attribute'])\n","\n","        elif step['type'] == 'reference-guided':\n","            reference = transform(Image.open(step['reference']).convert('RGB')).unsqueeze(0).to(device)\n","            s_trg = F(reference, step['tag'])\n","        \n","        c_trg = T(c_trg, s_trg, step['tag'])\n","            \n","    x_trg = G(c_trg)\n","    output = x_trg.squeeze(0).cpu().permute(1, 2, 0).add(1).mul(1/2).clamp(0,1).detach().numpy()\n","    return output"],"outputs":[],"metadata":{"id":"2C9heJvYSxVG"}},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"6cdAba8NrAZF"}},{"cell_type":"markdown","source":["The following commands set the desired tags, the attributes and the styles to perform translation. They use in-built example images for translation. Users can opt for their own data images."],"metadata":{"id":"lJWK0p1LJr4H"}},{"cell_type":"code","execution_count":null,"source":["input = 'examples/input_0.jpg'\n","\n","# e.g.1 change tag 'Bangs' to attribute 'with' using 3x latent-guided styles (generated by random noise). \n","steps = [\n","    {'type': 'latent-guided', 'tag': 0, 'attribute': 0, 'seed': None}\n","]\n","plt.figure(figsize=(12,4))\n","for i in range(3):\n","    plt.subplot(1, 3, i+1)\n","    output = translate(input, steps)\n","    plt.imshow(output, aspect='auto')\n","plt.show()"],"outputs":[],"metadata":{"id":"hU6ZMaKWTdLm"}},{"cell_type":"markdown","source":["Second example inference:"],"metadata":{"id":"S-ey1oEUJvza"}},{"cell_type":"code","execution_count":null,"source":["input = 'examples/input_1.jpg'\n","plt.figure(figsize=(12,4))\n","# e.g.2 change tag 'Glasses' to attribute 'with' using reference-guided styles (extracted from another image). \n","steps = [\n","    {'type': 'reference-guided', 'tag': 1, 'reference': 'examples/reference_glasses_0.jpg'}\n","]\n","\n","output = translate(input, steps)\n","plt.subplot(131)\n","plt.imshow(output, aspect='auto')\n","\n","steps = [\n","    {'type': 'reference-guided', 'tag': 1, 'reference': 'examples/reference_glasses_1.jpg'}\n","]\n","\n","output = translate(input, steps)\n","plt.subplot(132)\n","plt.imshow(output, aspect='auto')\n","\n","steps = [\n","    {'type': 'reference-guided', 'tag': 1, 'reference': 'examples/reference_glasses_2.jpg'}\n","]\n","\n","output = translate(input, steps)\n","plt.subplot(133)\n","plt.imshow(output, aspect='auto')\n","plt.show()"],"outputs":[],"metadata":{"id":"L9G0POtWkHIV"}},{"cell_type":"markdown","source":["Third example inference:"],"metadata":{"id":"llMlt98xJyN5"}},{"cell_type":"code","execution_count":null,"source":["input = 'examples/input_2.jpg'\n","\n","# e.g.3 change tag 'Glasses' and 'Bangs 'to attribute 'with', 'Hair color' to 'black' during one translation. \n","steps = [\n","    {'type': 'reference-guided', 'tag': 0, 'reference': 'examples/reference_bangs_0.jpg'},\n","    {'type': 'reference-guided', 'tag': 1, 'reference': 'examples/reference_glasses_0.jpg'},\n","    {'type': 'latent-guided', 'tag': 2, 'attribute': 0, 'seed': None}\n","]\n","\n","output = translate(input, steps)\n","plt.figure(figsize=(5,5))\n","plt.imshow(output, aspect='auto')\n","plt.show()"],"outputs":[],"metadata":{"id":"7_lqbUU9k2yy"}},{"cell_type":"markdown","source":["Thank you for your time!"],"metadata":{"id":"egNp_Wlorlwn"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Image to Image Translation](https://analyticsindiamag.com/hisd-python-implementation-of-image-to-image-translation/)\n","\n","> * [Guide to Kornia](https://analyticsindiamag.com/guide-to-kornia-an-opencv-inspired-pytorch-framework/)\n","\n","> * [Extract Foreground Images with GrabCut Algorithm](https://analyticsindiamag.com/how-to-extract-foreground-from-images-interactively-using-grabcut/)\n","\n","> * [GAN in simple 8 Steps](https://analyticsindiamag.com/how-to-build-a-generative-adversarial-network-in-8-simple-steps/)\n","\n","> * [PyTorch vs Keras vs Caffe](https://analyticsindiamag.com/keras-vs-pytorch-vs-caffe-comparing-the-implementation-of-cnn/)\n","\n","> * [Face Emotion Recognizer](https://analyticsindiamag.com/face-emotion-recognizer-in-6-lines-of-code/)\n","\n","> * [Sign Language Classification using CNN](https://analyticsindiamag.com/hands-on-guide-to-sign-language-classification-using-cnn/)\n","\n","\n"],"metadata":{"id":"1ZtPNx78qe5T"}}]}