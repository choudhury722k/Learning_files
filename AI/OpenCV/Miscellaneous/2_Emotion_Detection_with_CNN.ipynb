{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_Emotion_Detection_with_CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHpN4TTnq3Hf0EIq3QV2uz"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **Emotion Detection Using Convolutional Neural Network**"],"metadata":{"id":"ZkG4PIcLcBqF"}},{"cell_type":"markdown","source":["Computer vision (CV) is the field of study that helps computers to study using different techniques and methods so that it can capture what exists in an image or a video. There are a large number of applications of computer vision that are present today like facial recognition, driverless cars, medical diagnostics, etc. We will discuss one of the interesting applications of CV that is Emotion Detection through facial expressions. CV can recognize and tell you what your emotion is by just looking at your facial expressions. It can detect whether you are angry, happy, sad, etc."],"metadata":{"id":"2OCNBMkXcE9O"}},{"cell_type":"markdown","source":["In this practice session, we will demonstrates a computer vision model that we will build using Keras and VGG16 – a variant of Convolutional Neural Network. We will use this model to check the emotions in real-time using OpenCV and webcam. We will be working with Google Colab to build the model as it gives us the GPU and TPU. You can use any other IDE as well."],"metadata":{"id":"KXG4-MDgcFtY"}},{"cell_type":"markdown","source":["## **The Dataset**\n","\n","The name of the data set is [fer2013](https://www.kaggle.com/deadskull7/fer2013) which is an open-source data set that was made publicly available for a Kaggle competition. It contains 48 X 48-pixel grayscale images of the face. There are seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) present in the data. The CSV file contains two columns that are emotion that contains numeric code from 0-6 and a pixel column that includes a string surrounded in quotes for each image."],"metadata":{"id":"h15vWPyFcLlH"}},{"cell_type":"markdown","source":["## **Implementing VGG16 Network for Classification of Emotions**"],"metadata":{"id":"QG8JIVtKcShM"}},{"cell_type":"markdown","source":["Import the required libraries for building the network. The code for importing the libraries is given below."],"metadata":{"id":"AUcxil49cWQD"}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers.core import Flatten, Dense, Dropout\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n","from tensorflow.keras.optimizers import SGD\n","import cv2\n","from keras.utils import np_utils"],"outputs":[],"metadata":{"id":"yMy4DsxWb4Mo"}},{"cell_type":"markdown","source":["We have now imported all the libraries and now we will import the data set. I have already saved it in my drive so I will read it from there. You can give directory in round brackets where your data is stored as shown in the code below. After importing we have printed the data frame as shown in the image."],"metadata":{"id":"yUBu_xAfcckR"}},{"cell_type":"code","execution_count":null,"source":["emotion_data = pd.read_csv('https://gitlab.com/AnalyticsIndiaMagazine/practicedatasets/-/raw/main/emotion_detection/fer2013.csv')\n","print(emotion_data)"],"outputs":[],"metadata":{"id":"DBCrIxILdEHy"}},{"cell_type":"markdown","source":["We then create different lists of storing the testing and training image pixels. After this, we check if the pixel belongs to training then we append it into the training list & training labels. Similarly, for pixels belonging to the Public test, we append it to testing lists. The code for the same is shown below."],"metadata":{"id":"f8juuQjrdGfG"}},{"cell_type":"code","execution_count":null,"source":["X_train = []\n","y_train = []\n","X_test = []\n","y_test = []\n","for index, row in emotion_data.iterrows():\n","    k = row['pixels'].split(\" \")\n","    k = list(map(float, k))\n","    if row['Usage'] == 'Training':\n","        X_train.append(np.array(k))\n","        y_train.append(row['emotion'])\n","    elif row['Usage'] == 'PublicTest':\n","        X_test.append(np.array(k))\n","        y_test.append(row['emotion'])"],"outputs":[],"metadata":{"id":"xlDVX3TTdIfj"}},{"cell_type":"markdown","source":["Once we have added the pixel to the lists then we convert them into NumPy arrays and reshape X_train, X_test. After doing this we convert the training labels and testing labels into categorical ones. The code of the same is given below."],"metadata":{"id":"wdRz9n4ydKob"}},{"cell_type":"code","execution_count":null,"source":["X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","X_test = np.array(X_test)\n","y_test = np.array(y_test)\n","\n","X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n","X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n","\n","y_train= np_utils.to_categorical(y_train, num_classes=7)\n","y_test = np_utils.to_categorical(y_test, num_classes=7)"],"outputs":[],"metadata":{"id":"kvKO9VgadMa3"}},{"cell_type":"markdown","source":["**VGG16 Model for Emotion Detection**\n","\n","Now it’s time to design the CNN model for emotion detection with different layers. We start with the initialization of the model followed by batch normalization layer and then different convents layers with ReLu as an activation function, max pool layers, and dropouts to do learning efficiently. You can also change the architecture by initiating the layers of your choices with different numbers of neurons and activation functions."],"metadata":{"id":"dvMtUkZLdQiS"}},{"cell_type":"code","execution_count":null,"source":["model = Sequential()\n","model.add(ZeroPadding2D((1,1),input_shape=(48,48,1)))\n","model.add(Convolution2D(64, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(64, 3, 3, activation='relu'))\n","model.add(MaxPooling2D((2,2),strides=(2,2), padding='same'))\n","\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(128, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(128, 3, 3, activation='relu'))\n","model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n","\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(256, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(256, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(256, 3, 3, activation='relu'))\n","model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n","\n","\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(512, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(512, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(512, 3, 3, activation='relu'))\n","model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n","\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(512, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(512, 3, 3, activation='relu'))\n","model.add(ZeroPadding2D((1,1)))\n","model.add(Convolution2D(512, 3, 3, activation='relu'))\n","model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n","\n","model.add(Flatten())\n","model.add(Dense(4096, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(4096, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(7, activation='softmax'))"],"outputs":[],"metadata":{"id":"pHbBQ1B4dUpJ"}},{"cell_type":"markdown","source":["After this, we compile the model using Adam as an optimizer, loss as categorical cross-entropy, and metrics as accuracy as shown in the below code."],"metadata":{"id":"jTigA_1vd6cX"}},{"cell_type":"code","execution_count":null,"source":["model.compile(loss='categorical_crossentropy',\n","            optimizer='adam',\n","            metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"Opp3T6P7d8Ww"}},{"cell_type":"code","execution_count":null,"source":["print(model.summary())"],"outputs":[],"metadata":{"id":"O52tIfttlw0U"}},{"cell_type":"markdown","source":["After compiling the model we then fit the data for training and validation. Here, we are taking the batch size to be 32 with 30 epochs. You can tune them according to your wish."],"metadata":{"id":"cFf1wO22d7vq"}},{"cell_type":"code","execution_count":12,"source":["model.fit(X_train,y_train,batch_size=32,epochs=30,verbose=1,validation_data=(X_test, y_test))"],"outputs":[{"output_type":"stream","name":"stdout","text":["851/898 [===========================>..] - ETA: 58s - loss: 1.8129 - accuracy: 0.2515"]}],"metadata":{"id":"FNjSAXQzevXi"}},{"cell_type":"markdown","source":["Once the training has been done we can evaluate the model and compute loss and accuracy using the below code. "],"metadata":{"id":"BqD5ElMxfHYu"}},{"cell_type":"code","execution_count":null,"source":["loss_and_metrics = model.evaluate(X_test,y_test)\n","print(loss_and_metrics)"],"outputs":[],"metadata":{"id":"EBj1_isMfJLz"}},{"cell_type":"markdown","source":["We now serialize the model to JSON and save the model weights in an hd5 file so that we can make use of this file to make predictions rather than training the network again. You can do this task by using the below code."],"metadata":{"id":"AhbevYD7fK4n"}},{"cell_type":"code","execution_count":null,"source":["model_json = model.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","  json_file.write(model_json)\n","model.save_weights(\"model.h5\")\n","print(\"Saved model to disk\")"],"outputs":[],"metadata":{"id":"cmPyvqVvfMnN"}},{"cell_type":"markdown","source":["**Testing the model in Real-time using OpenCV and WebCam**"],"metadata":{"id":"vlPpsFm6fOHN"}},{"cell_type":"markdown","source":["Now we will test the model that we build for emotion detection in real-time using OpenCV and webcam. To do so we will write a python script. We will use the Jupyter notebook in our local system to make use of a webcam. You can use other IDEs as well. First, we will install a few libraries that are required. Use the below code to import those all."],"metadata":{"id":"LLgWtDMbfSII"}},{"cell_type":"code","execution_count":null,"source":["import os\n","import cv2\n","import numpy as np\n","from keras.models import model_from_json\n","from keras.preprocessing import image"],"outputs":[],"metadata":{"id":"3KtTKrMXfReU"}},{"cell_type":"markdown","source":["After importing all the required libraries we will load the model weights that we saved earlier after training. Use the below code to load your saved model. After importing the model weights we have imported a haar cascade file that is designed by open cv to detect the frontal face."],"metadata":{"id":"eW32bwqvfVG-"}},{"cell_type":"code","execution_count":null,"source":["model = model_from_json(open(\"model.json\", \"r\").read())\n","model.load_weights('model.h5')\n","face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"],"outputs":[],"metadata":{"id":"nxcJ7PSIfYOl"}},{"cell_type":"markdown","source":["After importing the haar cascade file we will have written a code to detect faces and classify the desired emotions. We have assigned the labels that will be different emotions like angry, happy, sad, surprise, neutral. It will then detect the face of the person, draw a bounding box over the detected person, and then convert the RGB image into grayscale & classify it in real-time. Please refer to the below code for the same and sample outputs that are shown in the images. \n","\n"],"metadata":{"id":"pznP_hemfaPL"}},{"cell_type":"markdown","source":["Downloading images for testing"],"metadata":{"id":"NpWt_3GuF_8M"}},{"cell_type":"code","execution_count":null,"source":["# !wget -O angry.jpg https://image.shutterstock.com/image-photo/portrait-young-angry-man-260nw-157245086.jpg"],"outputs":[],"metadata":{"id":"CyAp_KBIGKS-"}},{"cell_type":"code","execution_count":null,"source":["# !wget -O surprise.jpg https://www.allbusiness.com/asset/2017/06/Surprised-girl-in-the-shop-300x235.jpg"],"outputs":[],"metadata":{"id":"kSzSkHmVF_qi"}},{"cell_type":"code","execution_count":null,"source":["# !wget -O happy.jpg https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fwomensmedia%2Ffiles%2F2018%2F07%2FPhoto-happy-1-unsplash-michael-dam.jpg"],"outputs":[],"metadata":{"id":"i85UbfpmHAVn"}},{"cell_type":"code","execution_count":null,"source":["# !wget -O neutral.jpg https://image.shutterstock.com/image-photo/close-headshot-young-caucasian-man-260nw-1487254088.jpg"],"outputs":[],"metadata":{"id":"NNR9f-9CHPaU"}},{"cell_type":"code","execution_count":null,"source":["test_images = ['neutral.jpg', 'happy.jpg', 'surprise.jpg', 'angry.jpg']"],"outputs":[],"metadata":{"id":"Z6Y0LaAdHUhe"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","for i in test_images:\n","  #setting image resizing parameters\n","  WIDTH = 48\n","  HEIGHT = 48\n","  x=None\n","  y=None\n","  labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n","\n","  #loading image\n","  full_size_image = cv2.imread(i)\n","  print(\"Image Loaded\")\n","  gray=cv2.cvtColor(full_size_image,cv2.COLOR_RGB2GRAY)\n","  face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","  faces = face.detectMultiScale(gray, 1.3  , 10)\n","\n","  #detecting faces\n","  for (x, y, w, h) in faces:\n","          roi_gray = gray[y:y + h, x:x + w]\n","          cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n","          cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n","          cv2.rectangle(full_size_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n","          #predicting the emotion\n","          yhat= model.predict(cropped_img)\n","          cv2.putText(full_size_image, labels[int(np.argmax(yhat))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n","          print(\"Emotion: \"+labels[int(np.argmax(yhat))])\n","\n","  plt.imshow(full_size_image)\n","  plt.show()"],"outputs":[],"metadata":{"id":"q2JiOemdF_Bl"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Emotion Detection](https://analyticsindiamag.com/my-first-cnn-project-emotion-detection-using-convolutional-neural-network-with-tpu/)\n","\n","> * [Roboflow](https://analyticsindiamag.com/step-by-step-guide-to-object-detection-using-roboflow/)\n","\n","> * [Capsule Network](https://analyticsindiamag.com/understanding-capsule-net-with-its-implementation-in-computer-vision/)\n","\n","> * [Face Attendance System](https://analyticsindiamag.com/a-complete-guide-on-building-a-face-attendance-system/)\n","\n","> * [6 MNIST Image Dataset](https://analyticsindiamag.com/mnist/)\n","\n","> * [Vision Transformers](https://analyticsindiamag.com/hands-on-vision-transformers-with-pytorch/)"],"metadata":{"id":"1ZtPNx78qe5T"}}]}