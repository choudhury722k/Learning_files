{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_LayoutParser_table_OCR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzlwzUlC2U3g1kLsAC6lGC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LayoutParser\n","\n","A Python library for Document Image Analysis (DIA)\n","\n","References:\n","\n","Official Notebook\n","https://github.com/Layout-Parser/layout-parser/blob/master/examples/OCR%20Tables%20and%20Parse%20the%20Output.ipynb\n","\n","Github repo\n","https://github.com/Layout-Parser/layout-parser\n","\n","Research paper\n","https://arxiv.org/abs/2103.15348\n","https://arxiv.org/pdf/2103.15348.pdf\n","\n","\n","Dataset\n","https://arxiv.org/abs/2004.08686\n","\n"],"metadata":{"id":"-wqX0KMy-qPk"}},{"cell_type":"markdown","source":["You can refer [this](https://analyticsindiamag.com/guide-to-layoutparser-a-document-image-analysis-python-library/) article for detailing."],"metadata":{"id":"6WGWekuVBBsO"}},{"cell_type":"markdown","source":["# OCR from Table Document Image"],"metadata":{"id":"FVl9MFNBAebL"}},{"cell_type":"markdown","source":["install LayoutParser library from PyPi package and Tesseract OCR Engine. Install other dependencies."],"metadata":{"id":"QZIqGqWCAnSH"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image pillow --user -q --no-warn-script-location"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["%%bash\n","python -m pip install -U layoutparser --user -q\n","python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2' --user -q\n","python -m pip install layoutparser[ocr] --user -q\n","\n","\n"],"outputs":[],"metadata":{"id":"jDyjgUdu-lbU"}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Import the libraries"],"metadata":{"id":"0HcBlUGp-pDN"}},{"cell_type":"code","execution_count":null,"source":["import layoutparser as lp\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import matplotlib\n","%matplotlib inline\n","import cv2"],"outputs":[],"metadata":{"id":"iRsYGv6cAetA"}},{"cell_type":"markdown","source":["Read an image from the source files to infer on it"],"metadata":{"id":"dfeBPsEeYwy3"}},{"cell_type":"code","execution_count":null,"source":["image = cv2.imread('https://raw.githubusercontent.com/Layout-Parser/layout-parser/master/examples/data/example-table.jpeg')\n","# display image\n","plt.figure(figsize=(12,16))\n","plt.imshow(image)\n","plt.xticks([])\n","plt.yticks([])\n","plt.show()"],"outputs":[],"metadata":{"id":"ZsxUTTNpBCQ5"}},{"cell_type":"markdown","source":["Load the TesseractAgent OCR Engine"],"metadata":{"id":"N7FXayWaZFst"}},{"cell_type":"code","execution_count":null,"source":["model = lp.TesseractAgent()"],"outputs":[],"metadata":{"id":"-yKx8tlbCjzv"}},{"cell_type":"markdown","source":["Detect the texts and their locations from the sample image."],"metadata":{"id":"vO8kYed_ZS8m"}},{"cell_type":"code","execution_count":null,"source":["res = model.detect(image, return_response=True)"],"outputs":[],"metadata":{"id":"4yAiTjceCo-x"}},{"cell_type":"markdown","source":["Collect texts and their bounding boxes details as a processible data structure."],"metadata":{"id":"6ULiN2ACKzaQ"}},{"cell_type":"code","execution_count":null,"source":["ocr  = model.gather_data(res, lp.TesseractFeatureType(4)) \n"],"outputs":[],"metadata":{"id":"wxCYZ5Q6nr22"}},{"cell_type":"code","execution_count":null,"source":["ocr"],"outputs":[],"metadata":{"id":"6sltiwWMwqPn"}},{"cell_type":"markdown","source":["Display the image with texts along with their bounding boxes"],"metadata":{"id":"ei0jGv9EZcfM"}},{"cell_type":"code","execution_count":null,"source":["lp.draw_text(image, ocr, font_size=12, with_box_on_text=True,\n","             text_box_width=1)"],"outputs":[],"metadata":{"id":"26ME4lQ2CsDu"}},{"cell_type":"markdown","source":["We can recognize that the output texts are reproduced with Engine-specified fonts and sizes. Thus the system has recognized texts and their locations precisely. Further, we can post-process these texts in a column-wise manner or row-wise manner as per need.\n","\n"],"metadata":{"id":"FpYpaaeAZ5I_"}}]}