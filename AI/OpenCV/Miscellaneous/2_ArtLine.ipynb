{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_ArtLine.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO1ZM4jS33I1sSP8P9Q0zko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **ArtLine: To Create Line Art Portraits, Movie Posters & Cartoonize Images**"],"metadata":{"id":"g6CPvxvbjP9s"}},{"cell_type":"markdown","source":["\n","\n","Deep Learning and Computer Vision has evolved and done wonders time and again. Today we are going to talk about one such recently done amazing project called ‘ArtLine’ that uses deep learning algorithms to achieve fine quality line art portraits. Besides that, it also can be used to generate movie posters and cartoonize images. It is currently the most trending topic in both GitHub and paperswithcode. It is created by Vijish Madhavan, a deep learning researcher. "],"metadata":{"id":"a021bIE0jhdT"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/artline-to-create-line-art-portraits-in-python/) article."],"metadata":{"id":"AnkbxeF7jmKL"}},{"cell_type":"markdown","source":["## **Model Training Code Implementation**"],"metadata":{"id":"0Blayqlpj5dL"}},{"cell_type":"markdown","source":["Importing necessary libraries"],"metadata":{"id":"UsR2cjqSj8pT"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image pillow fastai --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import torch\n","import torch.nn as nn\n","import fastai\n","from fastai.vision import *\n","from fastai.callbacks import *\n","from fastai.vision.gan import *\n","from torchvision.models import vgg16_bn\n","from fastai.utils.mem import *\n","from PIL import Image\n","import numpy as np\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms "],"outputs":[],"metadata":{"id":"BNF3rzzXhfKA","executionInfo":{"status":"ok","timestamp":1623049582826,"user_tz":-330,"elapsed":4347,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Edge Detection – this function uses the convolutional neural network to detect edges from an image as to features and use it as a gradient."],"metadata":{"id":"Ddz0ueQHkA48"}},{"cell_type":"code","execution_count":null,"source":["def _gradient_img(img):\n","  img = img.squeeze(0)\n","  ten=torch.unbind(img)\n","  x=ten[0].unsqueeze(0).unsqueeze(0)\n","  a=np.array([[1, 0, -1],[2,0,-2],[1,0,-1]])\n","  conv1=nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False) \n","  conv1.weight=nn.Parameter(torch.from_numpy(a).float().unsqueeze(0).unsqueeze(0))\n","  G_x=conv1(Variable(x)).data.view(1,x.shape[2],x.shape[3])\n","  b=np.array([[1, 2, 1],[0,0,0],[-1,-2,-1]])\n","  conv2=nn.Conv2d(1, 1, kernel_size=4, stride=2, padding=2, bias=False)  \n","  conv2.weight=nn.Parameter(torch.from_numpy(b).float().unsqueeze(0).unsqueeze(0))\n","  G_y=conv2(Variable(x)).data.view(1,x.shape[2],x.shape[3])\n","  G=torch.sqrt(torch.pow(G_x,2)+ torch.pow(G_y,2))\n","  return G\n","gradient = TfmPixel(_gradient_img) "],"outputs":[],"metadata":{"id":"vifRigAakMWH","executionInfo":{"status":"ok","timestamp":1623049582838,"user_tz":-330,"elapsed":44,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["PATH – redirecting to the saved APDrawing dataset and a selective picture from the Anime sketch colourization pair."],"metadata":{"id":"se79rh13kqfy"}},{"cell_type":"code","execution_count":null,"source":["!wget https://cg.cs.tsinghua.edu.cn/people/~Yongjin/APDrawingDB.zip"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BYpfSCNmZ2o","executionInfo":{"status":"ok","timestamp":1623050561611,"user_tz":-330,"elapsed":263943,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"363732b7-19d6-4b20-8866-f9701237be7d"}},{"cell_type":"code","execution_count":null,"source":["!unzip APDrawingDB.zip"],"outputs":[],"metadata":{"id":"KvWRpeitoA-Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623050568789,"user_tz":-330,"elapsed":4180,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"62bcfa5c-706b-48f1-cd35-17418b1b4798"}},{"cell_type":"code","execution_count":null,"source":["path = Path('Apdrawing')\n","#Blended Facial Features\n","path_hr = Path('Apdrawing/draw tiny')\n","path_lr = Path('Apdrawing/Tiny Real')\n","#Portrait Pair\n","path_hr3 = Path('Apdrawing/drawing')\n","path_lr3= Path('Apdrawing/Real')\n","#Architecture -  pretrained resnet34 model is used\n","arch = models.resnet34 "],"outputs":[],"metadata":{"id":"8Jzv7q-ZksuQ"}},{"cell_type":"markdown","source":["Detecting Facial Features"],"metadata":{"id":"I1AStie3Nm5N"}},{"cell_type":"code","execution_count":null,"source":["src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.3, seed=42)\n","def get_data(bs,size):\n","  data = (src.label_from_func(lambda x: path_hr/x.name)\n","          .transform(get_transforms(xtra_tfms=[gradient()]), size=size, tfm_y=True)\n","          .databunch(bs=bs,num_workers = 0).normalize(imagenet_stats, do_y=True))\n","  data.c = 3\n","  return data "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"id":"ky7xxZX5NqB4","executionInfo":{"status":"error","timestamp":1623048254038,"user_tz":-330,"elapsed":679,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"c38650be-414a-4789-9765-aaba4aa3de78"}},{"cell_type":"markdown","source":["Progressive resizing by the Fastai library helps gradually increase the size of the image and the adjusting learning rates, thereby generalizing the images as it goes through different stages. \n","\n","64px"],"metadata":{"id":"bV-ZRx8eNtO0"}},{"cell_type":"code","execution_count":null,"source":["bs,size=20, 64\n","data = get_data(bs,size)\n","data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))\n","t = data.valid_ds[0][1].data\n","t = torch.stack([t,t])\n","def gram_matrix(x):\n","    n,c,h,w = x.size()\n","    x = x.view(n, c, -1)\n","    return (x @ x.transpose(1,2))/(c*h*w)\n","gram_matrix(t)\n","base_loss = F.l1_loss\n","vgg_m = vgg16_bn(True).features.cuda().eval()\n","requires_grad(vgg_m, False)\n","blocks = [j-1 for j,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n","blocks, [vgg_m[i] for i in blocks] "],"outputs":[],"metadata":{"id":"W6oWVMzyOF2c"}},{"cell_type":"markdown","source":["Perpetual loss is calculated for image transformations based on the VGG_16 model. It speeds up training. This approach combines both a per-pixel loss between the output and ground-truth images and optimizing perceptual loss functions based on high-level features extracted from pre-trained networks. The results are then used to train a feed-forward network."],"metadata":{"id":"r0O8-poZOI-8"}},{"cell_type":"code","execution_count":null,"source":["class FeatureLoss(nn.Module):\n","    def __init__(self, m_feat, layer_ids, layer_wgts):\n","        super().__init__()\n","        self.m_feat = m_feat\n","        self.losses = [self.m_feat[i] for i in layer_ids]\n","        self.hooks = hook_outputs(self.losses, detach=False)\n","        self.wgts = layer_wgts\n","        self.metrics_name = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))] + [f'gram_{i}' for i in range(len(layer_ids))]\n","    def make_features(self, x, clone=False):\n","        self.m_feat(x)\n","        return [(p.clone() if clone else p) for p in self.hooks.stored]\n","    def forward(self, input, target):\n","        out_feat = self.make_features(target, clone=True)\n","        in_feat = self.make_features(input)\n","        self.feat_losses = [base_loss(input,target)]\n","        self.feat_losses += [base_loss(f_in, f_out)*w\n","        for in, out, w in zip(in_feat, out_feat, self.wgts)]\n","        self.feat_losses += [base_loss(gram_matrix(in),  gram_matrix(out))*w**2 * 5e3\n","        for in, out, w in zip(in_feat, out_feat, self.wgts)]\n","        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n","        return sum(self.feat_losses)\n","    def __del__(self): self.hooks.remove()\n","feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])\n","wd = 1e-3\n","y_range = (-3.,3.) "],"outputs":[],"metadata":{"id":"rzpBt0ldOLS7"}},{"cell_type":"markdown","source":["This function uses the self-attention model generator with U-Net and spatial normalization. This is a No GAN training which stabilizes colour images. Here minimal time is spent in direct GAN training instead, separately pretraining the generator and critic. This was introduced in another project named DeOldify. It helps largely in getting accurate facial features."],"metadata":{"id":"PWNfgTmHONwx"}},{"cell_type":"code","execution_count":null,"source":["def create_gen_learner():\n","    return unet_learner(data, arch, wd=wd, blur=True, norm_type=NormType.Spectral,self_attention=True, y_range=(-3.0, 3.0),loss_func=feat_loss, callback_fns=LossMetrics)\n","gc.collect();\n","learn_gen = create_gen_learner()\n","learn_gen.lr_find()\n","lr = 1-01\n","epoch = 5 "],"outputs":[],"metadata":{"id":"MkmCNn2tOPv1"}},{"cell_type":"markdown","source":["fitting the model"],"metadata":{"id":"a2BfVIm2ORbC"}},{"cell_type":"code","execution_count":null,"source":["def do_fit(save_name, lrs=slice(lr), pct_start=0.9):\n","    learn_gen.fit_one_cycle(epoch, lrs, pct_start=pct_start,)\n","    learn_gen.save(save_name)\n","    learn_gen.show_results(rows=1, imgsize=5)\n","do_fit('da', slice(lr))\n","#lr*10\n","learn_gen.unfreeze()\n","learn_gen.lr_find()\n","epoch = 5\n","do_fit('db', slice(1E-2))"],"outputs":[],"metadata":{"id":"PhnvVHxuOTqY"}},{"cell_type":"markdown","source":["128px"],"metadata":{"id":"pri9DXQWOVje"}},{"cell_type":"code","execution_count":null,"source":["data = get_data(8,128)\n","learn_gen.data = data\n","learn_gen.freeze()\n","gc.collect()\n","learn_gen.load('db');\n","epoch =5\n","lr = 1E-03\n","do_fit('db2',slice(lr))\n","learn_gen.unfreeze()\n","epoch = 5\n","do_fit('db3', slice(1e-02,1e-5), pct_start=0.3) "],"outputs":[],"metadata":{"id":"2GT2El53OX1L"}},{"cell_type":"markdown","source":["192px"],"metadata":{"id":"tAHScntIOZjw"}},{"cell_type":"code","execution_count":null,"source":["data = get_data(5,192)\n","learn_gen.data = data\n","learn_gen.freeze()\n","gc.collect()\n","learn_gen.load('db3');\n","epoch =5\n","lr = 1E-06\n","do_fit('db4')\n","learn_gen.unfreeze()\n","epoch = 5\n","do_fit('db5', slice(1e-06,1e-4), pct_start=0.3) "],"outputs":[],"metadata":{"id":"LaUCZzYzObgH"}},{"cell_type":"markdown","source":["##**Acquiring data for portrait images**"],"metadata":{"id":"vLvRaQ1oOdNa"}},{"cell_type":"code","execution_count":null,"source":["src = ImageImageList.from_folder(path_lr3).split_by_rand_pct(0.2, seed=42)\n","def get_data(bs,size):\n","    data = (src.label_from_func(lambda x: path_hr3/x.name)\n","          .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True).databunch(bs=bs,num_workers = 0).normalize(imagenet_stats, do_y=True))\n","    data.c = 3\n","    return data "],"outputs":[],"metadata":{"id":"XlmkTmLsOjCh"}},{"cell_type":"markdown","source":["128px"],"metadata":{"id":"7HDho9C4OlDg"}},{"cell_type":"code","execution_count":null,"source":["data = get_data(8,128)\n","learn_gen.data = data\n","learn_gen.freeze()\n","gc.collect()\n","learn_gen.load('db5');\n","data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))\n","learn_gen.lr_find()\n","epoch = 5\n","lr = 1e-03\n","do_fit('db6')\n","learn_gen.unfreeze()\n","epoch = 5\n","do_fit('db7', slice(6.31E-07,1e-5), pct_start=0.3) "],"outputs":[],"metadata":{"id":"yH6vikZaOnI6"}},{"cell_type":"markdown","source":["192px"],"metadata":{"id":"a-YbIUhvOpGw"}},{"cell_type":"code","execution_count":null,"source":["data = get_data(4,192)\n","learn_gen.data = data\n","learn_gen.freeze()\n","gc.collect()\n","learn_gen.load('db7');\n","learn_gen.lr_find()\n","epoch = 5\n","lr = 4.37E-05\n","do_fit('db8')\n","learn_gen.unfreeze()\n","epoch = 5\n","do_fit('db9', slice(1.00E-05,1e-3), pct_start=0.3) "],"outputs":[],"metadata":{"id":"pRu7uzwuOror"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Exploring Artline](https://analyticsindiamag.com/artline-to-create-line-art-portraits-in-python/)\n","\n","> * [Guide to Google's Objectron](https://analyticsindiamag.com/objectron-dataset-tutorial-in-python/)\n","\n","> * [Guide to OpenAI's CLIP](https://analyticsindiamag.com/hands-on-guide-to-openais-clip-connecting-text-to-images/)\n","\n","> * [Guide to VISSL](https://analyticsindiamag.com/guide-to-vissl-vision-library-for-self-supervised-learning/)\n","\n","> * [Unet Image Segmentation Model](https://analyticsindiamag.com/my-experiment-with-unet-building-an-image-segmentation-model/)\n","\n","> * [Comparison of Semantic, Instance and Panoptic Segmentation](https://analyticsindiamag.com/semantic-vs-instance-vs-panoptic-which-image-segmentation-technique-to-choose/)\n"],"metadata":{"id":"1ZtPNx78qe5T"}}]}