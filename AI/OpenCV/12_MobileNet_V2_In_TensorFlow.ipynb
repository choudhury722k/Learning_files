{"cells":[{"cell_type":"markdown","source":["# **MobileNet V2 In TensorFlow**"],"metadata":{"id":"I6ywPDs_ZzJh"}},{"cell_type":"markdown","source":["Transfer Learning is not a new concept. Ever since us humans began to train machines to learn, classify and predict data, we have looked for ways to retain what the machine has already learnt."],"metadata":{"id":"F27XzmYTaSxK"}},{"cell_type":"markdown","source":["Transfer learning is simply the process of using a pre-trained model that has been trained on a dataset for training and predicting on a new given dataset.\n","\n","“A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task.“\n","\n","In this article, we will use transfer learning to classify the images of cats and dogs from Machinehack’s Who Let The Dogs Out: Pets Breed Classification Hackathon."],"metadata":{"id":"F3cWXhAraYaz"}},{"cell_type":"markdown","source":["## **Getting the dataset**"],"metadata":{"id":"G9QmBnckavRR"}},{"cell_type":"markdown","source":["Head to MachineHack, sign up and start the [Who Let The Dogs Out: Pets Breed Classification Hackathon](https://machinehack.com/hackathons/who_let_the_dogs_out_pets_breed_classification_hackathon/overview). The datasets can be downloaded on the assignment page. The training set consists of 6206 images of both cats and dogs of different breeds. We will use these images and their respective classes provided in the train.csv file to train our classifier to categorize a given image as either the image of a cat or a dog."],"metadata":{"id":"Isxj3e5Ea7Bm"}},{"cell_type":"markdown","source":["## **Transfer Learning With MobileNet V2**"],"metadata":{"id":"_FhylU59a9P1"}},{"cell_type":"markdown","source":["MobileNet V2 model was developed at Google, pre-trained on the ImageNet dataset with 1.4M images and 1000 classes of web images. We will use this as our base model to train with our dataset and classify the images of cats and dogs.\n"],"metadata":{"id":"bs81FdCIbBDK"}},{"cell_type":"markdown","source":["Importing Tensorflow and necessary libraries"],"metadata":{"id":"EcdQAPxMbDPc"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image --user -q --no-warn-script-location\n","\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","print(tf.__version__)"],"outputs":[],"metadata":{"id":"Ox2PgyiFZu0D"}},{"cell_type":"markdown","source":["The print function will return your TensorFlow version. "],"metadata":{"id":"JpJq5BpwbHFT"}},{"cell_type":"markdown","source":["Preparing the training data"],"metadata":{"id":"CWCG1pcFbJHN"}},{"cell_type":"code","execution_count":null,"source":["#Read the train.csv file from the right location\n","training_data = pd.read_csv(\"train.csv\")\n","#Appending the file extension to the image names\n","training_imgs = [\"{}.jpg\".format(x) for x in list(training_data.id)]\n","\n","#Creating a new dataframe with updated images names\n","training_labels_1 = list(training_data['class_name'])\n","training_data = pd.DataFrame( {'Images': training_imgs,'Animal': training_labels_1})\n","\n","#Changing the type of categorical variable(from int to str)\n","training_data.Animal = training_data.Animal.astype(str)"],"outputs":[],"metadata":{"id":"F1FBwlzhbI5K"}},{"cell_type":"markdown","source":["Before feeding the data into the network, we will load and prepare it for processing. We will read the train.csv file and update it by adding the file type(.jpg) to each of the images names.We will also change the type of the categorical variable class_name which is later renamed as ‘Animal’ from integer to string.\n","\n","Let’s have a look at the new dataset."],"metadata":{"id":"2eT5lT6bbS8U"}},{"cell_type":"code","execution_count":null,"source":["training_data.head()"],"outputs":[],"metadata":{"id":"w17upiZIbVQ6"}},{"cell_type":"markdown","source":["The label 1 refers to the image of a cat and label 2 refers to the image of a dog."],"metadata":{"id":"eZbteuOJbXTy"}},{"cell_type":"markdown","source":["Creating training and validation sets"],"metadata":{"id":"62cZtyQXbYv6"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\n","training_set, validation_set = train_test_split(training_data, random_state = 0, test_size = 0.2)"],"outputs":[],"metadata":{"id":"2AqsH7csba0j"}},{"cell_type":"markdown","source":["We will split the training data into two different datasets, a training set to train the model and a validation set to evaluate the performance of the model."],"metadata":{"id":"nZ33YnXibd0f"}},{"cell_type":"markdown","source":["Preprocessing the Image data"],"metadata":{"id":"Nzm1zFchbfZh"}},{"cell_type":"code","execution_count":null,"source":["!gdown https://drive.google.com/uc?id=1W-9crZ3f30ZDdXPZ8-wADQo33CkG8TgH"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!unzip image_data.zip"],"outputs":[],"metadata":{"id":"RLvO79e5by9s"}},{"cell_type":"code","execution_count":null,"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_dataGen = ImageDataGenerator(rescale = 1./255,\n","                                      shear_range = 0.2,\n","                                      zoom_range = 0.2,\n","                                      horizontal_flip = True)\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","train_generator = train_dataGen.flow_from_dataframe(dataframe = training_set, directory=\"image_data/images_train/\",x_col=\"Images\", y_col=\"Animal\", class_mode=\"binary\", target_size=(160,160), batch_size=32)\n","validation_generator = validation_datagen.flow_from_dataframe(dataframe= validation_set, directory=\"image_data/images_train/\", x_col=\"Images\", y_col=\"Animal\", class_mode=\"binary\", target_size=(160,160), batch_size=32)"],"outputs":[],"metadata":{"id":"UI-TFf0BbhZ-"}},{"cell_type":"markdown","source":["We will use the ImageDataGenerator from TensorFlow to generate batches of tensor image data. First, we will initialize the ImageDataGenerator object for both training_set and validation_set with a set of parameters like rescale, shear_range, zoom_range, horizontal_flip. These parameters will help in transforming the image vectors for maximum feature extraction. We will then use these objects to generate tensors from the actual images.\n","\n","The flow_from_dataframe method uses the data frame to load the images. The directory parameter specifies the exact location of the images. x_col and y_col are the independent and dependent variables, in this case, the images and the labels. class_mode=”binary” specifies that the data consists of only 2 distinct classes which are cats and dogs. target_size=(160,160) will generate an image of size 160 x 160. Batch size is the number of images sampled at once\n","\n"],"metadata":{"id":"9eoTaSUMcaXS"}},{"cell_type":"markdown","source":["Initializing the base model"],"metadata":{"id":"wmhOeqXccc1_"}},{"cell_type":"code","execution_count":null,"source":["image_size = 160\n","IMG_SHAPE = (image_size, image_size, 3)\n","\n","#Create the base model from the pre-trained model MobileNet V2\n","base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n","                                              include_top=False,\n","                                              weights='imagenet')"],"outputs":[],"metadata":{"id":"TfjRFg3Ec1ea"}},{"cell_type":"markdown","source":["The base model is the model that is pre-trained. We will create a base model using MobileNet V2.\n","\n","We will also initialize the base model with a matching input size as to the pre-processed image data we have which is 160×160. The base model will have the same weights from imagenet. We will exclude the top layers of the pre-trained model by specifying include_top=False which is ideal for feature extraction.\n","\n","The above code block will download the pre-trained model and initializes it with the given parameters. We should also prevent the weights of the convolution from being updated before the model is compiled and trained. To do this we set the trainable attribute to false."],"metadata":{"id":"scQNGYB1c3wK"}},{"cell_type":"code","execution_count":null,"source":["base_model.trainable = False"],"outputs":[],"metadata":{"id":"ujFOp94Sc5xB"}},{"cell_type":"markdown","source":["Adding Extra layers to Pre-trained Model\n","\n","Since the pre-trained model is trained to classify into 1000 classes, we will manually set the output layers to adapt to our problem. Here we need a single node output layer as we have a binary classification problem. We will add the final layers to the base_model network as follows:"],"metadata":{"id":"PjQ8fkRmc8UB"}},{"cell_type":"code","execution_count":null,"source":["model = tf.keras.Sequential([\n","                          base_model,\n","                          keras.layers.GlobalAveragePooling2D(),\n","                          keras.layers.Dense(1, activation='sigmoid')])"],"outputs":[],"metadata":{"id":"nWeLHGJec-4M"}},{"cell_type":"markdown","source":["Compiling the model\n","\n","Its time to compile our new model by initializing the right optimizer, loss function and metrics. "],"metadata":{"id":"ea0PtTM5dA4m"}},{"cell_type":"code","execution_count":null,"source":["model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"9QBeE4EPdCy4"}},{"cell_type":"markdown","source":["Training the model\n","\n","Finally, it’s time to train the model with the image data we have. We will train the model for a 100 epochs, each set with 70 steps_per_epoch for both training and validation. \n","\n","steps_per_epoch is the number of times the weights are updated for each cycle of training. (The ideal value for steps_per_epoch is the number of samples per batch)\n","\n","We will fit the training and validation data generators and specified parameters to the model. On executing the below code block, the model will actually start to train."],"metadata":{"id":"oesnqIcgdHKO"}},{"cell_type":"code","execution_count":null,"source":["epochs = 100\n","steps_per_epoch = 70\n","validation_steps = 70\n","\n","history = model.fit_generator(train_generator,\n","                              steps_per_epoch = steps_per_epoch,\n","                              epochs=epochs,\n","                              workers=4,\n","                              validation_data=validation_generator,\n","                              validation_steps=validation_steps)"],"outputs":[],"metadata":{"id":"FPwjHBnKdE94"}},{"cell_type":"markdown","source":["Visualizing the training and Validation performance"],"metadata":{"id":"mP_DydxmdKPK"}},{"cell_type":"markdown","source":["Execute the below code blocks to plot graphs for varying training and validation accuracies and losses."],"metadata":{"id":"_8oHAuepdNFA"}},{"cell_type":"code","execution_count":null,"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,max(plt.ylim())])\n","plt.title('Training and Validation Loss')\n","plt.show()"],"outputs":[],"metadata":{"id":"sm3bZuuudaTX"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Transfer Learning in Tensorflow Keras](https://analyticsindiamag.com/a-practical-guide-to-implement-transfer-learning-in-tensorflow/)\n","\n","> * [Differentiable Augmentation for Data-Efficient GAN Training](https://analyticsindiamag.com/guide-to-differentiable-augmentation-for-data-efficient-gan-training/)\n","\n","> * [Guide to Albumentation](https://analyticsindiamag.com/hands-on-guide-to-albumentation/)\n","\n","> * [Google STAC](https://analyticsindiamag.com/googles-stac-ssl-framework-for-object-detection/)\n","\n","> * [Point Transformers](https://analyticsindiamag.com/how-point-transformer-excels-in-3d-image-processing/)\n","\n","> * [Comparison of Transfer Learning with Multi Class Classification](https://analyticsindiamag.com/practical-comparison-of-transfer-learning-models-in-multi-class-image-classification/)\n"],"metadata":{"id":"1ZtPNx78qe5T"}}],"metadata":{"colab":{"authorship_tag":"ABX9TyM4nkzJgLEaa9yjoizvTK4+","collapsed_sections":[],"mount_file_id":"1KavuAWx3xnno0Pei5FQ2qI84lIQIX1MK","name":"1_MobileNet_V2_In_TensorFlow.ipynb","provenance":[]},"interpreter":{"hash":"bf1c1c88dcd4a5670b9a47bf2bba5adec14ec31820c4d35855d0349e55a0f49f"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":2}