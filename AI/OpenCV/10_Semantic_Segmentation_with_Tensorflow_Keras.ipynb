{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Semantic_Segmentation_with_Tensorflow_Keras.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOcEUOR+areTbFitAUUcB99"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Semantic Segmentation Using TensorFlow Keras**"],"metadata":{"id":"NuuKadYHHwg5"}},{"cell_type":"markdown","source":["Semantic Segmentation laid down the fundamental path to advanced Computer Vision tasks such as object detection, shape recognition, autonomous driving, robotics, and virtual reality. Semantic segmentation can be defined as the process of pixel-level image classification into two or more Object classes. It differs from image classification entirely, as the latter performs image-level classification. For instance, consider an image that consists mainly of a zebra, surrounded by grass fields, a tree and a flying bird. Image classification tells us that the image belongs to the ‘zebra’ class. It can not tell where the zebra is or what its size or pose is. But, semantic segmentation of that image may tell that there is a zebra, grass field, a bird and a tree in the given image (classifies parts of an image into separate classes). And it tells us which pixels in the image belong to which class."],"metadata":{"id":"Vya_-AQhQtZE"}},{"cell_type":"markdown","source":["In this practice session, we will discuss semantic segmentation using TensorFlow Keras. Readers are expected to have a fundamental knowledge of deep learning, image classification and transfer learning. "],"metadata":{"id":"L4zen9iiQusA"}},{"cell_type":"markdown","source":["## **Implementation**\n","\n","\n"],"metadata":{"id":"Ipyg2ZN0Ic_q"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras opencv-python pillow scikit-image torch torchvision \\\n","     tqdm --user -q --no-warn-script-location\n","\n","!python -m pip install -q git+https://github.com/tensorflow/examples.git --user -q\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Import necessary frameworks, libraries and modules"],"metadata":{"id":"B3UqqFdiIc_y"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import cv2\n","from scipy import io\n","import tensorflow_datasets as tfds\n","from tensorflow_examples.models.pix2pix import pix2pix\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"trusted":true,"id":"MiT_RdtAIc_z"}},{"cell_type":"markdown","source":["# Download and prepare data"],"metadata":{"id":"KTmN55j5Ic_0"}},{"cell_type":"markdown","source":["We use Clothing Co-Parsing public dataset as our supervised dataset. This dataset has 1000 images of people (one person per image). There are 1000 label images corresponding to those original images. Label images have 59 segmented classes corresponding to classes such as hair, shirt, shoes, skin, sunglasses and cap"],"metadata":{"id":"LsrAuN_NRGXd"}},{"cell_type":"code","execution_count":null,"source":["!git clone https://github.com/bearpaw/clothing-co-parsing.git"],"outputs":[],"metadata":{"trusted":true,"id":"iC9kQAWbIc_0"}},{"cell_type":"code","execution_count":null,"source":["!echo $PWD"],"outputs":[],"metadata":{"trusted":true,"id":"XF0c2w6uIc_1"}},{"cell_type":"code","execution_count":null,"source":["!ls clothing-co-parsing/"],"outputs":[],"metadata":{"trusted":true,"id":"ISaIwaVJIc_2"}},{"cell_type":"markdown","source":["Input images are in the photos directory, and labelled images are in the annotations directory. Let’s extract the input images from the respective source directory."],"metadata":{"id":"LvCrPO5IRK93"}},{"cell_type":"code","execution_count":null,"source":["images = []\n","for i in range(1,1001):\n","    url = './clothing-co-parsing/photos/%04d.jpg'%(i)\n","    img = cv2.imread(url)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    images.append(tf.convert_to_tensor(img))"],"outputs":[],"metadata":{"trusted":true,"id":"6w2jv_ZnIc_2"}},{"cell_type":"markdown","source":["Let’s extract the labelled images (segmented images) from the respective source directory."],"metadata":{"id":"YQnrLRYqRN7M"}},{"cell_type":"code","execution_count":null,"source":["masks = []\n","for i in range(1,1001):\n","    url = './clothing-co-parsing/annotations/pixel-level/%04d.mat'%(i)\n","    file = io.loadmat(url)\n","    mask = tf.convert_to_tensor(file['groundtruth'])\n","    masks.append(mask)"],"outputs":[],"metadata":{"trusted":true,"id":"ubG_nOFvIc_3"}},{"cell_type":"markdown","source":["How many examples do we have now?"],"metadata":{"id":"7RVkNT5BRQau"}},{"cell_type":"code","execution_count":null,"source":["len(images), len(masks)"],"outputs":[],"metadata":{"trusted":true,"id":"jvSNQPATIc_5"}},{"cell_type":"markdown","source":["As mentioned in the source files, there are 1000 images and 1000 labels. Visualize some images to get better insights."],"metadata":{"id":"UPQBClrkRSu6"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(10,4))\n","for i in range(1,4):\n","    plt.subplot(1,3,i)\n","    img = images[i]\n","    plt.imshow(img, cmap='jet')\n","    plt.colorbar()\n","    plt.axis('off')\n","plt.show()"],"outputs":[],"metadata":{"trusted":true,"id":"TpiO7p_MIc_6"}},{"cell_type":"markdown","source":["Each colour in the above images refer to a specific class. We observe that the person and her/his wearings are segmented, leaving the surrounding unsegmented. "],"metadata":{"id":"9SbYMC2CRW8_"}},{"cell_type":"code","execution_count":null,"source":["masks[0].numpy().min(), masks[0].numpy().max()"],"outputs":[],"metadata":{"trusted":true,"id":"dVf-HupsIc_7"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(10,4))\n","for i in range(1,4):\n","    plt.subplot(1,3,i)\n","    img = masks[i]\n","    plt.imshow(img, cmap='jet')\n","    plt.colorbar()\n","    plt.axis('off')\n","plt.show()"],"outputs":[],"metadata":{"trusted":true,"id":"1ZhtO4ygIc_7"}},{"cell_type":"markdown","source":["# Build Downstack with a Pre-trained CNN"],"metadata":{"id":"ehdbzj8fIc_9"}},{"cell_type":"markdown","source":["Load DenseNet121 from in-built applications."],"metadata":{"id":"j8OePIK5Rcmr"}},{"cell_type":"code","execution_count":null,"source":["base = keras.applications.DenseNet121(input_shape=[128,128,3], \n","                                      include_top=False, \n","                                      weights='imagenet')"],"outputs":[],"metadata":{"trusted":true,"id":"CBLXmRv_Ic_9"}},{"cell_type":"code","execution_count":null,"source":["len(base.layers)"],"outputs":[],"metadata":{"trusted":true,"id":"-1ttQI1yIc_-"}},{"cell_type":"markdown","source":["The DenseNet121 model has 427 layers. We need to identify suitable layers whose output will be used for skip connections. Plot the entire model, along with the feature shapes."],"metadata":{"id":"DzWIBqnQRfUQ"}},{"cell_type":"code","execution_count":null,"source":["keras.utils.plot_model(base, show_shapes=True)"],"outputs":[],"metadata":{"trusted":true,"id":"KZrb5NQjIc_-"}},{"cell_type":"markdown","source":["We select the final ReLU activation layer for each feature map size, i.e. 4, 8, 16, 32, and 64, required for skip-connections. Write down the names of the selected ReLU layers in a list."],"metadata":{"id":"tNDIrSwPRh2B"}},{"cell_type":"code","execution_count":null,"source":["skip_names = ['conv1/relu', # size 64*64\n","             'pool2_relu',  # size 32*32\n","             'pool3_relu',  # size 16*16\n","             'pool4_relu',  # size 8*8\n","             'relu'        # size 4*4\n","             ]"],"outputs":[],"metadata":{"trusted":true,"id":"MMcr_UUKIc__"}},{"cell_type":"markdown","source":["Obtain the outputs of these layers."],"metadata":{"id":"dOVP2KCqRjxA"}},{"cell_type":"code","execution_count":null,"source":["skip_outputs = [base.get_layer(name).output for name in skip_names]\n","for i in range(len(skip_outputs)):\n","    print(skip_outputs[i])"],"outputs":[],"metadata":{"trusted":true,"id":"xvB-Vs86IdAA"}},{"cell_type":"markdown","source":["Build the downstack with the above layers. We use the pre-trained model as such, without any fine-tuning."],"metadata":{"id":"aOWTNwJ9Rl8X"}},{"cell_type":"code","execution_count":null,"source":["downstack = keras.Model(inputs=base.input,\n","                       outputs=skip_outputs)\n","downstack.trainable = False"],"outputs":[],"metadata":{"trusted":true,"id":"rZ1ajJYgIdAA"}},{"cell_type":"markdown","source":["# Build Upstack"],"metadata":{"id":"L088SW5NIdAA"}},{"cell_type":"markdown","source":["\n","Build the upstack using an upsampling template."],"metadata":{"id":"padlYfBxRnuA"}},{"cell_type":"code","execution_count":null,"source":["# Four upstack layers for upsampling sizes \n","# 4->8, 8->16, 16->32, 32->64 \n","upstack = [pix2pix.upsample(512,3),\n","          pix2pix.upsample(256,3),\n","          pix2pix.upsample(128,3),\n","          pix2pix.upsample(64,3)]"],"outputs":[],"metadata":{"trusted":true,"id":"CdeOY9WvIdAB"}},{"cell_type":"markdown","source":["We can explore the individual layers in each upstack block."],"metadata":{"id":"r7u0BbvCRr09"}},{"cell_type":"code","execution_count":null,"source":["upstack[0].layers"],"outputs":[],"metadata":{"trusted":true,"id":"jbN3tmdgIdAB"}},{"cell_type":"markdown","source":["# Build U-Net model with skip-connections"],"metadata":{"id":"HGQR_7TkIdAB"}},{"cell_type":"markdown","source":["Build a U-Net model by merging downstack and upstack with skip-connections."],"metadata":{"id":"NOtcTxOaRu9q"}},{"cell_type":"code","execution_count":null,"source":["# define the input layer\n","inputs = keras.layers.Input(shape=[128,128,3])\n","\n","# downsample \n","down = downstack(inputs)\n","out = down[-1]\n","\n","# prepare skip-connections\n","skips = reversed(down[:-1])\n","# choose the last layer at first 4 --> 8\n","\n","# upsample with skip-connections\n","for up, skip in zip(upstack,skips):\n","    out = up(out)\n","    out = keras.layers.Concatenate()([out,skip])\n","    \n","# define the final transpose conv layer\n","# image 128 by 128 with 59 classes\n","out = keras.layers.Conv2DTranspose(59, 3,\n","                                  strides=2,\n","                                  padding='same',\n","                                  )(out)\n","# complete unet model\n","unet = keras.Model(inputs=inputs, outputs=out)"],"outputs":[],"metadata":{"trusted":true,"id":"YvvcbtSMIdAB"}},{"cell_type":"markdown","source":["Visualize our U-Net model."],"metadata":{"id":"D4R05JBDRxE9"}},{"cell_type":"code","execution_count":null,"source":["keras.utils.plot_model(unet, show_shapes=True)"],"outputs":[],"metadata":{"trusted":true,"id":"vEKVGdvHIdAB"}},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"Mx8Ljb6tIdAC"}},{"cell_type":"markdown","source":["The model is perfectly ready. We can start training if data preprocessing is performed. Since we have limited images, we prepare more data through augmentation."],"metadata":{"id":"kB3DshMUR02D"}},{"cell_type":"code","execution_count":null,"source":["def resize_image(image):\n","    image = tf.cast(image, tf.float32)\n","    image = image/255.0\n","    # resize image\n","    image = tf.image.resize(image, (128,128))\n","    return image\n","\n","def resize_mask(mask):\n","    mask = tf.expand_dims(mask, axis=-1)\n","    mask = tf.image.resize(mask, (128,128))\n","    mask = tf.cast(mask, tf.uint8)\n","    return mask    "],"outputs":[],"metadata":{"trusted":true,"id":"wTVLwgRHIdAC"}},{"cell_type":"markdown","source":["Resize images and masks to the size 128 by 128."],"metadata":{"id":"lX7_KxXrR3ZS"}},{"cell_type":"code","execution_count":null,"source":["X = [resize_image(i) for i in images]\n","y = [resize_mask(m) for m in masks]\n","len(X), len(y)"],"outputs":[],"metadata":{"trusted":true,"id":"PtAAIgDBIdAC"}},{"cell_type":"code","execution_count":null,"source":["images[0].dtype, masks[0].dtype, X[0].dtype, y[0].dtype"],"outputs":[],"metadata":{"trusted":true,"id":"pzbCrq7eIdAE"}},{"cell_type":"code","execution_count":null,"source":["plt.imshow(X[0])\n","plt.colorbar()\n","plt.show()\n","\n","plt.imshow(np.squeeze(y[0]), cmap='jet')\n","plt.colorbar()\n","plt.show()"],"outputs":[],"metadata":{"trusted":true,"id":"8mVWdM-oIdAE"}},{"cell_type":"markdown","source":["Functions for augmentation"],"metadata":{"id":"1b-49MdEIdAE"}},{"cell_type":"markdown","source":["We have 800 train examples. That’s too low for training. We define a couple of augmentation functions to generate more train examples."],"metadata":{"id":"iT1xrQiLSBJ2"}},{"cell_type":"code","execution_count":null,"source":["def brightness(img, mask):\n","    img = tf.image.adjust_brightness(img, 0.1)\n","    return img, mask\n","\n","def gamma(img, mask):\n","    img = tf.image.adjust_gamma(img, 0.1)\n","    return img, mask\n","\n","def hue(img, mask):\n","    img = tf.image.adjust_hue(img, -0.1)\n","    return img, mask\n","\n","def crop(img, mask):\n","    img = tf.image.central_crop(img, 0.7)\n","    img = tf.image.resize(img, (128,128))\n","    mask = tf.image.central_crop(mask, 0.7)\n","    mask = tf.image.resize(mask, (128,128))\n","    mask = tf.cast(mask, tf.uint8)\n","    return img, mask\n","\n","def flip_hori(img, mask):\n","    img = tf.image.flip_left_right(img)\n","    mask = tf.image.flip_left_right(mask)\n","    return img, mask\n","\n","def flip_vert(img, mask):\n","    img = tf.image.flip_up_down(img)\n","    mask = tf.image.flip_up_down(mask)\n","    return img, mask\n","\n","def rotate(img, mask):\n","    img = tf.image.rot90(img)\n","    mask = tf.image.rot90(mask)\n","    return img, mask"],"outputs":[],"metadata":{"trusted":true,"id":"OQUVdFPTIdAF"}},{"cell_type":"markdown","source":["# Split Data for training and validation"],"metadata":{"id":"lCj_PbCuIdAF"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\n","\n","train_X, val_X,train_y, val_y = train_test_split(X,y, \n","                                                      test_size=0.2, \n","                                                      random_state=0\n","                                                     )\n","train_X = tf.data.Dataset.from_tensor_slices(train_X)\n","val_X = tf.data.Dataset.from_tensor_slices(val_X)\n","\n","train_y = tf.data.Dataset.from_tensor_slices(train_y)\n","val_y = tf.data.Dataset.from_tensor_slices(val_y)\n","\n","train_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec"],"outputs":[],"metadata":{"trusted":true,"id":"ofhXWxYOIdAF"}},{"cell_type":"markdown","source":["# Data Augmentation "],"metadata":{"id":"jDn8y2IaIdAF"}},{"cell_type":"markdown","source":["Apply augmentation to the data with the above functions. With 7 augmentation functions and 800 input examples, we can get 7*800 = 5600 new examples. Including original examples, we get 5600+800 = 6400 examples for training. That sounds good!"],"metadata":{"id":"0x8l1vY3Sm5D"}},{"cell_type":"code","execution_count":null,"source":["#Zip input images and ground truth masks. \n","train = tf.data.Dataset.zip((train_X, train_y))\n","val = tf.data.Dataset.zip((val_X, val_y))\n","\n","# perform augmentation on train data only\n","\n","a = train.map(brightness)\n","b = train.map(gamma)\n","c = train.map(hue)\n","d = train.map(crop)\n","e = train.map(flip_hori)\n","f = train.map(flip_vert)\n","g = train.map(rotate)\n","\n","train = train.concatenate(a)\n","train = train.concatenate(b)\n","train = train.concatenate(c)\n","train = train.concatenate(d)\n","train = train.concatenate(e)\n","train = train.concatenate(f)\n","train = train.concatenate(g)"],"outputs":[],"metadata":{"trusted":true,"id":"uA94au7-IdAF"}},{"cell_type":"markdown","source":["Prepare data batches. Shuffle the train data."],"metadata":{"id":"5n1hm3nTSpCR"}},{"cell_type":"code","execution_count":null,"source":["BATCH = 64\n","AT = tf.data.AUTOTUNE\n","BUFFER = 1000\n","\n","STEPS_PER_EPOCH = 800//BATCH\n","VALIDATION_STEPS = 200//BATCH"],"outputs":[],"metadata":{"trusted":true,"id":"XBUTW6IEIdAF"}},{"cell_type":"code","execution_count":null,"source":["train = train.cache().shuffle(BUFFER).batch(BATCH).repeat()\n","train = train.prefetch(buffer_size=AT)\n","val = val.batch(BATCH)"],"outputs":[],"metadata":{"trusted":true,"id":"SHtPcIZwIdAG"}},{"cell_type":"markdown","source":["# Check for Model and Data compatibility"],"metadata":{"id":"JYr6fLO4IdAG"}},{"cell_type":"markdown","source":["Let’s check whether everything is good with the data and the model by sampling one example image and predict it with the untrained model."],"metadata":{"id":"AZNJoWRmSsCT"}},{"cell_type":"code","execution_count":null,"source":["example = next(iter(train))\n","preds = unet(example[0])\n","plt.imshow(example[0][60])\n","plt.colorbar()\n","plt.show()"],"outputs":[],"metadata":{"trusted":true,"id":"F0j7N2n3IdAG"}},{"cell_type":"code","execution_count":null,"source":["pred_mask = tf.argmax(preds, axis=-1)\n","pred_mask = tf.expand_dims(pred_mask, -1)\n","plt.imshow(np.squeeze(pred_mask[0]))\n","plt.colorbar()"],"outputs":[],"metadata":{"trusted":true,"id":"1Hk7TjaYIdAG"}},{"cell_type":"markdown","source":["Compile the model with RMSprop optimizer, Sparse Categorical Cross-entropy loss function and accuracy metric. Train the model for 20 epochs."],"metadata":{"id":"X3PJGyR6SwPH"}},{"cell_type":"code","execution_count":null,"source":["unet.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","            optimizer=keras.optimizers.RMSprop(lr=0.001),\n","            metrics=['accuracy']) "],"outputs":[],"metadata":{"trusted":true,"id":"BbC4EaoiIdAI"}},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"bpIIlr7gIdAI"}},{"cell_type":"code","execution_count":null,"source":["hist = unet.fit(train,\n","               validation_data=val,\n","               steps_per_epoch=STEPS_PER_EPOCH,\n","               validation_steps=VALIDATION_STEPS,\n","               epochs=50)"],"outputs":[],"metadata":{"trusted":true,"id":"TKgqJ4n1IdAI"}},{"cell_type":"markdown","source":["# Prediction"],"metadata":{"id":"B4IAXvH1IdAJ"}},{"cell_type":"markdown","source":["Let’s check how our model performs."],"metadata":{"id":"WIimScnfS1aC"}},{"cell_type":"code","execution_count":null,"source":["img, mask = next(iter(val))\n","pred = unet.predict(img)\n","plt.figure(figsize=(10,5))\n","for i in pred:\n","    plt.subplot(121)\n","    i = tf.argmax(i, axis=-1)\n","    plt.imshow(i,cmap='jet')\n","    plt.axis('off')\n","    plt.title('Prediction')\n","    break\n","plt.subplot(122)\n","plt.imshow(np.squeeze(mask[0]), cmap='jet')\n","plt.axis('off')\n","plt.title('Ground Truth')\n","plt.show()"],"outputs":[],"metadata":{"trusted":true,"id":"oHWS2jYcIdAJ"}},{"cell_type":"markdown","source":["# Performance Curves"],"metadata":{"id":"Rj-QVLyZIdAJ"}},{"cell_type":"code","execution_count":null,"source":["history = hist.history\n","acc=history['accuracy']\n","val_acc = history['val_accuracy']\n","\n","plt.plot(acc, '-', label='Training Accuracy')\n","plt.plot(val_acc, '--', label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"outputs":[],"metadata":{"trusted":true,"id":"6NxycEo8IdAK"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Semantic Segmentation Using TensorFlow Keras](https://analyticsindiamag.com/semantic-segmentation-using-tensorflow-keras/)\n","\n","> * [Convert Image to Pencil Sketch](https://analyticsindiamag.com/converting-image-into-a-pencil-sketch-in-python/)\n","\n","> * [Image Classification Task with and without Data Augmentation](https://analyticsindiamag.com/image-data-augmentation-impacts-performance-of-image-classification-with-codes/)\n","\n","> * [Image Data Augmentation Work As A Regularizer](https://analyticsindiamag.com/why-does-image-data-augmentation-work-as-a-regularizer-in-deep-learning/)\n","\n","> * [Guide to Pillow](https://analyticsindiamag.com/hands-on-guide-to-pillow-python-library-for-image-processing/)\n","\n","> * [Guide to Pgmagick](https://analyticsindiamag.com/complete-guide-on-pgmagick-python-tool-for-image-processing/)\n","\n"],"metadata":{"id":"1ZtPNx78qe5T"}}]}