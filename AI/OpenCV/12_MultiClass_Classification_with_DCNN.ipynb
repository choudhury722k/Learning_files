{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_MultiClass_Classification_with_DCNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNyidSJJQGGA6kJaJB9pNHV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Transfer Learning For Multi-Class Image Classification Using Deep Convolutional Neural Network**"],"metadata":{"id":"9JG9_HFRJqE-"}},{"cell_type":"markdown","source":["In this practice session, we will implement the multiclass image classification using the VGG-19 Deep Convolutional Network used as a Transfer Learning framework where the VGGNet comes pre-trained on the ImageNet dataset. For the experiment, we will use the CIFAR-10 dataset and classify the image objects into 10 classes. The classification accuracies of the VGG-19 model will be visualized using the non-normalized and normalized confusion matrices. "],"metadata":{"id":"EDT4hbi_J1FJ"}},{"cell_type":"markdown","source":["For theoretical knowledge, please refer [this](https://analyticsindiamag.com/transfer-learning-for-multi-class-image-classification-using-deep-convolutional-neural-network/) article."],"metadata":{"id":"qDUZyvKZK76n"}},{"cell_type":"markdown","source":["## **The Dataset**"],"metadata":{"id":"BuzwUqDPLDF9"}},{"cell_type":"markdown","source":["In this experiment, we will be using the CIFAR-10 dataset that is a publically available image data set provided by the Canadian Institute for Advanced Research (CIFAR). It consists of 60000 32Ã—32 colour images in 10 classes, with 6000 images per class. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 50000 training images and 10000 test images in this dataset."],"metadata":{"id":"gZw5zE2YLFyP"}},{"cell_type":"markdown","source":["## **Implementation in Python**"],"metadata":{"id":"P50SHzUFLH_X"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tensorflow keras torch torchvision \\\n","    tqdm scikit-image --user -q --no-warn-script-location\n","\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#Keras library for CIFAR-10 dataset\n","from keras.datasets import cifar10\n","#Downloading the CIFAR dataset\n","(x_train,y_train),(x_test,y_test)=cifar10.load_data()"],"outputs":[],"metadata":{"id":"FALQXKXAIdvj"}},{"cell_type":"markdown","source":["We will import the remaining libraries that are going to be required in our experiment."],"metadata":{"id":"iTkkm8VWLNoE"}},{"cell_type":"code","execution_count":null,"source":["#importing other required libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.utils.multiclass import unique_labels\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sns\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from keras import Sequential\n","from keras.applications.vgg19 import VGG19 #For Transfer Learning\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import SGD,Adam\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n","from keras.utils.np_utils import to_categorical"],"outputs":[],"metadata":{"id":"w06A09lcLP9l"}},{"cell_type":"markdown","source":["Here, we will split the downloaded dataset into training, test and validation sets."],"metadata":{"id":"PUVmKfrZLUk3"}},{"cell_type":"code","execution_count":null,"source":["#defining training and test sets\n","x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)"],"outputs":[],"metadata":{"id":"O1Tf1B3mLT_v"}},{"cell_type":"markdown","source":["Once split, we will see the shape of our data. It should be same as given in the dataset description at its parent website."],"metadata":{"id":"a0LsQwiAL3GB"}},{"cell_type":"code","execution_count":null,"source":["#Dimension of the dataset\n","print((x_train.shape,y_train.shape))\n","print((x_val.shape,y_val.shape))\n","print((x_test.shape,y_test.shape))"],"outputs":[],"metadata":{"id":"0NGEmHZeL4qc"}},{"cell_type":"markdown","source":["We need to do one hot encoding here because we have 10 classes and we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10"],"metadata":{"id":"SRo1BZWAL6Yv"}},{"cell_type":"code","execution_count":null,"source":["#One Hot Encoding\n","y_train=to_categorical(y_train)\n","y_val=to_categorical(y_val)\n","y_test=to_categorical(y_test)"],"outputs":[],"metadata":{"id":"VWtrz-N9L8EO"}},{"cell_type":"markdown","source":["After one hot encoding, we will ensure that we have obtained the required shape."],"metadata":{"id":"O-UalMUhL9yh"}},{"cell_type":"code","execution_count":null,"source":["#Verifying the dimension after one hot encoding\n","print((x_train.shape,y_train.shape))\n","print((x_val.shape,y_val.shape))\n","print((x_test.shape,y_test.shape))"],"outputs":[],"metadata":{"id":"mxIt8v9kL_7P"}},{"cell_type":"markdown","source":["Here, we will perform the image data augmentation. This is the technique that is used to expand the size of a training dataset by creating modified versions of images in the dataset. First, we will define individual instances of ImageDataGenerator for augmentation and then we will fit them with each of the training, test and validation datasets. "],"metadata":{"id":"ymEmMXMbMCQ_"}},{"cell_type":"code","execution_count":null,"source":["#Image Data Augmentation\n","train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=.1)\n","\n","val_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=.1)\n","\n","test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True, zoom_range=.1)\n","\n","#Fitting the augmentation defined above to the data\n","train_generator.fit(x_train)\n","val_generator.fit(x_val)\n","test_generator.fit(x_test)"],"outputs":[],"metadata":{"id":"9BJWbj5ZMEcS"}},{"cell_type":"markdown","source":["We will use the learning rate annealer in this experiment. The learning rate annealer decreases the learning rate after a certain number of epochs if the error rate does not change. Here, through this technique, we will monitor the validation accuracy and if it seems to be a plateau in 3 epochs, it will reduce the learning rate by 0.01."],"metadata":{"id":"f_FEAs4tMHim"}},{"cell_type":"code","execution_count":null,"source":["#Learning Rate Annealer\n","lrr= ReduceLROnPlateau(monitor='val_acc', factor=.01, patience=3, min_lr=1e-5)"],"outputs":[],"metadata":{"id":"GmbVxZOTMGWJ"}},{"cell_type":"markdown","source":["Now, we will instantiate the VGG19 that is a deep convolutional neural network as a transfer learning model."],"metadata":{"id":"poys4g_aMM1k"}},{"cell_type":"markdown","source":["## **Defining VGG19 as a Deep Convolutional Neural Network**"],"metadata":{"id":"her19YYJMOm8"}},{"cell_type":"code","execution_count":null,"source":["#Defining the VGG Convolutional Neural Net\n","base_model = VGG19(include_top = False, weights = 'imagenet', input_shape = (32,32,3), classes = y_train.shape[1])"],"outputs":[],"metadata":{"id":"oErU0Vh8MSzS"}},{"cell_type":"markdown","source":["Now, we will define VGG19 as a deep learning architecture. For this purpose, it will be defined as a Keras Sequential model with several dense layers. "],"metadata":{"id":"oQohWgDUMhl-"}},{"cell_type":"code","execution_count":null,"source":["#Adding the final layers to the above base models where the actual classification is done in the dense layers\n","model= Sequential()\n","model.add(base_model) \n","model.add(Flatten()) "],"outputs":[],"metadata":{"id":"WvGD80DHMjW0"}},{"cell_type":"markdown","source":["Now, to add further layers, we need to see the dimension of our model."],"metadata":{"id":"JM3qZeCtMlOV"}},{"cell_type":"code","execution_count":null,"source":["#Model summary\n","model.summary()"],"outputs":[],"metadata":{"id":"KZSebIshMngS"}},{"cell_type":"code","execution_count":null,"source":["#Adding the Dense layers along with activation and batch normalization\n","model.add(Dense(1024,activation=('relu'),input_dim=512))\n","model.add(Dense(512,activation=('relu'))) \n","model.add(Dense(256,activation=('relu'))) \n","model.add(Dropout(.3))\n","model.add(Dense(128,activation=('relu')))\n","#model.add(Dropout(.2))\n","model.add(Dense(10,activation=('softmax'))) \n","\n","#Checking the final model summary\n","model.summary()\n"],"outputs":[],"metadata":{"id":"jiKbRLvAMpzZ"}},{"cell_type":"markdown","source":["As we have defined our model, now we need to initialize the hyperparameters that are required to train the model and then finally, we will compile our model."],"metadata":{"id":"aQ4lDSl5MsqI"}},{"cell_type":"code","execution_count":null,"source":["#Initializing the hyperparameters\n","batch_size= 100\n","epochs=50\n","learn_rate=.001\n","sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n","adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"5SPJxk2nMvr_"}},{"cell_type":"markdown","source":["Now, we start training our VGG10, the deep convolutional neural network model."],"metadata":{"id":"zO4NMcivMm3E"}},{"cell_type":"code","execution_count":null,"source":["#Training the model\n","model.fit_generator(train_generator.flow(x_train, y_train, batch_size= batch_size),\n","                    epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, \n","                    validation_data = val_generator.flow(x_val, y_val, batch_size = batch_size), \n","                    validation_steps = 250, callbacks=[lrr], verbose = 1)"],"outputs":[],"metadata":{"id":"S4Kh4CksM2Fq"}},{"cell_type":"markdown","source":["As we can see in the above picture, we have achieved the training accuracy by 99.22% and validation accuracy by 85.41%. Now we will visualize the accuracy and loss during training."],"metadata":{"id":"-TTBqe9ZPZdW"}},{"cell_type":"code","execution_count":null,"source":["#Plotting the training and validation loss and accuracy\n","f,ax=plt.subplots(2,1) \n","\n","#Loss\n","ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n","ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n","\n","#Accuracy\n","ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n","ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')"],"outputs":[],"metadata":{"id":"2xflU93QPb2I"}},{"cell_type":"markdown","source":["We will make image class predictions through this model using the test data set."],"metadata":{"id":"96BtiGUoPeYe"}},{"cell_type":"code","execution_count":null,"source":["#Making prediction\n","y_pred=model.predict_classes(x_test)\n","y_true=np.argmax(y_test,axis=1)"],"outputs":[],"metadata":{"id":"SQ5dB6FHPgAJ"}},{"cell_type":"markdown","source":["## **Performance of VGG19 â€“ The Deep Convolutional Neural Network**\n","\n","Finally, we will visualize the classification performance on test data using confusion matrices. "],"metadata":{"id":"SggbrLA0Ph-2"}},{"cell_type":"code","execution_count":null,"source":["#Defining function for confusion matrix plot\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    #Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","#print(cm)\n","\n","    fig, ax = plt.subplots(figsize=(7,7))\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","\n","    #Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax\n","\n","np.set_printoptions(precision=2)"],"outputs":[],"metadata":{"id":"FDd9hp0YPlcl"}},{"cell_type":"markdown","source":["First, we will see the exact number of correct and incorrect classification using the non-normalized confusion matrix and then we will see the same in percentage using the normalized confusion matrix. "],"metadata":{"id":"w2Mjbt8APoGe"}},{"cell_type":"code","execution_count":null,"source":["#Plotting the confusion matrix\n","confusion_mtx = confusion_matrix(y_true, y_pred)\n","\n","#Defining the class labels\n","class_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Plotting non-normalized confusion matrix\n","plot_confusion_matrix(y_true, y_pred, classes = class_names, title='Confusion matrix, without normalization')"],"outputs":[],"metadata":{"id":"ZIYZg4MCPqHW"}},{"cell_type":"code","execution_count":null,"source":["#Plotting normalized confusion matrix\n","plot_confusion_matrix(y_true, y_pred, classes = class_names, normalize = True, title = 'Normalized confusion matrix')"],"outputs":[],"metadata":{"id":"wnGyn2p2PtU9"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Transfer Learning for multi class classification](https://analyticsindiamag.com/transfer-learning-for-multi-class-image-classification-using-deep-convolutional-neural-network/)\n","\n","> * [FastAI with PyTorch for multiclass Image Classification](https://analyticsindiamag.com/fastai-with-tpu-in-pytorch-for-multiclass-image-classification/)\n","\n","> * [SuffleNet V1 for Multiclass Image Classification](https://analyticsindiamag.com/complete-guide-to-shufflenet-v1-with-implementation-in-multiclass-image-classification/)\n","\n","> * [Image Compression using K-Means Clustering](https://analyticsindiamag.com/beginners-guide-to-image-compression-using-k-means-clustering/)\n","\n","> * [PCA for images](https://analyticsindiamag.com/how-does-pca-dimension-reduction-work-for-images/)\n","\n","> * [Image Generation with Tensorflow Keras](https://analyticsindiamag.com/getting-started-image-generation-tensorflow-keras/)\n","\n"],"metadata":{"id":"1ZtPNx78qe5T"}}]}