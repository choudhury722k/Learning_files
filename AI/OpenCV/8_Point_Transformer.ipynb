{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Point_Transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOY0UnzbvPfHqjgnvxDHiQV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Point Transformers**"],"metadata":{"id":"AF1GUw-d8EJE"}},{"cell_type":"markdown","source":["Transformers outshine convolutional neural networks and recurrent neural networks in many applications from various domains, including natural language processing, image classification and medical image segmentation. Point Transformer is introduced to establish state-of-the-art performances in 3D image data processing as another piece of evidence. Point Transformer is robust to perform multiple tasks such as 3D image semantic segmentation, 3D image classification and 3D image part segmentation."],"metadata":{"id":"El_D_d0A8DYM"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/how-point-transformer-excels-in-3d-image-processing/) article."],"metadata":{"id":"3NA36dx68JpB"}},{"cell_type":"markdown","source":["## **Python Implementation of Point Transformer**"],"metadata":{"id":"68CKBRJC877P"}},{"cell_type":"markdown","source":["Point Transformer is available as a PyPi package. It can be simply pip installed to use in applications. Point Transformer is implemented in the PyTorch environment. Its requirements are Python 3.7+, PyTorch 1.6+ and einops 0.3+."],"metadata":{"id":"cRa3NkSJ8-_Z"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras opencv-python pillow scikit-image torch torchvision \\\n","    point-transformer-pytorch einops --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Import the necessary libraries and modules."],"metadata":{"id":"VtR1ySVo9DAG"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","from point_transformer_pytorch import PointTransformerLayer "],"outputs":[],"metadata":{"id":"VFD9fL129FPL","executionInfo":{"status":"ok","timestamp":1622708368094,"user_tz":-330,"elapsed":444,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["An example implementation of a Point Transformer layer is provided in the following codes."],"metadata":{"id":"leRldiDr9KEz"}},{"cell_type":"code","execution_count":null,"source":["attn = PointTransformerLayer(\n","    dim = 128,\n","    pos_mlp_hidden_dim = 64,\n","    attn_mlp_hidden_mult = 4\n",")\n","feats = torch.randn(1, 16, 128)\n","pos = torch.randn(1, 16, 3)\n","mask = torch.ones(1, 16).bool()\n","attn(feats, pos, mask = mask) # (1, 16, 128) "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCnGtHbm9NNB","executionInfo":{"status":"ok","timestamp":1622708381937,"user_tz":-330,"elapsed":391,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"c0a56aa2-330a-4b9f-8495-825ec92115bf"}},{"cell_type":"markdown","source":["Number of nearest neighbors can be controlled through the corresponding argument in the PointTransformerLayer module. In the following example implementation, the number of nearest neighbors is set to 16. While processing, the layer will consider 16 nearest points in the 3D cloud space."],"metadata":{"id":"rtafJlES9QB3"}},{"cell_type":"code","execution_count":null,"source":["attn = PointTransformerLayer(\n","    dim = 128,\n","    pos_mlp_hidden_dim = 64,\n","    attn_mlp_hidden_mult = 4,\n","    num_neighbors = 16          \n","    # only the 16 nearest neighbors would be attended to for each point\n",")\n","feats = torch.randn(1, 2048, 128)\n","pos = torch.randn(1, 2048, 3)\n","mask = torch.ones(1, 2048).bool()\n","attn(feats, pos, mask = mask) # (1, 16, 128) "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fU_MJ8L09SF6","executionInfo":{"status":"ok","timestamp":1622708409623,"user_tz":-330,"elapsed":6394,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"909d4f0d-762b-45ea-a42b-b9c15e8d196c"}},{"cell_type":"markdown","source":["The background source implementation of PointTransformerLayer is expressed in the following codes. The PyTorch environment is created by importing the necessary packages."],"metadata":{"id":"egepNzsT9U1v"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","from torch import nn, einsum\n","from einops import repeat "],"outputs":[],"metadata":{"id":"HrulEYL69Wm6","executionInfo":{"status":"ok","timestamp":1622708421145,"user_tz":-330,"elapsed":591,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Helper functions for the layer development are defined as follows:"],"metadata":{"id":"zK97PiYK9ZQl"}},{"cell_type":"code","execution_count":null,"source":["def exists(val):\n","    return val is not None\n","def max_value(t):\n","    return torch.finfo(t.dtype).max\n","def batched_index_select(values, indices, dim = 1):\n","    value_dims = values.shape[(dim + 1):]\n","    values_shape, indices_shape = map(lambda t: list(t.shape), (values, indices))\n","    indices = indices[(..., *((None,) * len(value_dims)))]\n","    indices = indices.expand(*((-1,) * len(indices_shape)), *value_dims)\n","    value_expand_len = len(indices_shape) - (dim + 1)\n","    values = values[(*((slice(None),) * dim), *((None,) * value_expand_len), ...)]\n","    value_expand_shape = [-1] * len(values.shape)\n","    expand_slice = slice(dim, (dim + value_expand_len))\n","    value_expand_shape[expand_slice] = indices.shape[expand_slice]\n","    values = values.expand(*value_expand_shape)\n","    dim += value_expand_len\n","    return values.gather(dim, indices) "],"outputs":[],"metadata":{"id":"d0xOaGeK9aKJ","executionInfo":{"status":"ok","timestamp":1622708445838,"user_tz":-330,"elapsed":926,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Finally, the layer is developed on top of PyTorchâ€™s nn module as a Python Class. It performs masking, attention and aggregation through its forward method."],"metadata":{"id":"bBpse4Yc9fkw"}},{"cell_type":"code","execution_count":null,"source":["class PointTransformerLayer(nn.Module):\n","     def __init__(\n","         self,\n","         *,\n","         dim,\n","         pos_mlp_hidden_dim = 64,\n","         attn_mlp_hidden_mult = 4,\n","         num_neighbors = None\n","     ):\n","         super().__init__()\n","         self.num_neighbors = num_neighbors\n","         self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\n","         self.pos_mlp = nn.Sequential(\n","             nn.Linear(3, pos_mlp_hidden_dim),\n","             nn.ReLU(),\n","             nn.Linear(pos_mlp_hidden_dim, dim)\n","         )\n","         self.attn_mlp = nn.Sequential(\n","             nn.Linear(dim, dim * attn_mlp_hidden_mult),\n","             nn.ReLU(),\n","             nn.Linear(dim * attn_mlp_hidden_mult, dim),\n","         )\n","     def forward(self, x, pos, mask = None):\n","         n, num_neighbors = x.shape[1], self.num_neighbors\n","         # get queries, keys, values\n","         q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n","         # calculate relative positional embeddings\n","         rel_pos = pos[:, :, None, :] - pos[:, None, :, :]\n","         rel_pos_emb = self.pos_mlp(rel_pos)\n","         # use subtraction of queries to keys. i suppose this is a better inductive bias for point clouds than dot product\n","         qk_rel = q[:, :, None, :] - k[:, None, :, :]\n","         # prepare mask\n","         if exists(mask):\n","             mask = mask[:, :, None] * mask[:, None, :]\n","         # expand values\n","         v = repeat(v, 'b j d -> b i j d', i = n)\n","         # determine k nearest neighbors for each point, if specified\n","         if exists(num_neighbors) and num_neighbors < n:\n","             rel_dist = rel_pos.norm(dim = -1)\n","             if exists(mask):\n","                 mask_value = max_value(rel_dist)\n","                 rel_dist.masked_fill_(~mask, mask_value)\n","             dist, indices = rel_dist.topk(num_neighbors, largest = False)\n","             v = batched_index_select(v, indices, dim = 2)\n","             qk_rel = batched_index_select(qk_rel, indices, dim = 2)\n","             rel_pos_emb = batched_index_select(rel_pos_emb, indices, dim = 2)\n","             mask = batched_index_select(mask, indices, dim = 2) if exists(mask) else None\n","         # add relative positional embeddings to value\n","         v = v + rel_pos_emb\n","         # use attention mlp, making sure to add relative positional embedding first\n","         sim = self.attn_mlp(qk_rel + rel_pos_emb)\n","         # masking\n","         if exists(mask):\n","             mask_value = -max_value(sim)\n","             sim.masked_fill_(~mask[..., None], mask_value)\n","         # attention\n","         attn = sim.softmax(dim = -2)\n","         # aggregate\n","         agg = einsum('b i j d, b i j d -> b i d', attn, v)\n","         return agg "],"outputs":[],"metadata":{"id":"EY-dSZDj9iK8","executionInfo":{"status":"ok","timestamp":1622708467866,"user_tz":-330,"elapsed":1153,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Point Transformers](https://analyticsindiamag.com/how-point-transformer-excels-in-3d-image-processing/)\n","\n","> * [Comparison of Transfer Learning with Multi Class Classification](https://analyticsindiamag.com/practical-comparison-of-transfer-learning-models-in-multi-class-image-classification/)\n","\n","> * [Fruit Recognition with CNN](https://analyticsindiamag.com/fruit-recognition-using-the-convolutional-neural-network/)\n","\n","> * [Semantic Segmentation Using TensorFlow Keras](https://analyticsindiamag.com/semantic-segmentation-using-tensorflow-keras/)\n","\n","> * [Convert Image to Pencil Sketch](https://analyticsindiamag.com/converting-image-into-a-pencil-sketch-in-python/)\n","\n","> * [Image Classification Task with and without Data Augmentation](https://analyticsindiamag.com/image-data-augmentation-impacts-performance-of-image-classification-with-codes/)\n"],"metadata":{"id":"1ZtPNx78qe5T"}}]}