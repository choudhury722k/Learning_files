{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Face_Swapping_Application_With_OpenCV.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOjf7tQUSeMzhCyzCYIRh05"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **Face-Swapping Application With OpenCV**"],"metadata":{"id":"KF7QDf0rTgfv"}},{"cell_type":"markdown","source":["In this practice session, we will implement a face-swapping technique for two images of celebrities using OpenCV and python.\n","\n","Steps used for this project:\n","\n","> * Taking two images – one as the source and another as a destination.\n","> * Using the dlib landmark detector on both these images. \n","> * Joining the dots in the landmark detector to form triangles. \n","> * Extracting these triangles\n","> * Placing the source image on the destination\n","> * Smoothening the face"],"metadata":{"id":"ptHhZzP-TrfM"}},{"cell_type":"markdown","source":["## **Image selection**"],"metadata":{"id":"g-bLpdR-T2TZ"}},{"cell_type":"markdown","source":["You can select any two images of your choice. It could be your image and your friend, it could be two celebrity images as well. I have selected two images of Shah Rukh Khan  and Katrina Kaif. Both the images are front-facing and are well lit."],"metadata":{"id":"JdbHeiG2T5yJ"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://i.pinimg.com/originals/d4/a8/74/d4a8749079d5f0dd1184ddceea4996d3.jpg "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gmp9amGkTCS0","executionInfo":{"status":"ok","timestamp":1622462933711,"user_tz":-330,"elapsed":611,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"9639fe87-2cb9-4ed7-e228-ebdccee7ad20"}},{"cell_type":"code","execution_count":null,"source":["# !wget https://cdn.faceshapeapp.com/q/QdJqlgb_ahcGBtry__1080x1440.jpg "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdgst6V3USJW","executionInfo":{"status":"ok","timestamp":1622462959188,"user_tz":-330,"elapsed":343,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"0ef3e29f-3cfe-475a-fe83-d929c906e547"}},{"cell_type":"markdown","source":["Now let us read these images using OpenCV code."],"metadata":{"id":"Xh07lAvtU0t4"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim scikit-image opencv-python pillow dlib --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import cv2\n","import numpy as np\n","import dlib\n","import time\n","source_image = cv2.imread(\"d4a8749079d5f0dd1184ddceea4996d3.jpg\")\n","source_image_gray = cv2.cvtColor(source_image, cv2.COLOR_BGR2GRAY)\n","dest_image = cv2.imread(\"QdJqlgb_ahcGBtry__1080x1440.jpg\")\n","dest_image_gray = cv2.cvtColor(dest_image, cv2.COLOR_BGR2GRAY)\n","# Create empty matrices in the images' shapes\n","height, width = source_image_gray.shape\n","mask = np.zeros((height, width), np.uint8)\n","height, width, channels = dest_image_gray.shape"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212},"id":"SboEsiDiU0IO","executionInfo":{"status":"error","timestamp":1622466930196,"user_tz":-330,"elapsed":517,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"d67c056a-c4c9-4c17-900c-c07e48bb3045"}},{"cell_type":"markdown","source":["## **Using the dlib landmark detector on the images**"],"metadata":{"id":"qvTzPpaDVMSv"}},{"cell_type":"markdown","source":["Dlib is a python library that provides us with landmark detectors to detect important facial landmarks. These 68 points are important to identify the different features in both faces."],"metadata":{"id":"f6f7tRuBVRQN"}},{"cell_type":"markdown","source":["Once we have the 68 points shape predictor downloaded, let us apply them to the first face. \n","\n","Then we will use the convexhull to detect the faces after using the landmark detector. "],"metadata":{"id":"otrodmrdVVuz"}},{"cell_type":"code","execution_count":null,"source":["# !wget -nd https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wneRiDMQVrjv","executionInfo":{"status":"ok","timestamp":1622465752964,"user_tz":-330,"elapsed":1899,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"6d148b54-8f71-454e-8452-9474c7d59a84"}},{"cell_type":"code","execution_count":null,"source":["# Loading models and predictors of the dlib library to detect landmarks in both faces\n","detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor(\"https://gitlab.com/AnalyticsIndiaMagazine/practicedatasets/-/raw/main/face_swapping/shape_predictor_68_face_landmarks.dat\")\n","\n","# Getting landmarks for the face that will be swapped into to the body\n","rect = detector(source_image_gray)[0]\n","\n","# This creates a with 68 pairs of integer values — these values are the (x, y)-coordinates of the facial structures \n","landmarks = predictor(source_image_gray, rect)\n","landmarks_points = [] \n","\n","def get_landmarks(landmarks, landmarks_points):\n","  for n in range(68):\n","      x = landmarks.part(n).x\n","      y = landmarks.part(n).y\n","      landmarks_points.append((x, y))\n","\n","get_landmarks(landmarks, landmarks_points)\n","\n","points = np.array(landmarks_points, np.int32)"],"outputs":[],"metadata":{"id":"RbDFtfKJVXl3","executionInfo":{"status":"ok","timestamp":1622466940556,"user_tz":-330,"elapsed":1931,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["These will create 68 points on the face as shown below."],"metadata":{"id":"3yZ2yNJQWMTG"}},{"cell_type":"markdown","source":["For destination face:"],"metadata":{"id":"fVTSyefxWOeh"}},{"cell_type":"code","execution_count":null,"source":["convexhull = cv2.convexHull(points) \n","\n","face_cp = face.copy()\n","plt.imshow(cv2.cvtColor((cv2.polylines(face_cp, [convexhull], True, (255,255,255), 3)), cv2.COLOR_BGR2RGB))\n","\n","face_image_1 = cv2.bitwise_and(face, face, mask=mask)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"fedT7ht1WQm6","executionInfo":{"status":"error","timestamp":1622466958722,"user_tz":-330,"elapsed":976,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"5afef8b7-47b9-4035-afe4-77bfe1cab313"}},{"cell_type":"code","execution_count":null,"source":["convexhull = cv2.convexHull(points) \n","face_cp = face.copy()\n","plt.imshow(cv2.cvtColor((cv2.polylines(face_cp, [convexhull], True, (255,255,255), 3)), cv2.COLOR_BGR2RGB))\n","face_image_1 = cv2.bitwise_and(face, face, mask=mask)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"GfSAM2L7i5TK","executionInfo":{"status":"error","timestamp":1622466610208,"user_tz":-330,"elapsed":613,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"1287048d-770c-4f89-91f3-b9a2d503c731"}},{"cell_type":"markdown","source":["## **Joining the dots in the landmark detector to form triangles for the source image.**"],"metadata":{"id":"_CIFPRAuWUCk"}},{"cell_type":"markdown","source":["To cut a portion of the face and fit it to the other we need to analyse the size and perspective of both the images. To do this, we will split the entire face into smaller triangles by joining the landmarks so that the originality of the image is not lost and it becomes easier to swap the triangles with the destination image. This entire process is called Delaunay triangulation. \n","\n","Note: Since we do this with respect to the facial landmark of the source the following code has to be put inside the for loop of source image. "],"metadata":{"id":"SDq55Q2QWXEP"}},{"cell_type":"code","execution_count":null,"source":["rectangle = cv2.boundingRect(convexhull)\n","divide_2d = cv2.Subdiv2D(rectangle)\n","divide_2d.insert(landmarks_points)\n","split_triangle = divide_2d.getTriangleList()\n","split_triangle = np.array(split_triangle, dtype=np.int32)\n","cv2.fillConvexPoly(mask, convexhull, 255)\n","face_image_1 = cv2.bitwise_and(source_image, source_image, mask=mask)\n","face_points2 = np.array(points2, np.int32)\n","convexhull2 = cv2.convexHull(face_points2)\n","def extract_index_nparray(nparray):\n","    index = None\n","    for num in nparray[0]:\n","        index = num\n","        break\n","    return index\n","    join_indexes = []\n","    for edge in triangles:\n","        first = (edge[0], edge[1])\n","        second = (edge[2], edge[3])\n","        third = (edge[4], edge[5])\n","        index_edge1 = np.where((points == first).all(axis=1))\n","        index_edge1 = extract_index_nparray(index_edge1)\n","        index_edge2 = np.where((points == pt2).all(axis=1))\n","        index_edge2 = extract_index_nparray(index_edge2)\n","        index_edge3 = np.where((points == pt3).all(axis=1))\n","        index_edge3 = extract_index_nparray(index_edge3)\n","        if index_edge1 is not None and index_edge2 is not None and index_edge3 is not None:\n","            triangle = [index_edge1, index_edge2, index_edge3]\n","            join_indexes.append(triangle)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"TH-rYAy3WZct","executionInfo":{"status":"error","timestamp":1622465768730,"user_tz":-330,"elapsed":393,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"78a25fa1-2326-40f7-d824-528ee97155a7"}},{"cell_type":"markdown","source":["Now that we have the triangles for the source image we need to make sure the same space can be extracted from the destination so that the overlapping can be done smoothly. \n","\n","To do this, we follow a slightly different approach to the destination image. \n","\n","The destination image needs to have the same patterns of triangles as the source. \n","\n","To do this we will create masks for the images as follows. \n","\n","First, let us create the source image. We are trying to match the patterns of first and second images here. "],"metadata":{"id":"FeNMdaMxWgNR"}},{"cell_type":"code","execution_count":null,"source":["source_mask = np.zeros_like(source_image_gray)\n","new_face = np.zeros_like(dest_image)\n","for index in indexes_triangles:\n","    tri_one = points[index[0]]\n","    tri_two = points[index[1]]\n","    tri_three = points[index[2]]\n","    triangle1 = np.array([tri_one, tri_two, tri_three], np.int32)\n","    first_rect = cv2.boundingRect(triangle1)\n","    (x, y, w, h) = first_rect\n","    cropped_triangle = source_image[y: y + h, x: x + w]\n","    cropped_tr1_mask = np.zeros((h, w), np.uint8)\n","    pts = np.array([[tri_one[0] - x, tri_one[1] - y],\n","                       [tri_two[0] - x, tri_two[1] - y],\n","                       [tri_three[0] - x, tri_three[1] - y]], np.int32)\n","    cv2.fillConvexPoly(cropped_tr1_mask, pts, 255)\n","    cv2.line(source_mask, tri_one, tri_two, 255)\n","    cv2.line(source_mask, tri_two, tri_three, 255)\n","    cv2.line(source_mask, tri_one, tri_three, 255)"],"outputs":[],"metadata":{"id":"7NM70iKNWiXb"}},{"cell_type":"markdown","source":["As you can see we have created a mask of the image."],"metadata":{"id":"e2M2iLMeWkR5"}},{"cell_type":"markdown","source":["Once we have the mask for the source image and the triangle points we can do the same for the destination as well. But for the destination, we need to crop out the region corresponding to the source mask. We can do this as shown below. "],"metadata":{"id":"btDDzFbWWmWO"}},{"cell_type":"code","execution_count":null,"source":["tri2_one = points2[index[0]]\n","tri2_two = points2[index[1]]\n","tri2_three = points2[index[2]]\n","triangle2 = np.array([tri2_one, tri2_two, tri2_three], np.int32)\n","second_rect = cv2.boundingRect(triangle2)\n","(x, y, w, h) = second_rect\n","cropped = np.zeros((h, w), np.uint8)\n","points2 = np.array([[tri2_one[0] - x, tri2_one[1] - y],\n","                    [tri2_two[0] - x, tri2_two[1] - y],\n","                    [tri2_three[0] - x, tri2_three[1] - y]], np.int32)\n","cv2.fillConvexPoly(cropped, points2, 255)"],"outputs":[],"metadata":{"id":"ZTBUUducWo_1"}},{"cell_type":"markdown","source":["This is the destination image triangles. "],"metadata":{"id":"Rk6M0IvRWu74"}},{"cell_type":"markdown","source":["## **Extracting these triangles**"],"metadata":{"id":"F-8KazATWwk5"}},{"cell_type":"markdown","source":["Once we have the triangles in source and destination the next step is to extract them from the source image. But we also need to take the coordinates of the destination triangles so that the sizes of the two can match. This technique is also called warping. "],"metadata":{"id":"mDmnVWWeW3Uf"}},{"cell_type":"code","execution_count":null,"source":["points = np.float32(points)\n","points2 = np.float32(points2)\n","transform = cv2.getAffineTransform(points, points2)\n","warping = cv2.warpAffine(cropped_triangle, transform, (w, h))\n","warping = cv2.bitwise_and(warping, warping, mask=cropped)"],"outputs":[],"metadata":{"id":"MJpxrOnQW5Gi"}},{"cell_type":"markdown","source":["## **Placing the source image on the destination**"],"metadata":{"id":"YD7gLqcRW7Bg"}},{"cell_type":"markdown","source":["Now, we can reconstruct the destination image and start placing the source image on the destination one. First we will make some alterations in the destination face. Then, we will make sure the lines created do not appear in the final output."],"metadata":{"id":"lsRCzB3DW-FB"}},{"cell_type":"code","execution_count":null,"source":["ht, wt, filters = dest_image.shape\n","dest_face = np.zeros((ht, wt, filters), np.uint8)\n","facial_area = dest_face[y: y + h, x: x + w]\n","facial_area_gray = cv2.cvtColor(facial_area, cv2.COLOR_BGR2GRAY)\n","_,triangle_mask = cv2.threshold(facial_area_gray, 1, 255, cv2.THRESH_BINARY_INV)\n","warping = cv2.bitwise_and(warping, warping, mask=triangle_mask)\n","facial_area = cv2.add(facial_area, warping)\n","dest_face[y: y + h, x: x + w] = facial_area"],"outputs":[],"metadata":{"id":"8TTp_xWnW9gS"}},{"cell_type":"markdown","source":["Finally, we will place the source image on the destination"],"metadata":{"id":"VPrEJZd1XHo3"}},{"cell_type":"code","execution_count":null,"source":["final_mask = np.zeros_like(dest_image_gray)\n","head_mask = cv2.fillConvexPoly(final_mask, convexhull2, 255)\n","final_mask = cv2.bitwise_not(head_mask)\n","combine = cv2.bitwise_and(dest_image, dest_image, mask=final_mask)\n","output = cv2.add(combine, dest_face)"],"outputs":[],"metadata":{"id":"JPZgKQtFXJgq"}},{"cell_type":"markdown","source":["Thought the images have been swapped, there is no match in color or smoothness in the swapping. To eliminate this we need to do another process."],"metadata":{"id":"gVHQDzbRXLNV"}},{"cell_type":"markdown","source":["## **Smoothening the face**\n","\n","The final step is to change the colours and to make the swapping look better. To do this OpenCV provides a library called seamless cloning. "],"metadata":{"id":"sBiMfDwsXM_b"}},{"cell_type":"code","execution_count":null,"source":["(x, y, w, h) = cv2.boundingRect(convexhull2)\n","seamless= (int((x + x + w) / 2), int((y + y + h) / 2))\n","seamlessclone = cv2.seamlessClone(output, dest_image, head_mask, seamless, cv2.NORMAL_CLONE)\n","cv2.imshow(\"seamlessclone\", seamlessclone)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"outputs":[],"metadata":{"id":"hiYFMMlcXPya"}},{"cell_type":"markdown","source":["You can see that this has some gradient and better fit when compared to the previous output. "],"metadata":{"id":"Pv3EnGJmXR-f"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Face Swaping with OpenCV](https://analyticsindiamag.com/a-fun-project-on-building-a-face-swapping-application-with-opencv/)\n","\n","> * [Create Watermark Images with OpenCV](https://analyticsindiamag.com/how-to-create-a-watermark-on-images-using-opencv/)\n","\n","> * [Convert Image to Cartoon](https://analyticsindiamag.com/converting-an-image-to-a-cartoon/)\n","\n","> * [Sudoku Game with Deep Learning, OpenCV and Backtracking](https://analyticsindiamag.com/solve-sudoku-puzzle-using-deep-learning-opencv-and-backtracking/)\n","\n","> * [Finding Waldo Game with OpenCV](https://analyticsindiamag.com/my-fun-project-with-opencv-finding-waldo-game/)\n","\n","> * [OpenCV to Extract Information From Table Images](https://analyticsindiamag.com/how-to-use-opencv-to-extract-information-from-table-images/)\n"],"metadata":{"id":"jqDB1l9I6Br-"}}]}