{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE68_hxqXyLt"
   },
   "source": [
    "# Compute Relevancy Of Transformer Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBwBy1fiX13F"
   },
   "source": [
    "Self-attention models, specifically Transformers have taken the computer vision field by storm be it OpenAI’s DALL-E or Google’s ViT models.  This creates a need for tools that can interpret and visualize the decision process behind transformer models. These visualizations can be used to debug models and verify that the models are fair and unbiased. A new approach for computing token relevance for Transformer models was proposed in the paper “Transformer Interpretability Beyond Attention Visualization” by Hila Chefer, Shir Gur, and Lior Wolf. The method assigns local relevance based on the Deep Taylor Decomposition and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip-connections; both involve the mixing activation maps and have poised unique challenges to existing approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBbx37cX250"
   },
   "source": [
    "To learn more about its architecture, please refer [this](https://analyticsindiamag.com/compute-relevancy-of-transformer-networks-via-novel-interpretable-transformer/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8T5vnEvoaV-"
   },
   "source": [
    "# Get the GitHub repo and install the requirements\n",
    "\n",
    "You'll have to restart the runtime, make sure to navigate into the newly created directory once you restart runtime otherwise you'll encounter `ImportError`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHSXMunZYEbl"
   },
   "source": [
    "Clone the Transformer-Explainability GitHub repository, navigate into the newly created Transformer-Explainability directory and install the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras opencv-python pillow scikit-image torch torchvision \\\n",
    "     tqdm --user -q --no-warn-script-location\n",
    "\n",
    "!python -m pip install timm==0.3.2 --user -q\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3676,
     "status": "ok",
     "timestamp": 1623908700458,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "ej3H_oMmAx3C",
    "outputId": "4d4adcfb-86de-4c56-8bde-fd0268488e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Transformer-Explainability' already exists and is not an empty directory.\n",
      "Requirement already satisfied: Pillow>=8.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (8.2.0)\n",
      "Requirement already satisfied: einops==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: matplotlib==3.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.19.2)\n",
      "Requirement already satisfied: opencv_python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.4.2.17)\n",
      "Requirement already satisfied: scikit_image==0.17.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.17.2)\n",
      "Requirement already satisfied: scipy==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.0)\n",
      "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.7.0)\n",
      "Requirement already satisfied: torchvision==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.8.1)\n",
      "Requirement already satisfied: tqdm==4.51.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.51.0)\n",
      "Requirement already satisfied: transformers==3.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.5.1)\n",
      "Requirement already satisfied: utils==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: Pygments>=2.7.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (2.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.8.0->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2021.5.30)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit_image==0.17.2->-r requirements.txt (line 8)) (2021.6.14)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit_image==0.17.2->-r requirements.txt (line 8)) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit_image==0.17.2->-r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 10)) (0.22.2.post1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirements.txt (line 11)) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirements.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirements.txt (line 11)) (0.6)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (0.1.91)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (3.12.4)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (0.9.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (0.0.45)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1->-r requirements.txt (line 14)) (20.9)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit_image==0.17.2->-r requirements.txt (line 8)) (4.4.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1->-r requirements.txt (line 14)) (57.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1->-r requirements.txt (line 14)) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1->-r requirements.txt (line 14)) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1->-r requirements.txt (line 14)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1->-r requirements.txt (line 14)) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
    "\n",
    "import os\n",
    "os.chdir(f'./Transformer-Explainability')\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhl9RhwwYHf6"
   },
   "source": [
    "You’ll have to restart the runtime after this. Make sure to navigate into the newly created directory once you restart runtime otherwise you’ll encounter ImportErrors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj6EnzRyAY5q"
   },
   "source": [
    "# **Transformer Interpretability Beyond Attention Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpda01BPYJXe"
   },
   "source": [
    "Import necessary libraries and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1623908700459,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "IdJ4YOiTBtAz"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.LOAD_TRUNCATED_IMAGES = True\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from baselines.ViT.ViT_LRP import deit_base_patch16_224 as vit_LRP\n",
    "from baselines.ViT.ViT_explanation_generator import LRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_VhBgXTOItp"
   },
   "source": [
    "#Imagenet class indices to names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL0T-INDYLev"
   },
   "source": [
    "Download the ImageNet class labels and create an index-to-class labels dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1623908700460,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "OAKXi_FGNKif",
    "outputId": "d46f4f95-3126-40fb-84b3-7f15073b0749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-17 05:45:00--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt.1’\n",
      "\n",
      "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-17 05:45:00 (96.6 MB/s) - ‘imagenet_classes.txt.1’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    index_to_class = {i: s.strip() for i, s in enumerate(f.readlines())}\n",
    "# index_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy-S7B3HFRDS"
   },
   "source": [
    "# **DeiT examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlt2mjaoYOKn"
   },
   "source": [
    "Load a pre-trained DeiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1887,
     "status": "ok",
     "timestamp": 1623908702340,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "hoM9_BRUFV96"
   },
   "outputs": [],
   "source": [
    "# initialize ViT pretrained with DeiT\n",
    "model = vit_LRP(pretrained=True).cpu()\n",
    "model.eval()\n",
    "attribution_generator = LRP(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Mqv97S7YVQk"
   },
   "source": [
    "Create two helper functions: one for visualizing the mask over images and second one for applying softmax to the final dense layer of the model to obtain the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1623908702340,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "thNjQDDAKor9"
   },
   "outputs": [],
   "source": [
    "# create heatmap from mask on image\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "#Create the function for interpreting the predictions process of DeiT model. generate_LRP is the only novel paper-specific method used in this function\n",
    "def generate_visualization(original_image, class_index=None):\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cpu(), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
    "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.reshape(224, 224).cpu).data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return vis\n",
    "\n",
    "def print_top_classes(predictions, **kwargs):    \n",
    "    # Print Top-5 predictions\n",
    "    prob = torch.softmax(predictions, dim=1)\n",
    "    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()\n",
    "    max_str_len = 0\n",
    "    class_names = []\n",
    "    for cls_idx in class_indices:\n",
    "        class_names.append(index_to_class[cls_idx])\n",
    "        if len(index_to_class[cls_idx]) > max_str_len:\n",
    "            max_str_len = len(index_to_class[cls_idx])\n",
    "    \n",
    "    print('Top 5 classes:')\n",
    "    for cls_idx in class_indices:\n",
    "        output_string = '\\t{} : {}'.format(cls_idx, index_to_class[cls_idx])\n",
    "        output_string += ' ' * (max_str_len - len(index_to_class[cls_idx])) + '\\t\\t'\n",
    "        output_string += 'value = {:.3f}\\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])\n",
    "        print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VXnz7lQYg7H"
   },
   "source": [
    "  Visualizing the relevance of image patches for particular predictions in a class-specific manner.\n",
    "\n",
    "Perform inference on the image to get the class index of the objects in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1623908702997,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "ZBtfuZOHMul9",
    "outputId": "3cfe91ce-1216-483f-bf14-91c7a03f1c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 classes:\n",
      "\t207 : golden retriever  \t\tvalue = 6.523\t prob = 35.7%\n",
      "\t208 : Labrador retriever\t\tvalue = 4.288\t prob = 3.8%\n",
      "\t285 : Egyptian cat      \t\tvalue = 3.641\t prob = 2.0%\n",
      "\t222 : kuvasz            \t\tvalue = 3.422\t prob = 1.6%\n",
      "\t281 : tabby             \t\tvalue = 2.778\t prob = 0.8%\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('samples/dogcat2.png')\n",
    "dog_cat_image = transform(image)\n",
    "\n",
    "output = model(dog_cat_image.unsqueeze(0).cpu())\n",
    "print_top_classes(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-tfPGkDYi0k"
   },
   "source": [
    "Visualize the relevance of image patches for different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJaPkTZYFv0T"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(21,7))\n",
    "axs[0].imshow(image);\n",
    "axs[0].axis('off');\n",
    "\n",
    "# dog - generate visualization for class 243: 'bull mastiff' - the predicted class\n",
    "#by default the predicted  class is visualized\n",
    "dog = generate_visualization(dog_cat_image)\n",
    "axs[1].imshow(dog);\n",
    "axs[1].axis('off');\n",
    "\n",
    "# cat - generate visualization for class 282 : 'tiger cat'\n",
    "#cat = generate_visualization(dog_cat_image, class_index=285)\n",
    "#axs[2].imshow(cat);\n",
    "#axs[2].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU5tBzgzjxP1"
   },
   "outputs": [],
   "source": [
    "image = Image.open('samples/adver.png').convert('RGB')\n",
    "dog_cat_image = transform(image)\n",
    "\n",
    "output = model(dog_cat_image.unsqueeze(0).cpu())\n",
    "print_top_classes(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH5iVIVpj4WJ"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3,figsize=(21,7))\n",
    "axs[0].imshow(image);\n",
    "axs[0].axis('off');\n",
    "# golden retriever - the predicted class\n",
    "dog = generate_visualization(dog_cat_image)\n",
    "axs[1].imshow(dog);\n",
    "axs[1].axis('off');\n",
    "\n",
    "# generate visualization for class 285: 'Egyptian cat'\n",
    "#cat = generate_visualization(dog_cat_image, class_index=285)\n",
    "#axs[2].imshow(cat);\n",
    "#axs[2].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3X62zimtRMwQ"
   },
   "outputs": [],
   "source": [
    "image = Image.open('samples/normal.png').convert('RGB')\n",
    "tusker_zebra_image = transform(image)\n",
    "\n",
    "output = model(tusker_zebra_image.unsqueeze(0).cpu())\n",
    "print_top_classes(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mfei_PQJGGrT"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(21,7))\n",
    "axs[0].imshow(image);\n",
    "axs[0].axis('off');\n",
    "\n",
    "# zebra - the predicted class\n",
    "zebra = generate_visualization(tusker_zebra_image)\n",
    "axs[1].imshow(zebra);\n",
    "axs[1].axis('off');\n",
    "\n",
    "# tusker  - generate visualization for class 101 : tusker\n",
    "tusker = generate_visualization(tusker_zebra_image, class_index=530)\n",
    "axs[2].imshow(tusker);\n",
    "axs[2].axis('off');"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTlLji4HNHQUoYwozEOXfI",
   "collapsed_sections": [],
   "name": "1_Compute_Relevancy_Of_Transformer_Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
