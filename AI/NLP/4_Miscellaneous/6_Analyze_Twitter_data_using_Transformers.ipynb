{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Analyze_Twitter_data_using_Transformers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOS8ctHQ39gJw7oNfJy1baQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e7bf9b2b7f5d43719cc2b226799fe46b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd5ad539cc0b4627959be27583b99161","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_591e24fa78f843e48d31ab779c4ccc95","IPY_MODEL_a306629c1c9e45d8b5539e7f0b3e3f4b"]}},"bd5ad539cc0b4627959be27583b99161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"591e24fa78f843e48d31ab779c4ccc95":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a540eb0458fc4b09855a305912587d93","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a892758ae1746e6b753488c964deb00"}},"a306629c1c9e45d8b5539e7f0b3e3f4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_181186135e6a454699ada9b26f3761f3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:01&lt;00:00, 119kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_04b025c6cf0640eb9b472620e2f0fbd4"}},"a540eb0458fc4b09855a305912587d93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5a892758ae1746e6b753488c964deb00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"181186135e6a454699ada9b26f3761f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"04b025c6cf0640eb9b472620e2f0fbd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d709854fb297465380b42c50a0b084d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_92c9047de1d64f74b5edaa9a5d425865","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4fdf469593e043ca8416ae98b3bb5baa","IPY_MODEL_46d55661ef574cd2bbb37ade381663f2"]}},"92c9047de1d64f74b5edaa9a5d425865":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fdf469593e043ca8416ae98b3bb5baa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_34cceeb4e4a54a9f812a1efad269b508","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_530063e621bc4d668d8668cecbd49a3b"}},"46d55661ef574cd2bbb37ade381663f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_28948bc569184b53a5bdc4d4641905c0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 41.5B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_311d23a1002340d8af7bfa94eb7194bd"}},"34cceeb4e4a54a9f812a1efad269b508":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"530063e621bc4d668d8668cecbd49a3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28948bc569184b53a5bdc4d4641905c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"311d23a1002340d8af7bfa94eb7194bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3339ca2ba8624784a1f85cdb2823ade0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2b26d4a07e8e47d0a8d0a4daaaaa0c0f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_152dfdb886b64fb18a4394f11d48a5a1","IPY_MODEL_073d25c6ea9143abb394c0cc1def578e"]}},"2b26d4a07e8e47d0a8d0a4daaaaa0c0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"152dfdb886b64fb18a4394f11d48a5a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f0da22870c9045d293e202670c507886","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4b4f9faac853431faf29f22c5d9b826c"}},"073d25c6ea9143abb394c0cc1def578e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8b2a97ba191f4dc3832383d451d04977","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.89MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d00dd99aca304796b580a68afdc77e76"}},"f0da22870c9045d293e202670c507886":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4b4f9faac853431faf29f22c5d9b826c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b2a97ba191f4dc3832383d451d04977":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d00dd99aca304796b580a68afdc77e76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["# **BERT to Analyze Twitter Data**"],"metadata":{"id":"o4kDQK1IIzsN"}},{"cell_type":"markdown","source":["In this session, we will talk about the working of BERT along with the different methodologies involved and will implement twitter sentiment analysis using the BERT model."],"metadata":{"id":"cY95J4PWJu2y"}},{"cell_type":"markdown","source":["To read about it more, please go through [this](https://analyticsindiamag.com/how-i-used-bidirectional-encoder-representations-from-transformers-bert-to-analyze-twitter-data/) article."],"metadata":{"id":"F-bejPe_Jyg1"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim torch keras tensorflow transformers --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## **Implementation of BERT to Analyze Twitter Data**"],"metadata":{"id":"1IFGB9rNKd2a"}},{"cell_type":"markdown","source":["Let us consider a simple dataset like twitter sentiment analysis data for the implementation of BERT. "],"metadata":{"id":"Qf3lWqeQKhNm"}},{"cell_type":"markdown","source":["Checking for GPU :"],"metadata":{"id":"RJjFA_ihKi6R"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","else:\n","    print('Using CPU.')\n","    device = torch.device(\"cpu\")"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzqx6kovHpGl","executionInfo":{"status":"ok","timestamp":1622191821036,"user_tz":-330,"elapsed":3097,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"4688dab8-f7d1-4f3c-a125-3a950ce4edfb"}},{"cell_type":"markdown","source":["Loading the data and converting it to NumPy array:"],"metadata":{"id":"0UeRmBDWKqhR"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","\n","tweet_train=pd.read_csv('https://raw.githubusercontent.com/MohamedAfham/Twitter-Sentiment-Analysis-Supervised-Learning/master/Data/train_tweets.csv')"],"outputs":[],"metadata":{"id":"0ryCKPOCLSzK","executionInfo":{"status":"ok","timestamp":1622191978858,"user_tz":-330,"elapsed":504,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["tweets = tweet_train.tweet.values\n","labels = tweet_train.label.values"],"outputs":[],"metadata":{"id":"89ZJSR9gLWGg","executionInfo":{"status":"ok","timestamp":1622191992126,"user_tz":-330,"elapsed":500,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Now, we will initialize the BERT tokenizer and convert each word to a unique token. Here we use a method called encode which helps in combining multiple steps. The method splits the sentences to tokens, adds the [cls] and [sep] tokens and also matches the tokens to id."],"metadata":{"id":"st_X7mh_LacP"}},{"cell_type":"code","execution_count":null,"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","tweetid = []\n","for tweet in tweets:\n","  encoded_tweet = tokenizer.encode(tweet,add_special_tokens = True,)\n","  tweetid.append(encoded_tweet)\n","\n","print('Original: ', tweets[0])\n","print('Token IDs:', tweetid[0])\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206,"referenced_widgets":["e7bf9b2b7f5d43719cc2b226799fe46b","bd5ad539cc0b4627959be27583b99161","591e24fa78f843e48d31ab779c4ccc95","a306629c1c9e45d8b5539e7f0b3e3f4b","a540eb0458fc4b09855a305912587d93","5a892758ae1746e6b753488c964deb00","181186135e6a454699ada9b26f3761f3","04b025c6cf0640eb9b472620e2f0fbd4","d709854fb297465380b42c50a0b084d8","92c9047de1d64f74b5edaa9a5d425865","4fdf469593e043ca8416ae98b3bb5baa","46d55661ef574cd2bbb37ade381663f2","34cceeb4e4a54a9f812a1efad269b508","530063e621bc4d668d8668cecbd49a3b","28948bc569184b53a5bdc4d4641905c0","311d23a1002340d8af7bfa94eb7194bd","3339ca2ba8624784a1f85cdb2823ade0","2b26d4a07e8e47d0a8d0a4daaaaa0c0f","152dfdb886b64fb18a4394f11d48a5a1","073d25c6ea9143abb394c0cc1def578e","f0da22870c9045d293e202670c507886","4b4f9faac853431faf29f22c5d9b826c","8b2a97ba191f4dc3832383d451d04977","d00dd99aca304796b580a68afdc77e76"]},"id":"gyBhd0gjLcRO","executionInfo":{"status":"ok","timestamp":1622192036271,"user_tz":-330,"elapsed":19258,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"12b65c76-7751-4be4-f62b-9a9871eb457a"}},{"cell_type":"markdown","source":["Next, we will truncate the sentences so that all the sentences have the same length. "],"metadata":{"id":"N9C7jVXYLhkb"}},{"cell_type":"code","execution_count":null,"source":["from keras.preprocessing.sequence import pad_sequences\n","MAX_LEN = 64\n","print('\\n Truncating all sentences to %d values...' % MAX_LEN)\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","tweetid = pad_sequences(tweetid, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RTXYI9ZLn3s","executionInfo":{"status":"ok","timestamp":1622192084207,"user_tz":-330,"elapsed":4605,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"f9ce259a-0188-4aea-f620-27719434a35f"}},{"cell_type":"markdown","source":["The final step before the training begins is to create masks in the input. "],"metadata":{"id":"vrOvH2DcLuWV"}},{"cell_type":"code","execution_count":null,"source":["masks = []\n","for tweet in tweetid:\n","  mask = [int(token_id > 0) for token_id in tweet]\n","  masks.append(mask)"],"outputs":[],"metadata":{"id":"HMJyTfXVLwhe","executionInfo":{"status":"ok","timestamp":1622192102217,"user_tz":-330,"elapsed":1969,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Let us first split the data into training and validation set. "],"metadata":{"id":"xZCri8NvL2dJ"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(tweetid, labels, random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(masks, labels, random_state=2018, test_size=0.1)"],"outputs":[],"metadata":{"id":"8PQZS0y2L8Oe","executionInfo":{"status":"ok","timestamp":1622192153426,"user_tz":-330,"elapsed":409,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Now, since we are implementing this in PyTorch we will convert the data into tensors. "],"metadata":{"id":"MYt3ggIgL-8O"}},{"cell_type":"code","execution_count":null,"source":["train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"outputs":[],"metadata":{"id":"tAEshBYUMBml","executionInfo":{"status":"ok","timestamp":1622192170957,"user_tz":-330,"elapsed":901,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["We will use the pre-trained BERT sequence classifier model on our data and Adam optimizer. We will set the learning rate to a very small value and initialize a scheduler. "],"metadata":{"id":"h06pDdpkMDuw"}},{"cell_type":"code","execution_count":null,"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels = 2, \n","    output_attentions = False, \n","    output_hidden_states = False, \n",")\n","\n","model.cpu()\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )\n","\n","from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 4\n","\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, \n","                                            num_training_steps = total_steps)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wg4TKtRIMGak","executionInfo":{"status":"ok","timestamp":1622192221356,"user_tz":-330,"elapsed":2440,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"a7bef431-8657-4e3b-d87e-4438bfbdca56"}},{"cell_type":"markdown","source":["Training and evaluation:"],"metadata":{"id":"d0TDz_zdMK8I"}},{"cell_type":"code","execution_count":null,"source":["def accuracy(preds, labels):\n","    pred = np.argmax(preds, axis=1).flatten()\n","    labels = labels.flatten()\n","    return np.sum(pred == labels) / len(labels)\n","\n","import random\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","loss_values = []\n","for epoch_i in range(0, epochs):\n","  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","  total_loss = 0\n","  model.train()\n","  for step, batch in enumerate(train_dataloader):\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    model.zero_grad()        \n","\n","    outputs = model(b_input_ids, \n","                token_type_ids=None, \n","                attention_mask=b_input_mask, \n","                labels=b_labels)\n","\n","    loss = outputs[0]    \n","    total_loss += loss.item()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    loss_values.append(avg_train_loss)\n","\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"validation\")\n","\n","    model.eval()\n","\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","for batch in validation_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","    with torch.no_grad():        \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    tmp_eval_accuracy = accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omWmr4KCMVr2","outputId":"aa34d5d4-68fa-4bb9-e928-9820241243b6"}},{"cell_type":"code","execution_count":null,"source":["#Graph: \n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","plt.plot(loss_values, 'b-o')\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","BERT to Analyze Twitter Data"],"outputs":[],"metadata":{"id":"maLkT663MbrC"}},{"cell_type":"markdown","source":["The results indicate that the accuracy for the BERT model is 97% which means the model performed well even on small datasets. The model has not overfitted as we can see no sharp spike in the graph shown above. "],"metadata":{"id":"1lGomjXRMhD3"}},{"cell_type":"markdown","source":["Testing"],"metadata":{"id":"_ujR2juKMmJf"}},{"cell_type":"markdown","source":["Let us see how our model performed on test data."],"metadata":{"id":"TWdDmcFBMo73"}},{"cell_type":"code","execution_count":null,"source":["tweet_test=pd.read_csv('https://raw.githubusercontent.com/bhoomikamadhukar/NLP/master/test.csv')"],"outputs":[],"metadata":{"id":"Og8EPHO4MsOd"}},{"cell_type":"markdown","source":["We will have to perform the same processing techniques as we did for training here as well."],"metadata":{"id":"IbYwp8X8MyCh"}},{"cell_type":"code","execution_count":null,"source":["tweetvalidation = tweet_test.tweet.values\n","labels=tweet_test.label.values\n","test_id = []\n","\n","for tweet in tweetvalidation:\n","    encoded_tweet = tokenizer.encode(\n","                        tweet,                    \n","                        add_special_tokens = True, \n","                   )\n","    test_id.append(encoded_tweet)\n","test_id = pad_sequences(test_id, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","attention_masks = []\n","for seq in test_id:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","prediction_inputs = torch.tensor(test_id)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","batch_size = 32  \n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","model.eval()\n","\n","predictions , true_labels = [], []\n","\n","for batch in prediction_dataloader:\n","  batch = tuple(t.to(device) for t in batch)\n","  b_input_ids, b_input_mask, b_labels = batch\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","  logits = outputs[0]\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","from sklearn.metrics import classification_report\n","for i in range(len(true_labels)):\n","  pred_labels_flattening = np.argmax(predictions[i], axis=1).flatten()\n","print(classification_report(true_labels[0], pred_labels_flattening) )"],"outputs":[],"metadata":{"id":"b-qMOoicM3VN"}},{"cell_type":"markdown","source":["Thus we can see that within a short period of time we can build a BERT model that works on test data with a fairly good score."],"metadata":{"id":"J_cUOUWbM7Ey"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [BERT to Analyze Twitter Data ](https://analyticsindiamag.com/how-i-used-bidirectional-encoder-representations-from-transformers-bert-to-analyze-twitter-data/)\n","\n","> * [Language Modelling using Unigram](https://analyticsindiamag.com/complete-guide-on-language-modelling-unigram-using-python/)\n","\n","> * [Predict the News Category](https://analyticsindiamag.com/guide-to-cracking-machinehacks-predict-the-news-category-hackathon/)\n","\n","> * [Guide to Sense2vec](https://analyticsindiamag.com/guide-to-sense2vec-contextually-keyed-word-vectors-for-nlp/)\n","\n","> * [Download Twitter Data and Analyze](https://analyticsindiamag.com/hands-on-guide-to-download-analyze-and-visualize-twitter-data/)\n","\n","> * [Sentiment Analysis using LSTM](https://analyticsindiamag.com/how-to-implement-lstm-rnn-network-for-sentiment-analysis/)"],"metadata":{"id":"prpNOPgzWgiU"}}]}