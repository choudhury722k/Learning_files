{"cells":[{"cell_type":"markdown","source":["# ALBERT – A Lite BERT"],"metadata":{"id":"Rwqu51BL4BC1"}},{"cell_type":"markdown","source":["Transformer models, especially BERT transformed the NLP pipeline. They solved the problem of sparse annotations for text data. Instead of training a model from scratch, we can now simply fine-tune existing pre-trained models. But the sheer size of BERT(340M parameters) makes it a bit unapproachable. It is very compute-intensive and time taking to run inference using BERT.ALBERT is a lite version of BERT which shrinks down the BERT in size while maintaining the performance.\n","\n","This model was published in a paper presented at ICLR 2020 by Zhenzhong Lan, Mingda Chen2, Sebastian Goodman, Kevin Gimpel, Piyush Sharma and Radu Soricut (researchers at Google Research and Toyota Technological Institute at Chicago).\n","\n","\n","The main Idea of ALBERT is to reduce the number of parameters(up to 90% reduction) using novel techniques while not taking a big hit to the performance. Now, this compressed version scales a lot better than the original BERT, improving the performance while still keeping the model small."],"metadata":{"id":"q694DREk4I6j"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/complete-guide-to-albert-a-lite-bertwith-python-code/) article."],"metadata":{"id":"bKcfpp9P4Njs"}},{"cell_type":"markdown","source":["# Usage of Albert"],"metadata":{"id":"KnFRbLMj4_yQ"}},{"cell_type":"markdown","source":["Tensorflow hub has made it extremely easy to use pre-trained models. Let’s use pretrained ALBERT base model for the classification of movie reviews."],"metadata":{"id":"8b_I1u-15D1B"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras nltk gensim simpletransformers --user -q --no-warn-script-location\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install sentencepiece --user -q --no-warn-script-location\n","!python -m pip install tensorflow_text --user -q --no-warn-script-location"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7464,"status":"ok","timestamp":1624000354454,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"IgJIOqHQ5WA-","outputId":"4fb62b27-73e6-4a66-9e80-2cce8a1be5ba"}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text \n","import tensorflow_datasets as tfds"],"outputs":[],"metadata":{"id":"Kid9UMDRvOFN"}},{"cell_type":"code","execution_count":null,"source":["albert_url='https://tfhub.dev/tensorflow/albert_en_base/2'\n","encoder = hub.KerasLayer(albert_url)\n","\n","preprocessor_url=\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\"\n","preprocessor = hub.KerasLayer(preprocessor_url)"],"outputs":[],"metadata":{"id":"n2SNUPRc3LH4"}},{"cell_type":"markdown","source":["Model and the required preprocessor are downloaded and loaded."],"metadata":{"id":"yuuNhKxv5GhA"}},{"cell_type":"code","execution_count":null,"source":["text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n","encoder_inputs = preprocessor(text_input)"],"outputs":[],"metadata":{"id":"KH9XWDD1WtCT"}},{"cell_type":"code","execution_count":null,"source":["encoder_inputs"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1624000373105,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"i4RAZvYw4rY9","outputId":"3d6b9e22-101c-46dc-fd32-9b66fee335c3"}},{"cell_type":"code","execution_count":null,"source":["outputs = encoder(encoder_inputs)\n","pooled_output = outputs[\"pooled_output\"]     \n","sequence_output = outputs[\"sequence_output\"]\n","pooled_output"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1624000373903,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"DSrPSxto83zm","outputId":"b5a7729e-a0b9-41b7-d38a-b628b82d4aeb"}},{"cell_type":"code","execution_count":null,"source":["train_data, validation_data, test_data = tfds.load(\n","    name=\"imdb_reviews\", \n","    split=('train[:60%]', 'train[60%:]', 'test'),\n","    as_supervised=True)"],"outputs":[],"metadata":{"id":"BC4usrVn9XP3"}},{"cell_type":"code","execution_count":null,"source":["embedding_model = tf.keras.Model(text_input, pooled_output)"],"outputs":[],"metadata":{"id":"z2nokVHdB0JP"}},{"cell_type":"markdown","source":["Just like that, we have our embedding layer. We just need to build a Fully connected neural network to predict whether a movie review is positive or negative."],"metadata":{"id":"Fq3CIwQc5L3s"}},{"cell_type":"code","execution_count":null,"source":["model = tf.keras.Sequential()\n","model.add(embedding_model)\n","\n","model.add(tf.keras.layers.Dense(30, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","model.add(tf.keras.layers.Dense(1))\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"5yl1vpFE__xO"}},{"cell_type":"code","execution_count":null,"source":["history = model.fit(train_data.shuffle(10000).batch(128),\n","                    epochs=100,\n","                    validation_data=validation_data.batch(128),\n","                    verbose=1)"],"outputs":[],"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UDMqwlN7CgwA"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","import matplotlib\n","font = {'family' : 'normal',\n","        'size'   : 22}\n","plt.tight_layout()\n","matplotlib.rc('font', **font)\n","plt.figure(figsize=(15,8))\n","plt.plot(history.history['accuracy'],label='aacuracy')\n","plt.plot(history.history['val_accuracy'],label='validation_accuracy')\n","plt.legend(bbox_to_anchor=(0.7, 0.7))\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.savefig('foo1.png')\n","plt.show()"],"outputs":[],"metadata":{"id":"hRLcBTJJDTww"}},{"cell_type":"markdown","source":["With this basic model validation accuracy, about 75% is a good number. Especially when we are not fine-tuning the embeddings at all. We can fine-tune the embeddings by just making the encoder trainable."],"metadata":{"id":"fjhqU9sJ5TUf"}},{"cell_type":"code","execution_count":null,"source":["albert_url='https://tfhub.dev/tensorflow/albert_en_base/2'\n","encoder = hub.KerasLayer(albert_url,trainable=True)\n","\n","preprocessor_url=\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\"\n","preprocessor = hub.KerasLayer(preprocessor_url)\n","\n","text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n","encoder_inputs = preprocessor(text_input)\n","outputs = encoder(encoder_inputs)\n","pooled_output = outputs[\"pooled_output\"]     \n","sequence_output = outputs[\"sequence_output\"]\n","embedding_model = tf.keras.Model(text_input, pooled_output)\n","model2 = tf.keras.Sequential()\n","model2.add(embedding_model)\n","model2.add(tf.keras.layers.Dense(128, activation='relu'))\n","model2.add(tf.keras.layers.BatchNormalization())\n","model2.add(tf.keras.layers.Dense(30, activation='relu'))\n","model2.add(tf.keras.layers.BatchNormalization())\n","model2.add(tf.keras.layers.Dense(8, activation='relu'))\n","model2.add(tf.keras.layers.BatchNormalization())\n","model2.add(tf.keras.layers.Dense(1))\n","model2.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n"],"outputs":[],"metadata":{"id":"6Cd3a_9i3cYN"}},{"cell_type":"code","execution_count":null,"source":["history2 = model2.fit(train_data.shuffle(10000).batch(128),\n","                    epochs=10,\n","                    validation_data=validation_data.batch(128),\n","                    verbose=1)"],"outputs":[],"metadata":{"id":"Au6Bb3EVXjlX"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(15,8))\n","plt.plot(history.history['accuracy'],label='accuracy')\n","plt.plot(history.history['val_accuracy'],label='validation_accuracy')\n","plt.plot(history2.history['accuracy'],label='accuracy_finetuned')\n","plt.plot(history2.history['val_accuracy'],label='validation_accuracy_finetuned')\n","plt.legend(bbox_to_anchor=(0.7, 0.7))\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.savefig('foo2.png')\n","plt.show()"],"outputs":[],"metadata":{"id":"RbdvPqC1XtQs"}}],"metadata":{"colab":{"authorship_tag":"ABX9TyPOPDdKs5Kh6RRncUx1kOwz","name":"1_ALBERT.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}