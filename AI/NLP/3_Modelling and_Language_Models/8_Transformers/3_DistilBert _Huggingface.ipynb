{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"DistilBert _Huggingface.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP22k2yKCgBx+G8Fy0yMlss"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **HuggingFace DistilBERT**"],"metadata":{"id":"QSs-aJ-trfnM"}},{"cell_type":"markdown","source":[" Due to the large size of BERT, it is difficult for it to put it into production. Suppose we want to use these models on mobile phones, so we require a less weight yet efficient model, that’s when Distil-BERT comes into the picture. Distil-BERT has 97% of BERT’s performance while being trained on half of the parameters of BERT. BERT-base has 110 parameters and BERT-large has 340 parameters, which are hard to deal with. For this problem’s solution, distillation technique is used to reduce the size of these large models. "],"metadata":{"id":"GuviqbLkrpvx"}},{"cell_type":"markdown","source":["## **Installation**"],"metadata":{"id":"5zmRSyn9ruR_"}},{"cell_type":"markdown","source":["Install HuggingFace Transformers framework via PyPI."],"metadata":{"id":"7jMlLhTBrxWb"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras nltk gensim transformers --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## **Demo of HuggingFace DistilBERT**\n","\n","You can import the DistilBERT model from transformers as shown below : "],"metadata":{"id":"3ll1Ed0Vry_u"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertModel"],"outputs":[],"metadata":{"id":"B0OMn32AsPY-"}},{"cell_type":"markdown","source":["### **A. Checking the configuration**"],"metadata":{"id":"Z4lf3xldsRZ1"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertModel, DistilBertConfig\n","# Initializing a DistilBERT configuration\n","configuration = DistilBertConfig()\n","# Initializing a model from the configuration\n","model = DistilBertModel(configuration)\n","# Accessing the model configuration\n","configuration = model.config "],"outputs":[],"metadata":{"id":"maX3UO8vgDyW"}},{"cell_type":"markdown","source":["### **B. DistilBERT Tokenizer**"],"metadata":{"id":"8CeYoG5SsgU9"}},{"cell_type":"code","execution_count":null,"source":["#Similar to BERT Tokenizer, gives end-to-end tokenization for punctuation and word piece\n","from transformers import DistilBertTokenizer\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","inputs "],"outputs":[],"metadata":{"id":"5OZgX8mhsjiX"}},{"cell_type":"markdown","source":["The output of the tokenizer will be :"],"metadata":{"id":"-WWF6NQOsnDX"}},{"cell_type":"markdown","source":["`{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}`"],"metadata":{"id":"YyV2cwyUsptX"}},{"cell_type":"markdown","source":["where, input_ids are numerical representation for the sequence that the DistilBERT model will use.\n","\n","attention_mask represents which tokens to attend to or not.\n","\n","You can also check [DistilBERTTokenizerFast](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizerfast)."],"metadata":{"id":"sMdgjpcassYl"}},{"cell_type":"markdown","source":["### **C. DistilBERT Model**"],"metadata":{"id":"OLXAqQgMsxoq"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertTokenizer, DistilBertModel\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","outputs = model(**inputs)\n","last_hidden_states = outputs.last_hidden_state "],"outputs":[],"metadata":{"id":"JZXvmpbJwdlq"}},{"cell_type":"markdown","source":["The sequence of hidden-states at the output of the last layer of the model.\n","\n","You can also check [DistilBERTMaskedLM](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertformaskedlm)"],"metadata":{"id":"EKrCC6JDwkyS"}},{"cell_type":"markdown","source":["### **D. DistilBERT Masked Language Modeling**"],"metadata":{"id":"PK2bkzvFwqGI"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n","inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n","labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n","outputs = model(**inputs, labels=labels)\n","loss = outputs.loss\n","logits = outputs.logits "],"outputs":[],"metadata":{"id":"We3ZmE1lwtsf"}},{"cell_type":"markdown","source":["\n","where, loss is the masked language modeling loss.\n","\n","logits is Prediction scores of the language modeling head."],"metadata":{"id":"bt6WGQZ2wxMB"}},{"cell_type":"markdown","source":["### **E. DistilBERT for Sequence Classification**"],"metadata":{"id":"i3IGMeZ1w0Gj"}},{"cell_type":"markdown","source":["This model contains a pooler layer on the top of  pooled output that can be used for regression or classification problems."],"metadata":{"id":"AnySb6crw2bv"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n","outputs = model(**inputs, labels=labels)\n","loss = outputs.loss\n","logits = outputs.logits "],"outputs":[],"metadata":{"id":"wQ6-_X1rwwhB"}},{"cell_type":"markdown","source":["where, loss is the classification loss.\n","\n","logits are the classification score before Softmax."],"metadata":{"id":"IA2AK3L_w6dx"}},{"cell_type":"markdown","source":["### **F. DistilBERT for Multiple Choice**"],"metadata":{"id":"mbPGf1xAw8Ny"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertTokenizer, DistilBertForMultipleChoice\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n","model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-cased')\n","prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n","choice0 = \"It is eaten with a fork and a knife.\"\n","choice1 = \"It is eaten while held in the hand.\"\n","labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n","encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors='pt', padding=True)\n","outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels) # batch size is 1\n","# the linear classifier still needs to be trained\n","loss = outputs.loss\n","logits = outputs.logits "],"outputs":[],"metadata":{"id":"ewpdr0REw-f9"}},{"cell_type":"markdown","source":["### **G. DistilBERT for Token Classification**"],"metadata":{"id":"fmPGwB5AxAme"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased')\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n","outputs = model(**inputs, labels=labels)\n","loss = outputs.loss\n","logits = outputs.logits "],"outputs":[],"metadata":{"id":"pYtpDUUbxEQa"}},{"cell_type":"markdown","source":["### **H. DistilBERT For Question Answering**"],"metadata":{"id":"iOdFL3V3xGLg"}},{"cell_type":"code","execution_count":null,"source":["from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n","import torch\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n","question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n","inputs = tokenizer(question, text, return_tensors='pt')\n","start_positions = torch.tensor([1])\n","end_positions = torch.tensor([3])\n","outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n","loss = outputs.loss\n","start_scores = outputs.start_logits\n","end_scores = outputs.end_logits "],"outputs":[],"metadata":{"id":"77PDs84yxJD7"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [Guide to DistilBERT](https://analyticsindiamag.com/python-guide-to-huggingface-distilbert-smaller-faster-cheaper-distilled-bert/)\n","\n","> * [Introduction to Simple Transformers](https://analyticsindiamag.com/text-classification-using-simple-transformers/)\n","\n","> * [Google Sentence Embedder with Tensorflow](https://analyticsindiamag.com/guide-to-universal-sentence-encoder-with-tensorflow/)\n","\n","> * [Sequence-to-Sequence Modeling using LSTM for Language Translation](https://analyticsindiamag.com/sequence-to-sequence-modeling-using-lstm-for-language-translation/)\n","\n","> * [Text Generation using RNN](https://analyticsindiamag.com/recurrent-neural-network-in-pytorch-for-text-generation/)\n","\n","> * [SVD in Recommender System](https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/)\n","\n","\n"],"metadata":{"id":"prpNOPgzWgiU"}}]}