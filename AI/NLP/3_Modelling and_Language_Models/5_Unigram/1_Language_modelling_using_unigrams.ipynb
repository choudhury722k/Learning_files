{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Language_modelling_using_unigrams.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPsyFt28flpv+fDrLF6NJK2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Language Modelling**"],"metadata":{"id":"7bRkYSNC7-IO"}},{"cell_type":"markdown","source":["Language modelling is the speciality of deciding the likelihood of a succession of words. These are useful in many different Natural Language Processing applications like Machine translator, Speech recognition, Optical character recognition and many more.In recent times language models depend on neural networks, they anticipate precisely a word in a sentence dependent on encompassing words. However, in this project, we will discuss the most classic of language models: the n-gram models."],"metadata":{"id":"Mr19WCad82Xw"}},{"cell_type":"markdown","source":["## **Assumptions For a Unigram Model**"],"metadata":{"id":"IS3Ml-0L9EKG"}},{"cell_type":"markdown","source":["1.  It depends on the occurrence of the word among all the words in the dataset.\n","\n","2.  Probability of a word is independent of all the words before its occurrence."],"metadata":{"id":"pvSsoRAk9HjA"}},{"cell_type":"markdown","source":["## **Code Implementation**"],"metadata":{"id":"bDBbdcHS9T6t"}},{"cell_type":"markdown","source":["Import all the libraries required for this project."],"metadata":{"id":"i7mfx1Wg9WRe"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import nltk\n","nltk.download('reuters')\n","from nltk.corpus import reuters\n","nltk.download('punkt')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqk3nVIWAsA4","executionInfo":{"status":"ok","timestamp":1622189922591,"user_tz":-330,"elapsed":392,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"4db40a12-0f6e-4001-d148-bafa061b6ff7"}},{"cell_type":"markdown","source":["Reuters dataset consists of 10788 documents from the Reuters financial newswire services.\n","\n","Store the words in a list."],"metadata":{"id":"GLiwe8ln9bFS"}},{"cell_type":"code","execution_count":null,"source":["words = list(reuters.words())\n","words"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lqKLWQkw9va_","executionInfo":{"status":"error","timestamp":1622189931952,"user_tz":-330,"elapsed":732,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"a849fa78-c334-40e9-c152-96c5ee2bf0d8"}},{"cell_type":"code","execution_count":null,"source":["len(words)"],"outputs":[],"metadata":{"id":"qKLPkxkADjpw"}},{"cell_type":"markdown","source":["We will start by creating a class and defining every function in it. The idea is to generate words after the sentence using the n-gram model. Predicting the next word with Bigram or Trigram will lead to sparsity problems. To solve this issue we need to go for the unigram model as it is not dependent on the previous words.\n","\n","Letâ€™s calculate the unigram probability of a sentence using the Reuters corpus."],"metadata":{"id":"EQ7uiX4kDlom"}},{"cell_type":"code","execution_count":null,"source":["class NGrams:\n","    def __init__(self, words, sentence):\n","        self.words = words\n","        self.sentence = sentence\n","        self.tokens = sentence.split()\n","    def get_tokens(self):\n","        return self.tokens\n","    def add_tokens(self,value):\n","        temp = self.tokens\n","        temp.append(value)\n","        self.tokens = temp\n","        return self.tokens\n","    def unigram_model(self):\n","        self.next_words = np.random.choice(words, size=3)\n","        return self.next_words"],"outputs":[],"metadata":{"id":"DgIghqzIDndf","executionInfo":{"status":"ok","timestamp":1622189966223,"user_tz":-330,"elapsed":389,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Here we need to calculate the probabilities for all the various words present in the results of the over unigram model. Select the top three words based on probabilities."],"metadata":{"id":"rr4gPUmEDqTP"}},{"cell_type":"code","execution_count":null,"source":["def get_top_3_next_words(self,next_words):\n","    next_words_dict = dict()\n","    for word in next_words:\n","        if not word in next_words_dict.keys():\n","            next_words_dict[word] = 1\n","        else:\n","            next_words_dict[word] += 1\n","        for i,j in next_words_dict.items():\n","          next_words_dict[i] = np.round(j/len(next_words),2)\n","    return sorted(next_words_dict.items(), key = lambda k:(k[1], k[0]), reverse=True)[:3]\n","\n","def model_selection(self):\n","        top_words = self.unigram_model()\n","        print(\"unigram-model\")\n","        return top_words"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"fUolr8XLDss7","executionInfo":{"status":"error","timestamp":1622190019009,"user_tz":-330,"elapsed":619,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"1b80f5f7-8af2-4552-da08-167465bfec41"}},{"cell_type":"code","execution_count":null,"source":["model = NGrams(words=words, sentence=start_sent)"],"outputs":[],"metadata":{"id":"Ft31VHnhEvSX"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","for i in range(5):\n","    values = model.model_selection()\n","    print(values)\n","    value = input()\n","    model.add_tokens(value)"],"outputs":[],"metadata":{"id":"JDhCSGXJExNl"}},{"cell_type":"markdown","source":["The model generates the top three words. We can select a word from it that will succeed in the starting sentence. Repeat the process up to 5 times. The result is displayed below."],"metadata":{"id":"peryky8nE5-9"}},{"cell_type":"code","execution_count":null,"source":["print(model.get_tokens())"],"outputs":[],"metadata":{"id":"rYLi6pClE8p5"}},{"cell_type":"markdown","source":["Final step is to join the sentence that is produced from the unigram model."],"metadata":{"id":"dm8HUbSUE_K3"}},{"cell_type":"code","execution_count":null,"source":["print(\" \".join(model.get_tokens()))"],"outputs":[],"metadata":{"id":"eHYRgSsCFBPQ"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [Language Modelling using Unigram](https://analyticsindiamag.com/complete-guide-on-language-modelling-unigram-using-python/)\n","\n","> * [Predict the News Category](https://analyticsindiamag.com/guide-to-cracking-machinehacks-predict-the-news-category-hackathon/)\n","\n","> * [Guide to Sense2vec](https://analyticsindiamag.com/guide-to-sense2vec-contextually-keyed-word-vectors-for-nlp/)\n","\n","> * [Download Twitter Data and Analyze](https://analyticsindiamag.com/hands-on-guide-to-download-analyze-and-visualize-twitter-data/)\n","\n","> * [Sentiment Analysis using LSTM](https://analyticsindiamag.com/how-to-implement-lstm-rnn-network-for-sentiment-analysis/)\n","\n","> * [VADER Sentiment Analysis](https://analyticsindiamag.com/sentiment-analysis-made-easy-using-vader/)\n"],"metadata":{"id":"prpNOPgzWgiU"}}]}