{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Classification_Using_SVC_Naive Bayes_Random_Forest .ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOkwsQUnKWMaCFjiWx8SWeg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Reviews Classification Using SVC, Naive Bayes & Random Forest**"],"metadata":{"id":"cR53iQAhZ1fO"}},{"cell_type":"markdown","source":["In this article, we will analyze the reviews of customers about a restaurant and will predict based on these reviews whether the customer has liked the product of the restaurant or not. That means whether it is a positive review or negative review, based on the available text review. The prediction will be carried out by classification models and will find out which classification model is best in this task of prediction. "],"metadata":{"id":"n7nN25IGZ2mK"}},{"cell_type":"markdown","source":["## **The Dataset**"],"metadata":{"id":"7bpT9FfMaCzB"}},{"cell_type":"markdown","source":["In this experiment, a restaurant’s reviews dataset is used that is publically available on [Kaggle](https://www.kaggle.com/vigneshwarsofficial/reviews/data). The motivation behind using this dataset is that most of the restaurants request its customers for review. Based on these reviews, the restaurant will be able to make improvements in order to further customer satisfaction. This dataset comes in a TSV (Tab Separated Values) file and comprises 1,000 reviews of customers along with a binary feature indicating whether a customer has liked the product of the restaurant or not. "],"metadata":{"id":"WUj2bUkeaK8X"}},{"cell_type":"markdown","source":["## **Analyzing Reviews using Natural Language Processing**"],"metadata":{"id":"T0lAX33kanQ7"}},{"cell_type":"markdown","source":["First, we need to import the required libraries. The below code snippet will import those libraries."],"metadata":{"id":"ldpQtqFcbMUq"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#Importing required Libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC"],"outputs":[],"metadata":{"id":"W-JusMuRYoUI"}},{"cell_type":"markdown","source":["The dataset that is downloaded from the Kaggle will be read here. As we are reading a TSV file, we need to give the separating parameter that is delimiter here. For TSV files, the tab space is the delimiter so ‘\\t’ is used here. To ignore the quotes present in the text, we are including the quoting parameter here"],"metadata":{"id":"ZsNzBMcIbQiI"}},{"cell_type":"code","execution_count":null,"source":["# Reading the dataset\n","dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"],"outputs":[],"metadata":{"id":"3mcOMCltbTOE"}},{"cell_type":"markdown","source":["Now, once we read the dataset, we need to preprocess the data. It is expected that you are aware of the basics of natural language processing. Here we need text data only into consideration so a regular expression is used for taking alphabetical values only. Then we convert all the text into lower case. After that, it is split into words and converted to its original form by stemming. Finally, all the stopwords are removed from the text and every word is added to the corpus."],"metadata":{"id":"BDq1z4c2cTk8"}},{"cell_type":"code","execution_count":null,"source":["# Preprocessing\n","nltk.download('stopwords')\n","corpus = []\n","for i in range(0, 1000):\n","    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n","    review = review.lower()\n","    review = review.split()\n","    ps = PorterStemmer()\n","    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n","    review = ' '.join(review)\n","    corpus.append(review)"],"outputs":[],"metadata":{"id":"tn6-f5r5cVfK"}},{"cell_type":"markdown","source":["Once the corpus of words is ready, we create a bag of words model, that is actually a sparse matrix where we are using a maximum of 1500 features. From this sparse matrix, we have created the input and output features. The output feature in this data set is the binary response to whether the customer has liked the product of the restaurant or not. After that, we have defined the training and test sets. "],"metadata":{"id":"6tsZ9YszcXhs"}},{"cell_type":"code","execution_count":null,"source":["# Creating the Bag of Words model\n","cv = CountVectorizer(max_features = 1500)\n","X = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:, 1].values\n","\n","# Splitting the dataset into the Training set and Test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"],"outputs":[],"metadata":{"id":"QwwTEOeCcbvE"}},{"cell_type":"markdown","source":["## **Classification of Reviews**"],"metadata":{"id":"ytD4g5JTcdib"}},{"cell_type":"markdown","source":["Once we are ready with the training and test data, we will define three classification models through which we will predict whether the custom has liked the product or not, based on the reviews. We are going to define three classification models here – the Naive Bayes Model, the Random Forest Model, and the Support Vector Model."],"metadata":{"id":"DrIHoBTvcjRs"}},{"cell_type":"code","execution_count":null,"source":["#Clasiification\n","# Fitting Naive Bayes\n","NB_classifier = GaussianNB()\n","NB_classifier.fit(X_train, y_train)\n","y_pred_NB = NB_classifier.predict(X_test)\n","\n","# Random Forest\n","rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n","rf_classifier.fit(X_train, y_train)\n","y_pred_rf = rf_classifier.predict(X_test)\n","\n","#Support Vector\n","SVC_classifier = SVC(kernel = 'rbf')\n","SVC_classifier.fit(X_train, y_train)\n","y_pred_SVC = SVC_classifier.predict(X_test)"],"outputs":[],"metadata":{"id":"LyHQrMSwcpz8"}},{"cell_type":"markdown","source":["Now once we have defined and trained the three models and made predictions using all of these, we need to see their classification accuracies. This we are going to check through the confusion matrix and [CAP (Cumulative Accuracy Profile) Curve Analysis](https://analyticsindiamag.com/cumulative-accuracy-profile-cap-curve-analysis-classification-prediction/)."],"metadata":{"id":"tjaEu_K2csHK"}},{"cell_type":"code","execution_count":null,"source":["#Confusion Matrices\n","cm_NB = confusion_matrix(y_test, y_pred_NB)\n","cm_RandFor = confusion_matrix(y_test, y_pred_rf)\n","cm_SVC = confusion_matrix(y_test, y_pred_SVC)"],"outputs":[],"metadata":{"id":"WQZvY1UTc0La"}},{"cell_type":"code","execution_count":null,"source":["#CAP Analysis\n","total = len(y_test) \n","one_count = np.sum(y_test) \n","zero_count = total - one_count \n","lm_NB = [y for _, y in sorted(zip(y_pred_NB, y_test), reverse = True)] \n","lm_SVC = [y for _, y in sorted(zip(y_pred_SVC, y_test), reverse = True)] \n","lm_RandFor = [y for _, y in sorted(zip(y_pred_rf, y_test), reverse = True)] \n","x = np.arange(0, total + 1) \n","y_NB = np.append([0], np.cumsum(lm_NB)) \n","y_SVC = np.append([0], np.cumsum(lm_SVC)) \n","y_RandFor = np.append([0], np.cumsum(lm_RandFor)) \n","\n","plt.figure(figsize = (10, 10))\n","plt.title('CAP Curve Analysis')\n","plt.plot([0, total], [0, one_count], c = 'k', linestyle = '--', label = 'Random Model')\n","plt.plot([0, one_count, total], [0, one_count, one_count], c = 'grey', linewidth = 2, label = 'Perfect Model') \n","plt.plot(x, y_SVC, c = 'y', label = 'SVC', linewidth = 2)\n","plt.plot(x, y_NB, c = 'b', label = 'Naive Bayes', linewidth = 2)\n","plt.plot(x, y_RandFor, c = 'r', label = 'Rand Forest', linewidth = 2)\n","plt.legend()"],"outputs":[],"metadata":{"id":"5PZMfDAwc7bU"}},{"cell_type":"markdown","source":["By analyzing the confusion matrices and CAP curves of each of the three classifiers, it is clear that the support vector classifier is best among all three. In the task of predicting whether the customer likes the product or not, based on reviews, the support vector classifier has outperformed the other two classifiers. "],"metadata":{"id":"eLNGD-s0c-by"}},{"cell_type":"markdown","source":["# **Read more articles on:**\n","\n","> * [Review Classification](https://analyticsindiamag.com/step-by-step-guide-to-reviews-classification-using-svc-naive-bayes-random-forest/)\n","\n","> * [Multi Class Text Classification](https://analyticsindiamag.com/multi-class-text-classification-in-pytorch-using-torchtext/)\n","\n","> * [Text Classification](https://analyticsindiamag.com/how-to-solve-your-first-ever-nlp-classification-challenge/)\n","\n","> * [Word Frequency](https://analyticsindiamag.com/using-natural-language-processing-to-check-word-frequency-in-the-adventure-of-sherlock-holmes/)\n","\n","> * [Vocabulary Builder](https://analyticsindiamag.com/how-to-create-a-vocabulary-builder-for-nlp-tasks/)\n","\n","> * [Spacy Basics](https://analyticsindiamag.com/nlp-deep-learning-nlp-framework-nlp-model/)\n"],"metadata":{"id":"prpNOPgzWgiU"}}]}