{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NewsPlease.ipynb","provenance":[],"collapsed_sections":["EqSXhVKvfXlA"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"2MMYj8iMQBU7","colab_type":"code","outputId":"c785d43b-3b33-4185-aa1a-0a1ea4998e67","executionInfo":{"status":"ok","timestamp":1580065258980,"user_tz":-120,"elapsed":34339,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DZoPOVT5xRBR","colab_type":"code","outputId":"b6b0cf0c-2c83-4909-cf3a-798f4f509d4e","executionInfo":{"status":"ok","timestamp":1580239330737,"user_tz":-120,"elapsed":48636,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip3 install news-please   #https://github.com/fhamborg/news-please"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting news-please\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/80/bbcda1ddac744461b51475d12f204f5cd6f16eff97b0f185d99c31fc7629/news-please-1.4.25.tar.gz (62kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n","\u001b[?25hCollecting Scrapy>=1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e4/69b87d7827abf03dea2ea984230d50f347b00a7a3897bc93f6ec3dafa494/Scrapy-1.8.0-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 13.4MB/s \n","\u001b[?25hCollecting PyMySQL>=0.7.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/39/15045ae46f2a123019aa968dfcba0396c161c20f855f11dea6796bcaae95/PyMySQL-0.9.3-py2.py3-none-any.whl (47kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n","\u001b[?25hCollecting hjson>=1.5.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/92/6b6b85064f8a88cb3b31901d839e7b45c33e4ee450bb1b3cf0c226cca8ec/hjson-3.0.1.tar.gz (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n","\u001b[?25hCollecting elasticsearch>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/60/0c79dde3e81beffeed422599d9ac65419289095186d37a3201739d52a57d/elasticsearch-7.5.1-py2.py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n","\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.6.3)\n","Collecting readability-lxml>=0.6.2\n","  Downloading https://files.pythonhosted.org/packages/af/a7/8ea52b2d3de4a95c3ed8255077618435546386e35af8969744c0fa82d0d6/readability-lxml-0.7.1.tar.gz\n","Collecting newspaper3k>=0.2.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n","\u001b[K     |████████████████████████████████| 215kB 52.0MB/s \n","\u001b[?25hCollecting langdetect>=1.0.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n","\u001b[K     |████████████████████████████████| 1.0MB 52.9MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (2.6.1)\n","Requirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.9.6)\n","Collecting dotmap>=1.2.17\n","  Downloading https://files.pythonhosted.org/packages/35/a6/0f88e89673285daf190891985ad8b57d1d70ec4ccaf2d53b692e25f52ad4/dotmap-1.3.8-py3-none-any.whl\n","Collecting PyDispatcher>=2.0.5\n","  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n","Collecting warcio>=1.3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/c4/86bc02bc3bc33c34ab24e52af8a1c34eb6e03e7cd5b3904057ebcea311da/warcio-1.7.1-py2.py3-none-any.whl (41kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n","\u001b[?25hCollecting ago>=0.0.9\n","  Downloading https://files.pythonhosted.org/packages/f8/4e/976bd88566b0feec295873c3aa5a8712219edb2c07c7f6723c831e8840bd/ago-0.0.93.tar.gz\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from news-please) (1.12.0)\n","Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.6/dist-packages (from news-please) (4.2.6)\n","Collecting awscli>=1.11.117\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/38/857f48f129c6216537877eb68727526db8f1743afc7596b8ec18a208cb6d/awscli-1.17.9-py2.py3-none-any.whl (2.8MB)\n","\u001b[K     |████████████████████████████████| 2.8MB 40.0MB/s \n","\u001b[?25hCollecting hurry.filesize>=0.9\n","  Downloading https://files.pythonhosted.org/packages/ee/5e/16e17bedcf54d5b618dc0771690deda77178e5c310402881c3d2d6c5f27c/hurry.filesize-0.9.tar.gz\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from news-please) (0.0.1)\n","Collecting pyOpenSSL>=16.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n","\u001b[?25hCollecting service-identity>=16.0.0\n","  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n","Collecting queuelib>=1.4.2\n","  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n","Collecting cssselect>=0.9.1\n","  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n","Collecting w3lib>=1.17.0\n","  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n","Collecting parsel>=1.5.0\n","  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n","Collecting cryptography>=2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 46.1MB/s \n","\u001b[?25hCollecting zope.interface>=4.1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/16/79fe71428c91673194a21fedcc46f7f1349db799bc2a65da4ffdbe570343/zope.interface-4.7.1-cp36-cp36m-manylinux2010_x86_64.whl (168kB)\n","\u001b[K     |████████████████████████████████| 174kB 43.5MB/s \n","\u001b[?25hCollecting protego>=0.1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 39.5MB/s \n","\u001b[?25hCollecting Twisted>=17.9.0; python_version >= \"3.5\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/e2/0c21fadf0dff02d145db02f24a6ed2c24993e7242d138babbca41de2f5a2/Twisted-19.10.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 37.3MB/s \n","\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch>=2.4->news-please) (1.24.3)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml>=0.6.2->news-please) (3.0.4)\n","Collecting tldextract>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 518kB/s \n","\u001b[?25hCollecting tinysegmenter==0.3\n","  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (2.21.0)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (6.2.2)\n","Collecting feedparser>=5.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n","\u001b[K     |████████████████████████████████| 194kB 46.9MB/s \n","\u001b[?25hCollecting feedfinder2>=0.0.4\n","  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (3.2.5)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k>=0.2.8->news-please) (3.13)\n","Collecting jieba3k>=0.35.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n","\u001b[K     |████████████████████████████████| 7.4MB 33.8MB/s \n","\u001b[?25hCollecting colorama<0.4.2,>=0.2.5\n","  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.117->news-please) (0.15.2)\n","Collecting botocore==1.14.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/4c/b0b0d3b6f84a05f9135051b56d3eb8708012a289c4b82ee21c8c766f47b5/botocore-1.14.9-py2.py3-none-any.whl (5.9MB)\n","\u001b[K     |████████████████████████████████| 5.9MB 37.4MB/s \n","\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/48/a8252b6b3cd31774eab312b19d58a6ac55f296240c206617dcd38cd93bf8/s3transfer-0.3.2-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n","\u001b[?25hCollecting rsa<=3.5.0,>=3.1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from hurry.filesize>=0.9->news-please) (42.0.2)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.4.8)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (0.2.7)\n","Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->Scrapy>=1.1.0->news-please) (19.3.0)\n","Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->Scrapy>=1.1.0->news-please) (1.13.2)\n","Collecting incremental>=16.10.1\n","  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n","Collecting constantly>=15.1\n","  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n","Collecting PyHamcrest>=1.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/6c/a641af18e416e6501c10b03742387176626a1d48196100160df796f36632/PyHamcrest-2.0.0-py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n","\u001b[?25hCollecting hyperlink>=17.1.1\n","  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n","Collecting Automat>=0.3.0\n","  Downloading https://files.pythonhosted.org/packages/e5/11/756922e977bb296a79ccf38e8d45cafee446733157d59bcd751d3aee57f5/Automat-0.8.0-py2.py3-none-any.whl\n","Collecting requests-file>=1.4\n","  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n","Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->news-please) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->news-please) (2019.11.28)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.14.9->awscli>=1.11.117->news-please) (0.9.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->Scrapy>=1.1.0->news-please) (2.19)\n","Building wheels for collected packages: news-please, hjson, readability-lxml, langdetect, PyDispatcher, ago, hurry.filesize, protego, tinysegmenter, feedparser, feedfinder2, jieba3k\n","  Building wheel for news-please (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for news-please: filename=news_please-1.4.25-cp36-none-any.whl size=84289 sha256=b293eefc0118c6c7982f3576d0622f4e6d9d06fe924071a4231687b8a61ceda3\n","  Stored in directory: /root/.cache/pip/wheels/26/93/5a/35a7c223d3d69ee2f5be76bfbe5247e8a0a5f87ef9325f1a87\n","  Building wheel for hjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hjson: filename=hjson-3.0.1-cp36-none-any.whl size=54144 sha256=5ee99de9304e21f4e516a09bd297dc2a47abddbc104025bbcad92654efed96f6\n","  Stored in directory: /root/.cache/pip/wheels/34/2a/5b/254bcb7475d861a2bdc1f8f32f9924734b4e045a7c8dd596ae\n","  Building wheel for readability-lxml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for readability-lxml: filename=readability_lxml-0.7.1-cp36-none-any.whl size=16480 sha256=2da90389801a4407a8b5997de05390dc7ce3b486be52e05cd54576966c14ad9f\n","  Stored in directory: /root/.cache/pip/wheels/94/48/e5/d944e616d8b0734c3b9cf30a21f4afcf855a1e2b85f82f34fb\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=0dda07d81b4f795b485b2d9d8373f5c3bd4cf46b1a6366d81c6067f224758001\n","  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=857c3f78d209714dff8dd3326dbdb606ba898b3119e7a88aa1b4034b617aa2b2\n","  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n","  Building wheel for ago (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ago: filename=ago-0.0.93-cp36-none-any.whl size=3274 sha256=0db9e1d90e6200e6039aa4766b20e5c731b0a5727ae99b0d969c3f5e48ec0f1c\n","  Stored in directory: /root/.cache/pip/wheels/86/65/dc/27df15b96de756d2e07bc01433067df2e6ace2bae8de0576a2\n","  Building wheel for hurry.filesize (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-cp36-none-any.whl size=4134 sha256=81c81021748435f0c82fc650236b80811700a998f1d2fe87671beecaa1c2d2ff\n","  Stored in directory: /root/.cache/pip/wheels/2c/2b/7d/b63fffa26fe9949c4c44f226c566387b72617269c94b89eded\n","  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7765 sha256=e6bd67fea10d6fd2c736a3713bb614a349835993168e640e90e7e871fca0aa68\n","  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n","  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=87675be3b604ff26fd908be095c597f8c268c98bf9b8cf17c133eacd738e45c0\n","  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n","  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=ac0ad0d7eda3431efbc8e724e46b32e7c98fd0a32fa591380457769f23a01bc1\n","  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3356 sha256=39372d71716b932ec634d424d038724155a9ec736d7eac31d0e79039849dbeec\n","  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398407 sha256=1a66b72f5efec6ba1871143bd3170167059568de66aece15714b9468dcbbf386\n","  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n","Successfully built news-please hjson readability-lxml langdetect PyDispatcher ago hurry.filesize protego tinysegmenter feedparser feedfinder2 jieba3k\n","\u001b[31mERROR: boto3 1.10.47 has requirement botocore<1.14.0,>=1.13.47, but you'll have botocore 1.14.9 which is incompatible.\u001b[0m\n","\u001b[31mERROR: boto3 1.10.47 has requirement s3transfer<0.3.0,>=0.2.0, but you'll have s3transfer 0.3.2 which is incompatible.\u001b[0m\n","Installing collected packages: PyDispatcher, cryptography, pyOpenSSL, service-identity, queuelib, cssselect, w3lib, parsel, zope.interface, protego, incremental, constantly, PyHamcrest, hyperlink, Automat, Twisted, Scrapy, PyMySQL, hjson, elasticsearch, readability-lxml, requests-file, tldextract, tinysegmenter, feedparser, feedfinder2, jieba3k, newspaper3k, langdetect, dotmap, warcio, ago, colorama, botocore, s3transfer, rsa, awscli, hurry.filesize, news-please\n","  Found existing installation: botocore 1.13.47\n","    Uninstalling botocore-1.13.47:\n","      Successfully uninstalled botocore-1.13.47\n","  Found existing installation: s3transfer 0.2.1\n","    Uninstalling s3transfer-0.2.1:\n","      Successfully uninstalled s3transfer-0.2.1\n","  Found existing installation: rsa 4.0\n","    Uninstalling rsa-4.0:\n","      Successfully uninstalled rsa-4.0\n","Successfully installed Automat-0.8.0 PyDispatcher-2.0.5 PyHamcrest-2.0.0 PyMySQL-0.9.3 Scrapy-1.8.0 Twisted-19.10.0 ago-0.0.93 awscli-1.17.9 botocore-1.14.9 colorama-0.4.1 constantly-15.1.0 cryptography-2.8 cssselect-1.1.0 dotmap-1.3.8 elasticsearch-7.5.1 feedfinder2-0.0.4 feedparser-5.2.1 hjson-3.0.1 hurry.filesize-0.9 hyperlink-19.0.0 incremental-17.5.0 jieba3k-0.35.1 langdetect-1.0.7 news-please-1.4.25 newspaper3k-0.2.8 parsel-1.5.2 protego-0.1.16 pyOpenSSL-19.1.0 queuelib-1.5.0 readability-lxml-0.7.1 requests-file-1.4.3 rsa-3.4.2 s3transfer-0.3.2 service-identity-18.1.0 tinysegmenter-0.3 tldextract-2.2.2 w3lib-1.21.0 warcio-1.7.1 zope.interface-4.7.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["rsa"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"FtiRZuhioVcs","colab_type":"text"},"source":["##Config\n","\n","save these configurations under /root/news-please-repo/config , in google colab when you open the files tab, go up one level, then create folder under the root directory name it \"news-please-repo\" then under it create folder named \"config\"\n"]},{"cell_type":"markdown","metadata":{"id":"EqSXhVKvfXlA","colab_type":"text"},"source":["### config.cfg\n","\n","save this cell to a file called /root/news-please-repo/config/config.cfg"]},{"cell_type":"code","metadata":{"id":"NIvXgX0IoXwR","colab_type":"code","colab":{}},"source":["# IMPORTANT\n","# All variables get parsed to the correct python-types (if not other declared)!\n","# So bools have to be True or False (uppercase-first),\n","# Floats need dots . (not comma)\n","# Ints are just normal ints\n","# dicts need to be like this { key: value }\n","# arrays need to be like this [ value1, value2, value3 ]\n","# All values in dicts and arrays will also be parsed.\n","# Everything that does not match any of the above criteria will be parsed as string.\n","\n","\n","\n","[Crawler]\n","\n","# GENERAL\n","# -------\n","\n","# Crawling heuristics\n","# Default Crawlers:\n","# Possibilities: RecursiveCrawler, RecursiveSitemapCrawler, RssCrawler, SitemapCrawler, Download (./newsplease/crawler/spiders/-dir)\n","# default: SitemapCrawler\n","default = SitemapCrawler\n","\n","# default:\n","# fallbacks = {\n","#     \"RssCrawler\": None,\n","#     \"RecursiveSitemapCrawler\": \"RecursiveCrawler\",\n","#     \"SitemapCrawler\": \"RecursiveCrawler\",\n","#     \"RecursiveCrawler\": None,\n","#     \"Download\": None\n","#     }\n","fallbacks = {\n","    \"RssCrawler\": None,\n","    \"RecursiveSitemapCrawler\": \"RecursiveCrawler\",\n","    \"SitemapCrawler\": \"RecursiveCrawler\",\n","    \"RecursiveCrawler\": None,\n","    \"Download\": None\n","    }\n","\n","# Determines how many hours need to pass since the last download of a webpage\n","# to be downloaded again by the RssCrawler\n","# default: 6\n","hours_to_pass_for_redownload_by_rss_crawler = 6\n","\n","\n","\n","# PROCESSES\n","# ---------\n","\n","# Number of crawlers, that should crawl parallel\n","# not counting in daemonized crawlers\n","# default: 5\n","number_of_parallel_crawlers = 5\n","\n","# Number of daemons, will be added to daemons.\n","# default: 10\n","number_of_parallel_daemons = 10\n","\n","\n","\n","# SPECIAL CASES\n","# -------------\n","\n","# urls which end on any of the following file extensions are ignored for recursive crawling\n","# default: \"(pdf)|(docx?)|(xlsx?)|(pptx?)|(epub)|(jpe?g)|(png)|(bmp)|(gif)|(tiff)|(webp)|(avi)|(mpe?g)|(mov)|(qt)|(webm)|(ogg)|(midi)|(mid)|(mp3)|(wav)|(zip)|(rar)|(exe)|(apk)|(css)\"\n","ignore_file_extensions = \"(pdf)|(docx?)|(xlsx?)|(pptx?)|(epub)|(jpe?g)|(png)|(bmp)|(gif)|(tiff)|(webp)|(avi)|(mpe?g)|(mov)|(qt)|(webm)|(ogg)|(midi)|(mid)|(mp3)|(wav)|(zip)|(rar)|(exe)|(apk)|(css)\"\n","\n","# urls which match the following regex are ignored for recursive crawling\n","# default: \"\"\n","ignore_regex = \"\"\n","\n","# Crawl the sitemaps of subdomains (if sitemap is enabled)\n","# If True, any SitemapCrawler will try to crawl on the sitemap of the given domain including subdomains instead of a domain's main sitemap.\n","# e.g. if True, a SitemapCrawler to be started on https://blog.zeit.de will try to crawl on the sitemap listed in http://blog.zeit.de/robots.txt. If not found, it will fall back to the False setting.\n","#      if False, a SitemapCrawler to be started on https://blog.zeit.de will try to crawl on the sitemap listed in http://zeit.de/robots.txt\n","# default: True\n","sitemap_allow_subdomains = True\n","\n","\n","\n","[Heuristics]\n","\n","# Enabled heuristics,\n","# Currently:\n","#    - og_type\n","#    - linked_headlines\n","#    - self_linked_headlines\n","#    - is_not_from_subdomain (with this setting enabled, it can be assured that only pages that aren't from a subdomain are downloaded)\n","#    - meta_contains_article_keyword\n","#    - crawler_contains_only_article_alikes\n","# (maybe not up-to-date, see ./newsplease/helper_classes/heursitics.py:\n","#  Every method not starting with __ should be a heuristic, except is_article)\n","# These heuristics can be overwritten by sitelist.json for each site\n","# default: {\"og_type\": True, \"linked_headlines\": \"<=0.65\", \"self_linked_headlines\": \"<=0.56\"}\n","enabled_heuristics = {\"og_type\": True, \"linked_headlines\": \"<=0.65\", \"self_linked_headlines\": \"<=0.56\"}\n","\n","# Heuristics can be combined with others\n","# The heuristics need to have the same name as in enabled_heuristics\n","# Possible condition-characters / literals are: (, ), not, and, or\n","# All heuristics used here need to be enabled in enabled_heuristics as well!\n","# Examples:\n","#     \"og_type and (self_linked_headlines or linked_headlines)\"\n","#     \"og_type\"\n","# default: \"og_type and (linked_headlines or self_linked_headlines)\"\n","pass_heuristics_condition = \"og_type and (linked_headlines or self_linked_headlines)\"\n","\n","# The maximum ratio of headlines divided by linked_headlines in a file\n","\n","# The minimum number of headlines in a file to check for the ratio\n","# If less then this number are in the file, the file will pass the test.\n","# default: 5\n","min_headlines_for_linked_test = 5\n","\n","\n","\n","[Files]\n","\n","# GENERAL:\n","# -------\n","\n","# Paths:\n","# toggles relative paths to be relative to the start_processes.py script (True) or relative to this config file (False)\n","# This does not work for this config's 'Scrapy' section which is always relative to the dir the start_processes.py script is called from\n","# Default: True\n","relative_to_start_processes_file = True\n","\n","\n","\n","# INPUT:\n","# -----\n","\n","# Here you can specify the input JSON-Filename\n","# default: sitelist.hjson\n","url_input_file_name = sitelist.hjson\n","\n","\n","\n","# OUTPUT:\n","# ------\n","\n","# Toggles whether leading './' or '.\\' from above local_data_directory should be removed when saving the path into the Database\n","# True: ./data would become data\n","# default: True\n","working_path = \"/content/drive/My Drive/Amhari\"\n","\n","# Following Strings in the local_data_directory will be replaced: (md5 hashes have a standard length of 32 chars)\n","#\n","# %working_path                           = the path specified in OUTPUT[\"working_path\"]\n","# %time_download(<code>)                  = current time at download; will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/\n","# %time_execution(<code>)                 = current time at execution; will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/\n","# %timestamp_download                     = current time at download; unix-timestamp\n","# %timestamp_execution                    = current time at execution; unix-timestamp\n","# %domain(<size>)                         = first <size> chars of the domain of the crawled file (e.g. zeit.de)\n","# %appendmd5_domain(<size>)               = appends the md5 to %domain(<<size> - 32 (md5 length) - 1 (_ as separator)>) if domain is longer than <size>\n","# %md5_domain(<size>)                     = first <size> chars of md5 hash of %domain\n","# %full_domain(<size>)                    = first <size> chars of the domain including subdomains (e.g. panamapapers.sueddeutsche.de)\n","# %appendmd5_full_domain(<size>)          = appends the md5 to %full_domain(<<size> - 32 (md5 length) - 1 (_ as separator)>) if full_domain is longer than <size>\n","# %md5_full_domain(<size>)                = first <size> chars of md5 hash of %full_domain\n","# %subdomains(<size>)                     = first <size> chars of the domain's subdomains\n","# %appendmd5_subdomains(<size>)           = appends the md5 to %subdomains(<<size> - 32 (md5 length) - 1 (_ as separator)>) if subdomains is longer than <size>\n","# %md5_subdomains(<size>)                 = first <size> chars of md5 hash of %subdomains\n","# %url_directory_string(<size>)           = first <size> chars of the directories on the server (e.g. http://panamapapers.sueddeutsche.de/articles/56f2c00da1bb8d3c3495aa0a/ would evaluate to articles_56f2c00da1bb8d3c3495aa0a), no filename\n","# %appendmd5_url_directory_string(<size>) = appends the md5 to %url_directory_string(<<size> - 32 (md5 length) - 1 (_ as separator)>) if url_directory_string is longer than <size>\n","# %md5_url_directory_string(<size>)       = first <size> chars of md5 hash of %url_directory_string(<size>)\n","# %url_file_name(<size>)                  = first <size> chars of the file name (without type) on the server (e.g. http://www.spiegel.de/wirtschaft/soziales/ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466.html would evaluate to ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466, No filenames (indexes) will evaluate to index\n","# %md5_url_file_name(<size>)              = first <size> chars of md5 hash of %url_file_name\n","# %max_url_file_name                      = first x chars of %url_file_name, so the entire savepath has a length of the max possible length for a windows file system (260 characters - 1 <NUL>)\n","# %appendmd5_max_url_file_name            = appends the md5 to the first x - 32 (md5 length) - 1 (_ as separator) chars of %url_file_name if the entire savepath has a length longer than the max possible length for a windows file system (260 characters - 1 <NUL>)\n","#\n","# This path can be relative or absolute, though to be able to easily merge multiple data sets, it should be kept relative and consistent on all datasets.\n","# To be able to use cleanup commands, it should also start with a static folder name like 'data'.\n","#\n","# default: %working_path/data/%time_execution(%Y)/%time_execution(%m)/%time_execution(%d)/%appendmd5_full_domain(32)/%appendmd5_url_directory_string(60)_%appendmd5_max_url_file_name_%timestamp_download.html\n","local_data_directory = %working_path/data/%appendmd5_full_domain(32)/news%timestamp_download.html\n","\n","\n","# Toggles whether leading './' or '.\\' from above local_data_directory should be removed when saving the path into the Database\n","# True: ./data would become data\n","# default: True\n","format_relative_path = True\n","\n","\n","\n","[MySQL]\n","\n","# MySQL-Connection required for saving meta-informations\n","host = localhost\n","port = 3306\n","db = 'news-please'\n","username = 'root'\n","password = 'password'\n","\n","\n","\n","[Elasticsearch]\n","\n","# Elasticsearch-Connection required for saving detailed meta-information\n","host = localhost\n","port = 9200\n","index_current = 'news-please'\n","index_archive = 'news-please-archive'\n","\n","# Elasticsearch supports user authentication by CA certificates. If your database is protected by certificate\n","# fill in the following parameters, otherwise you can ignore them.\n","use_ca_certificates = False\n","ca_cert_path = /path/to/cacert.pem\n","client_cert_path = /path/to/client_cert.pem\n","client_key_path = /path/to/client_key.pem\n","username = 'root'\n","secret = 'password'\n","\n","# Properties of the document type used for storage.\n","mapping = {\"properties\": {\n","    \"url\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"source_domain\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"title_page\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"title_rss\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"localpath\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"filename\": {\"type\": \"keyword\"},\n","    \"ancestor\": {\"type\": \"keyword\"},\n","    \"descendant\": {\"type\": \"keyword\"},\n","    \"version\": {\"type\": \"long\"},\n","    \"date_download\": {\"type\": \"date\", \"format\":\"yyyy-MM-dd HH:mm:ss\"},\n","    \"date_modify\": {\"type\": \"date\", \"format\":\"yyyy-MM-dd HH:mm:ss\"},\n","    \"date_publish\": {\"type\": \"date\", \"format\":\"yyyy-MM-dd HH:mm:ss\"},\n","    \"title\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"description\":  {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"text\": {\"type\": \"text\"},\n","    \"authors\": {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"image_url\":  {\"type\": \"text\",\"fields\":{\"keyword\":{\"type\":\"keyword\"}}},\n","    \"language\": {\"type\": \"keyword\"}\n","    }}\n","\n","\n","\n","[ArticleMasterExtractor]\n","\n","# Choose which extractors you want to use.\n","#\n","# The Default is ['newspaper_extractor', 'readability_extractor', 'date_extractor', 'lang_detect_extractor'],\n","# which are all integrated extractors right now.\n","# Possibly extractors are 'newspaper_extractor' , 'readability_extractor' , 'date_extractor_extractor and 'lang_detect_extractor'\n","# Examples: -Only Newspaper and date_extractor: extractors = ['newspaper', 'date_extractor']\n","#           -Only Newspaper: extractors = ['newspaper']\n","extractors = ['newspaper_extractor', 'readability_extractor', 'date_extractor', 'lang_detect_extractor']\n","\n","\n","\n","[DateFilter]\n","\n","# If added to the pipeline, this module provides the means to filter the extracted articles based on the publishing date.\n","# Therefore this module has to be placed after the KM4 article extractor to access the publishing dates.\n","#\n","# All articles, with a publishing date outside of the given time interval are dropped. The dates used to specify the\n","# time interval are included and should follow this format: 'yyyy-mm-dd hh:mm:ss'.\n","#\n","# It is also possible to only define one date, assigning the other variable the value 'None' to create an half-bounded\n","# interval.\n","\n","start_date = '1999-01-01 00:00:00'\n","end_date = '2999-12-31 00:00:00'\n","\n","# If 'True' articles without a publishing date are dropped.\n","strict_mode = False\n","\n","\n","\n","[Scrapy]\n","\n","# Possible levels (must be UC-only): CRITICAL, ERROR, WARNING, INFO, DEBUG\n","# default: WARNING\n","LOG_LEVEL = INFO\n","\n","# logformat, see https://docs.python.org/2/library/logging.html#logrecord-attributes\n","# default: [%(name)s:%(lineno)d|%(levelname)s] %(message)s\n","LOG_FORMAT = [%(name)s:%(lineno)d|%(levelname)s] %(message)s\n","\n","# Can be a filename or None\n","# default: None\n","LOG_FILE = None\n","\n","LOG_DATEFORMAT = %Y-%m-%d %H:%M:%S\n","\n","LOG_STDOUT = False\n","\n","LOG_ENCODING = utf-8\n","\n","BOT_NAME = 'news-please'\n","\n","SPIDER_MODULES = ['newsplease.crawler.spiders']\n","NEWSPIDER_MODULE = 'newsplease.crawler.spiders'\n","\n","# Resume/Pause functionality activation\n","# default: .resume_jobdir\n","JOBDIRNAME = .resume_jobdir\n","\n","# Respect robots.txt activation\n","# default: True\n","ROBOTSTXT_OBEY=True\n","\n","# Maximum number of concurrent requests across all domains\n","# default: 16\n","# IMPORTANT: This setting does not work since each crawler has its own scrapy instance, but it might limit the concurrent_requests_per_domain if said setting has a higher number set than this one.\n","CONCURRENT_REQUESTS=16\n","\n","# Maximum number of active requests per domain\n","# default: 4\n","CONCURRENT_REQUESTS_PER_DOMAIN=4\n","\n","# User-agent activation\n","# default: 'news-please (+http://www.example.com/)'\n","USER_AGENT = 'news-please (+http://www.example.com/)'\n","\n","# Pipeline activation\n","# Syntax: '<relative location>.<Pipeline name>': <Order of execution from 0-1000>\n","# default: {'newsplease.pipeline.pipelines.ArticleMasterExtractor':100, 'newsplease.crawler.pipeline.HtmlFileStorage':200, 'newsplease.pipeline.pipelines.JsonFileStorage': 300}\n","# Further options: 'newsplease.pipeline.pipelines.ElasticsearchStorage': 350\n","ITEM_PIPELINES = {'newsplease.pipeline.pipelines.ArticleMasterExtractor':100,\n","                  ##'newsplease.pipeline.pipelines.HtmlFileStorage':200,\n","                  'newsplease.pipeline.pipelines.JsonFileStorage':300\n","                  }\n","\n","[Pandas]\n","file_name = \"PandasStorage\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LYp4_Crfhij","colab_type":"text"},"source":["### sitelist.hjson\n","\n","save this cell to a file called /root/news-please-repo/config/sitelist.hjson"]},{"cell_type":"markdown","metadata":{"id":"eP4S5BoLlWVM","colab_type":"text"},"source":["here we scrap from 9 websites, to add new websites, these are good links that would prove helpful\n","\n","https://bharatdiscovery.org/india/%E0%A4%B8%E0%A4%AE%E0%A4%BE%E0%A4%9A%E0%A4%BE%E0%A4%B0_%E0%A4%AA%E0%A4%A4%E0%A5%8D%E0%A4%B0\n","https://bharatdiscovery.org/india/%E0%A4%A8%E0%A4%88%E0%A4%A6%E0%A5%81%E0%A4%A8%E0%A4%BF%E0%A4%AF%E0%A4%BE\n","https://bharatdiscovery.org/india/%E0%A4%86%E0%A4%9C\n","https://www.naidunia.com/latest-news"]},{"cell_type":"code","metadata":{"id":"j6xAbovLocSK","colab_type":"code","colab":{}},"source":["# This is a HJSON-File, so comments and so on can be used! See https://hjson.org/\n","# Furthermore this is first of all the actual config file, but as default just filled with examples.\n","{\n","  # Every URL has to be in an array-object in \"base_urls\".\n","  # The same URL in combination with the same crawler may only appear once in this array.\n","  \"base_urls\" : [\n","\t{\n","      # Start crawling from timesofindia\n","      \"url\": \"https://www.dw.com/am\",\n","\n","      # Overwrite the default crawler and use th RecursiveCrawler instead\n","      \"crawler\": \"RecursiveCrawler\",\n","\n","      # Because this site is weirt, use the\n","      # meta_contains_article_keyword-heuristic and disable all others because\n","      # overwrite will merge the defaults from \"newscrawler.cfg\" with\n","      # this\n","      \"overwrite_heuristics\": {\n","        \"meta_contains_article_keyword\": true,\n","        \"og_type\": false,\n","        \"linked_headlines\": false,\n","        \"self_linked_headlines\": false\n","      },\n","      # Also state that in the condition, all heuristics used in the condition\n","      # have to be activated in \"overwrite_heuristics\" (or default) as well.\n","      \"pass_heuristics_condition\": \"meta_contains_article_keyword\"\n","    },\n","\t{\n","      # Start crawling from timesofindia\n","      \"url\": \"https://amharic.voanews.com/\",\n","\n","      # Overwrite the default crawler and use th RecursiveCrawler instead\n","      \"crawler\": \"RecursiveCrawler\",\n","\n","      # Because this site is weirt, use the\n","      # meta_contains_article_keyword-heuristic and disable all others because\n","      # overwrite will merge the defaults from \"newscrawler.cfg\" with\n","      # this\n","      \"overwrite_heuristics\": {\n","        \"meta_contains_article_keyword\": true,\n","        \"og_type\": false,\n","        \"linked_headlines\": false,\n","        \"self_linked_headlines\": false\n","      },\n","      # Also state that in the condition, all heuristics used in the condition\n","      # have to be activated in \"overwrite_heuristics\" (or default) as well.\n","      \"pass_heuristics_condition\": \"meta_contains_article_keyword\"\n","    },\n","\t{\n","      # Start crawling from timesofindia\n","      \"url\": \"https://www.zehabesha.com/amharic/\",\n","\n","      # Overwrite the default crawler and use th RecursiveCrawler instead\n","      \"crawler\": \"RecursiveCrawler\",\n","\n","      # Because this site is weirt, use the\n","      # meta_contains_article_keyword-heuristic and disable all others because\n","      # overwrite will merge the defaults from \"newscrawler.cfg\" with\n","      # this\n","      \"overwrite_heuristics\": {\n","        \"meta_contains_article_keyword\": true,\n","        \"og_type\": false,\n","        \"linked_headlines\": false,\n","        \"self_linked_headlines\": false\n","      },\n","      # Also state that in the condition, all heuristics used in the condition\n","      # have to be activated in \"overwrite_heuristics\" (or default) as well.\n","      \"pass_heuristics_condition\": \"meta_contains_article_keyword\"\n","    },\n","\t{\n","      # Start crawling from timesofindia\n","      \"url\": \"https://www.addisadmassnews.com/\",\n","\n","      # Overwrite the default crawler and use th RecursiveCrawler instead\n","      \"crawler\": \"RecursiveCrawler\",\n","\n","      # Because this site is weirt, use the\n","      # meta_contains_article_keyword-heuristic and disable all others because\n","      # overwrite will merge the defaults from \"newscrawler.cfg\" with\n","      # this\n","      \"overwrite_heuristics\": {\n","        \"meta_contains_article_keyword\": true,\n","        \"og_type\": false,\n","        \"linked_headlines\": false,\n","        \"self_linked_headlines\": false\n","      },\n","      # Also state that in the condition, all heuristics used in the condition\n","      # have to be activated in \"overwrite_heuristics\" (or default) as well.\n","      \"pass_heuristics_condition\": \"meta_contains_article_keyword\"\n","    },\n","\t{\n","      # Start crawling from timesofindia\n","      \"url\": \"https://www.ethiopianreporter.com/\",\n","\n","      # Overwrite the default crawler and use th RecursiveCrawler instead\n","      \"crawler\": \"RecursiveCrawler\",\n","\n","      # Because this site is weirt, use the\n","      # meta_contains_article_keyword-heuristic and disable all others because\n","      # overwrite will merge the defaults from \"newscrawler.cfg\" with\n","      # this\n","      \"overwrite_heuristics\": {\n","        \"meta_contains_article_keyword\": true,\n","        \"og_type\": false,\n","        \"linked_headlines\": false,\n","        \"self_linked_headlines\": false\n","      },\n","      # Also state that in the condition, all heuristics used in the condition\n","      # have to be activated in \"overwrite_heuristics\" (or default) as well.\n","      \"pass_heuristics_condition\": \"meta_contains_article_keyword\"\n","    }\n","\t]\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9yEXZmnWoeV3","colab_type":"text"},"source":["##Run"]},{"cell_type":"code","metadata":{"id":"8glo9SaJxdax","colab_type":"code","outputId":"6086f91c-8257-4d9f-de40-7e41d90171f8","executionInfo":{"status":"ok","timestamp":1580065568924,"user_tz":-120,"elapsed":257001,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["!news-please "],"execution_count":0,"outputs":[{"output_type":"stream","text":["[newsplease.config:164|INFO] Loading config-file (/root/news-please-repo/config/config.cfg)\n","[newsplease.config:164|INFO] Loading config-file (/root/news-please-repo/config/config.cfg)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KAGw0ugDA0im","colab_type":"code","outputId":"fe707513-80c8-4721-ffdf-fc08ac429352","executionInfo":{"status":"ok","timestamp":1580064972423,"user_tz":-120,"elapsed":3853,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!find \"/content/drive/My Drive/Amhari/data2/data/amharic.borkena.com\" -type f | wc -l"],"execution_count":0,"outputs":[{"output_type":"stream","text":["131\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6FpzWfg3AZGR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e21efd78-1fda-4bca-a29b-11a219c8d83b","executionInfo":{"status":"ok","timestamp":1580240506880,"user_tz":-120,"elapsed":2757,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}}},"source":["!find \"/root/news-please-repo/data/2020/01/28/bbc.com/amharic.*\" -type f | wc -l"],"execution_count":8,"outputs":[{"output_type":"stream","text":["find: ‘/root/news-please-repo/data/2020/01/28/bbc.com/amharic.*’: No such file or directory\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lKFxy993q18b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"f7cf0f30-f1da-408d-b567-29e1d28e303b","executionInfo":{"status":"ok","timestamp":1580240603159,"user_tz":-120,"elapsed":857,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}}},"source":["import glob\n","mylist = [f for f in glob.glob(\"/root/news-please-repo/data/2020/01/28/bbc.com/amharic*\")]\n","mylist"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51262102_1580239826.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51277957_1580239830.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51260440_1580239826.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51277846_1580239824.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_51270660_1580239824.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51282456_1580239823.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51276517_1580239827.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51260530_1580239825.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51276516_1580239825.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51276516_1580239825.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51277957_1580239830.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51277846_1580239824.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51282456_1580239823.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51276517_1580239827.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51262101_1580239825.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51260440_1580239826.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51262102_1580239826.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51260442_1580239827.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51260530_1580239825.html',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_51270660_1580239824.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51260442_1580239827.html.json',\n"," '/root/news-please-repo/data/2020/01/28/bbc.com/amharic_news-51262101_1580239825.html']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"L5mmiqN3rgKz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f07f5e65-d5ec-4782-e967-36581d2d246e","executionInfo":{"status":"ok","timestamp":1580240617286,"user_tz":-120,"elapsed":833,"user":{"displayName":"amr zaki","photoUrl":"","userId":"09456039094530776333"}}},"source":["len(mylist)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"kh5w5RWDrjpV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}