{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Selfie_Capture_When_We_Smile.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5KA8ug1OU/SIGmSjkqtP9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Selfie Capture When We Smile**"],"metadata":{"id":"76ntAUSVwVHO"}},{"cell_type":"markdown","source":["Most smartphones these days have a feature that automatically takes a selfie when we smile. It is amazing how accurately it detects smiles for not only one but multiple faces and captures a selfie immediately. If you have wondered how this is possible, it is actually quite simple. Using some of the libraries like dlib and OpenCV it is possible to build a selfie capturing application with just a few lines of code. The concept involved here to identify the mouth region using dlib, measure the distance between the corners of the lips when the user smiles and immediately capture a picture. Let’s get started!\n","\n","In this practice session, we will learn how to build a selfie capture application that automatically clicks pictures of you when you smile. "],"metadata":{"id":"yXTlxqGPwdat"}},{"cell_type":"markdown","source":["## **Procedure:**\n","\n","> * Using the 68-point landmark of dlib we will detect the lip points and write relevant functions.\n","> * Face recognition \n","> * Detect smile and automatically capture and save the image. "],"metadata":{"id":"XVEoLmz8whcj"}},{"cell_type":"markdown","source":["## **Finding the Mouth Region Using the 68 Point Landmark Detector.**"],"metadata":{"id":"h4UI-5MfyF_G"}},{"cell_type":"code","execution_count":null,"source":["\n","!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk opencv-python tensorflow keras torch torchvision \\\n","    tqdm scikit-image pixellib pytube dlib --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["# !wget -nd https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat"],"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-06-01 04:16:10--  https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/JeffTrain/selfie/master/shape_predictor_68_face_landmarks.dat [following]\n","--2021-06-01 04:16:10--  https://raw.githubusercontent.com/JeffTrain/selfie/master/shape_predictor_68_face_landmarks.dat\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99693937 (95M) [application/octet-stream]\n","Saving to: ‘shape_predictor_68_face_landmarks.dat’\n","\n","shape_predictor_68_ 100%[===================>]  95.08M   196MB/s    in 0.5s    \n","\n","2021-06-01 04:16:12 (196 MB/s) - ‘shape_predictor_68_face_landmarks.dat’ saved [99693937/99693937]\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Uv780JMvR3D","executionInfo":{"status":"ok","timestamp":1622520972569,"user_tz":-330,"elapsed":1741,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"be3fff79-f32f-4404-94ba-385ef844f20a"}},{"cell_type":"markdown","source":["The 68 point landmark detector is part of the dlib library. It assigns 68 coordinates to every human face which makes detection of specific regions like lips, eyes, nose easier. If you have not already installed dlib you can do so by "],"metadata":{"id":"oUBN6wdlyUuM"}},{"cell_type":"code","execution_count":2,"source":["!python -m pip install dlib --user -q"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dlib in /usr/local/lib/python3.7/dist-packages (19.18.0)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aj5UWGAcyXkX","executionInfo":{"status":"ok","timestamp":1622520992089,"user_tz":-330,"elapsed":3939,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"8ab4e20d-d837-49d0-ff70-1c8f7a1166f4"}},{"cell_type":"markdown","source":["The first step is to identify the region around the mouth"],"metadata":{"id":"bn__QQMsybK9"}},{"cell_type":"markdown","source":["We have identified that the mouth region lies between points 48 to 59. To establish the ration of the mouth we need to find the distance between the corner of the lips, top and bottom of the lip and the left and right regions of the mouth. \n","\n","To find the ratio of the region we can use the euclidean distance formula as follows:\n","![image.png](data:image/png;base64,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)\n","\n","Now with these 8 points, we can successfully isolate the mouth region. To avoid confusion while programming, we can splice the array between 0 and 8 instead of between 48 to 59.\n","\n","We will now load the libraries and write the function to isolate the mouth region. "],"metadata":{"id":"wlT7QAPIyd-H"}},{"cell_type":"code","execution_count":3,"source":["from imutils.video import VideoStream, FPS\n","from imutils import face_utils\n","import imutils\n","import numpy as np\n","import time\n","import dlib\n","import cv2\n","from scipy.spatial import distance as dist\n","landmark_detect = dlib.get_frontal_face_detector()\n","landmark_predict = dlib.shape_predictor('https://gitlab.com/AnalyticsIndiaMagazine/practicedatasets/-/raw/main/face_swapping/shape_predictor_68_face_landmarks.dat')\n","(smile_start,smile_end) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n","def detect_lips(lip):\n","    corner_A = dist.euclidean(lip[3], lip[9])\n","    corner_B = dist.euclidean(lip[2], lip[10])\n","    corner_C = dist.euclidean(lip[4], lip[8])\n","    avg = (corner_A+corner_B+corner_C)/3\n","    corner_D = dist.euclidean(lip[0], lip[6])\n","    ratio=avg/corner_D\n","    return ratio"],"outputs":[],"metadata":{"id":"YZ12HdAZynGj","executionInfo":{"status":"ok","timestamp":1622521057797,"user_tz":-330,"elapsed":3873,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["## **Face Recognition**"],"metadata":{"id":"j1pg_c4fyrSI"}},{"cell_type":"markdown","source":["Next, we will open the web camera. Before we implement a smile detector we need to recognize the face. We will use convexHull from the OpenCV to detect faces. Once the faces are detected we will draw anchor boxed around the mouth. "],"metadata":{"id":"MgSDqTu5yv26"}},{"cell_type":"code","execution_count":4,"source":["webcam = VideoStream(src=0).start()\n","while True:\n","    window_frame = webcam.read()\n","    window_frame = imutils.resize(window_frame, width=450)\n","    gray = cv2.cvtColor(window_frame, cv2.COLOR_BGR2GRAY)\n","    anchor = landmark_detect(gray, 0)\n","    for box in anchor:\n","        smile_finder = landmark_predict(gray, box)\n","        smile_finder = face_utils.shape_to_np(smile_finder)\n","        smile= smile_finder[smile_start:smile_end]\n","        ratio= detect_lips(smile)\n","        smileHull = cv2.convexHull(smile)\n","        cv2.drawContours(window_frame, [smileHull], -1, (255, 0, 0), 1)"],"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-29f6c697c84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwindow_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwindow_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0manchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlandmark_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"24k1L-4tyvfa","executionInfo":{"status":"error","timestamp":1622521096826,"user_tz":-330,"elapsed":357,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"17a779a3-cd27-42e9-cf90-a7bbc5cc0d01"}},{"cell_type":"markdown","source":["In the above code, we have used the dawContours to draw a red coloured box. Once this is done we just need to auto-capture the image."],"metadata":{"id":"AjZjML26y28-"}},{"cell_type":"markdown","source":["## **Selfie Capture**\n","\n"," We will set the time between 20 and 25 frames before the selfie is captured. The images are saved in the same folder as the file you are running. "],"metadata":{"id":"dHLTDZbIy5cU"}},{"cell_type":"code","execution_count":5,"source":["count = 0\n","tot = 0\n","if ratio <= .2 or ratio > .25 :\n","  count = count+1\n","else:\n","  if count >= 10:\n","    tot= tot+1\n","    window_frame = webcam.read()\n","    time.sleep(.3)\n","    frame2= window_frame.copy()\n","    save_img = \"selfie{}.png\".format(tot)\n","    cv2.imwrite(save_img, window_frame)\n","    print(\"{} captured\".format(save_img))\n","    count = 0"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-42d1748f764f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m.2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.25\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ratio' is not defined"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"6ysN9npuy8EB","executionInfo":{"status":"error","timestamp":1622521623131,"user_tz":-330,"elapsed":364,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"fc240c4e-42bd-40bd-e4f7-dc55df5c4a1a"}},{"cell_type":"markdown","source":["The last step is to show the frame on the screen and see the output. "],"metadata":{"id":"QOcnIgz6y9iH"}},{"cell_type":"code","execution_count":null,"source":["cv2.imshow(\"Frame\", window_frame)"],"outputs":[],"metadata":{"id":"w7050E4Ky_MH"}},{"cell_type":"markdown","source":["We will also set q as the exit button for closing the window once the user is done. "],"metadata":{"id":"I5N4pi9SzCVc"}},{"cell_type":"code","execution_count":null,"source":["key2 = cv2.waitKey(1) & 0xFF\n","    if key2 == ord('q'):\n","        break\n","cv2.destroyAllWindows()\n","webcam.stop()"],"outputs":[],"metadata":{"id":"fEGK_yNKzEHn"}},{"cell_type":"markdown","source":["All the captured images are stored in the same folder as your project. One of the captured pictures is given below."],"metadata":{"id":"lTE1DX6mzF7d"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [Selfie Capture When you Smile](https://analyticsindiamag.com/selfie-capture-when-we-smile-my-fun-project-using-opencv/)\n","\n","> * [Bitwise Operations On Images Using OpenCV](https://analyticsindiamag.com/how-to-implement-bitwise-operations-on-images-using-opencv/)\n","\n","> * [Face Swaping with OpenCV](https://analyticsindiamag.com/a-fun-project-on-building-a-face-swapping-application-with-opencv/)\n","\n","> * [Create Watermark Images with OpenCV](https://analyticsindiamag.com/how-to-create-a-watermark-on-images-using-opencv/)\n","\n","> * [Convert Image to Cartoon](https://analyticsindiamag.com/converting-an-image-to-a-cartoon/)\n","\n","> * [Sudoku Game with Deep Learning, OpenCV and Backtracking](https://analyticsindiamag.com/solve-sudoku-puzzle-using-deep-learning-opencv-and-backtracking/)\n","\n"],"metadata":{"id":"jqDB1l9I6Br-"}}]}