{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Biggest_Short_Video_Dataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOu7YlD7cbJUrXN/eBXkZfg"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# **Biggest Short Video Dataset**"],"metadata":{"id":"3vlGw2RbQtA8"}},{"cell_type":"markdown","source":["Moment in Time is one of the biggest human-commented video datasets catching visual and discernible short occasions created by people, creatures, articles and nature. It was developed in 2018 by the researchers: Mathew Monfort, Alex Andonian, Bolei Zhou and Kandan Ramakrishnan. The dataset comprises more than 1,000,000 3-second recordings relating to 339 unique action words. Every action word is related to more than 1,000 recordings bringing about a huge adjusted dataset for taking in powerful occasions from recordings. The various day to day activities associated with this dataset includes falling on the floor, the opening of the mouth, eye, swimming, bouncing etc."],"metadata":{"id":"TGCkZjvsQwRI"}},{"cell_type":"markdown","source":["Here, we will examine the information contained in this dataset, how it was assembled, and give some benchmark models that gave high exactness on this dataset. Further, we will execute the Moment in time dataset utilizing Pytorch"],"metadata":{"id":"9YS0VBzmQy7a"}},{"cell_type":"markdown","source":["Please refer [this](https://analyticsindiamag.com/moment-in-time-the-biggest-short-video-dataset-for-data-scientists/) article to know about it more."],"metadata":{"id":"oTe6NIKAQz2V"}},{"cell_type":"code","execution_count":null,"source":["\n","!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk opencv-python tensorflow keras torch torchvision \\\n","    tqdm scikit-image --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import os\n","import cv2\n","import argparse\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torchvision. models as models\n","from torch.nn import functional as F\n","from torchvision import transforms as torchN\n","def load_model(categories, weight_file):\n","    if not os.access(weight_file, os.W_OK):\n","        weight_url = 'http://moments.csail.mit.edu/moments_models/' + weight_file\n","        os.system('wget ' + weight_url)\n","    model = models.__dict__['resnet50'](num_classes=len(categories))\n","    useGPU = 0\n","    if useGPU == 1:\n","        checkpoint = torch.load(weight_file)\n","    else:\n","        checkpoint = torch.load(weight_file, map_location=lambda storage, loc: storage) # allow cpu\n","    dict1={str.replace(str(k),'module.',''):v for k,v in checkpoint['state_dict'].items()}\n","    model.load_state_dict(dict1)\n","    model.eval()\n","    return model"],"outputs":[],"metadata":{"id":"KO8dTrfCPg2h","executionInfo":{"status":"ok","timestamp":1623753395479,"user_tz":-330,"elapsed":3221,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":2,"source":["def load_transform():\n","    \"\"\"Load the image transformer.\"\"\"\n","    tensor = torchN.Compose([\n","        torchN.Resize((224, 224)),\n","        torchN.ToTensor(),\n","        torchN.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","    return tensor\n","def load_categories(filename):\n","    \"\"\"Load categories.\"\"\"\n","    with open(filename) as f:\n","        return [line.rstrip() for line in f.readlines()]"],"outputs":[],"metadata":{"id":"Nh2DDqaYPjqD","executionInfo":{"status":"ok","timestamp":1623753398067,"user_tz":-330,"elapsed":4,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":1,"source":["# !wget https://raw.githubusercontent.com/zhoubolei/moments_models/master/category_momentsv2.txt"],"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-29 12:08:25--  https://raw.githubusercontent.com/zhoubolei/moments_models/master/category_momentsv2.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2770 (2.7K) [text/plain]\n","Saving to: ‘category_momentsv2.txt’\n","\n","category_momentsv2. 100%[===================>]   2.71K  --.-KB/s    in 0s      \n","\n","2021-10-29 12:08:25 (29.7 MB/s) - ‘category_momentsv2.txt’ saved [2770/2770]\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sjnh5ER5QCP5","executionInfo":{"status":"ok","timestamp":1623753504043,"user_tz":-330,"elapsed":986,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"6b3876ce-7d9c-4648-c2cd-d5f74d3b72d0"}},{"cell_type":"code","execution_count":6,"source":["if __name__ == '__main__':\n","    # load categories and model\n","    categories = load_categories('category_momentsv2.txt')\n","    model = load_model(categories, 'moments_v2_RGB_resnet50_imagenetpretrained.pth.tar')\n","    # load the transformer\n","    tensorflow1 = load_transform()  # image transformer\n","        # load the test image\n","    if os.path.exists('test.jpg'):\n","        os.remove('test.jpg')\n","    img_url = 'http://places2.csail.mit.edu/imgs/demo/IMG_5970.JPG'\n","    os.system('wget %s -q -O test.jpg' % img_url)\n","    image = Image.open('test.jpg')\n","    input_img = tensorflow1(image).unsqueeze(0)"],"outputs":[],"metadata":{"id":"Bn2GrgT3Pobz","executionInfo":{"status":"ok","timestamp":1623753527623,"user_tz":-330,"elapsed":1172,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}}]}