{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_Preprocessing_of_Data.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNx0HcINZBS/apGiq6k3pLo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Preprocessing of Data**"],"metadata":{"id":"IDJC40pb-9Tz"}},{"cell_type":"markdown","source":["Data Preprocessing is the process of preparing the data for analysis. This is the first step in any machine learning model.\n"],"metadata":{"id":"5EciY9qy_Jn8"}},{"cell_type":"markdown","source":["Please refer [this article](https://analyticsindiamag.com/get-started-preparing-data-machine-learning/) to read about whole Machine Learning Cycle."],"metadata":{"id":"wNetrHzy_eT3"}},{"cell_type":"markdown","source":["Here in this simple tutorial we will learn to implement Data preprocessing to perform the following operations on a raw dataset:\n","\n","* Dealing with missing data\n","* Dealing with categorical data\n","* Splitting the dataset into training and testing sets\n","* Scaling the features\n"],"metadata":{"id":"oNJhKAmm_YZ6"}},{"cell_type":"markdown","source":["## **Importing the pandas**"],"metadata":{"id":"bB_Kshs98O-a"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn openpyxl --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":168,"source":["import pandas as pd"],"outputs":[],"metadata":{"id":"ELhWVlvVXkzC","executionInfo":{"status":"ok","timestamp":1621856799588,"user_tz":-330,"elapsed":342,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["## **Loading the dataset**"],"metadata":{"id":"o_vIkpjk8UcU"}},{"cell_type":"code","execution_count":169,"source":["dataset = pd.read_excel(\"age_salary.xlsx\")"],"outputs":[],"metadata":{"id":"HnHZizwb8Xvz","executionInfo":{"status":"ok","timestamp":1621856800979,"user_tz":-330,"elapsed":994,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["The data set used here is as simple as shown below:"],"metadata":{"id":"chxEO-5G8gIo"}},{"cell_type":"code","execution_count":170,"source":["dataset.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","      <td>44.0</td>\n","      <td>72000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Spain</td>\n","      <td>27.0</td>\n","      <td>48000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Germany</td>\n","      <td>30.0</td>\n","      <td>54000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Spain</td>\n","      <td>38.0</td>\n","      <td>61000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany</td>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Country   Age   Salary Purchased\n","0   France  44.0  72000.0        No\n","1    Spain  27.0  48000.0       Yes\n","2  Germany  30.0  54000.0        No\n","3    Spain  38.0  61000.0        No\n","4  Germany  40.0      NaN       Yes"]},"metadata":{"tags":[]},"execution_count":170}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"3mKOY5zj8gvC","executionInfo":{"status":"ok","timestamp":1621856800980,"user_tz":-330,"elapsed":44,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"c2233c6e-37cd-48c5-ae5a-81a8046e6975"}},{"cell_type":"markdown","source":["**Note:**\n","The ‘nan’ you see in some cells of the dataframe denotes the missing fields"],"metadata":{"id":"OzNjQoKS8qBT"}},{"cell_type":"markdown","source":["Lets take only two columns firstly"],"metadata":{"id":"k4mZrPRP9CRI"}},{"cell_type":"code","execution_count":171,"source":["df = dataset[['Age', 'Salary']]"],"outputs":[],"metadata":{"id":"mM64HL_09CAo","executionInfo":{"status":"ok","timestamp":1621856800981,"user_tz":-330,"elapsed":42,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":172,"source":["df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>44.0</td>\n","      <td>72000.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>27.0</td>\n","      <td>48000.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30.0</td>\n","      <td>54000.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>38.0</td>\n","      <td>61000.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Age   Salary\n","0  44.0  72000.0\n","1  27.0  48000.0\n","2  30.0  54000.0\n","3  38.0  61000.0\n","4  40.0      NaN"]},"metadata":{"tags":[]},"execution_count":172}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"hMB7irON9WZI","executionInfo":{"status":"ok","timestamp":1621856800983,"user_tz":-330,"elapsed":42,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"29fd8c33-ad62-4f7c-bec3-47875b1aff57"}},{"cell_type":"markdown","source":["Now that we have loaded our dataset lets play with it."],"metadata":{"id":"KxfmqJb78619"}},{"cell_type":"markdown","source":["## **Classifying the dependent and Independent Variables**"],"metadata":{"id":"LZKBOe3Q9Ofw"}},{"cell_type":"markdown","source":["Having seen the data we can clearly identify the dependent and independent factors.Here we just have 2 factors, age and salary.Salary is the dependent factor that changes with the independent factor age.Now let’s classify them programmatically."],"metadata":{"id":"MvS7XtdK9R7W"}},{"cell_type":"code","execution_count":173,"source":["X = df.iloc[:,:-1].values #Takes all rows of all columns except the last column\n","Y = df.iloc[:,-1].values # Takes all rows of the last column"],"outputs":[],"metadata":{"id":"FCHSNBMQ9VNn","executionInfo":{"status":"ok","timestamp":1621856800984,"user_tz":-330,"elapsed":40,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["> * X : independent variable set\n","> * Y : dependent variable set\n"],"metadata":{"id":"MMd5DQNP9bUZ"}},{"cell_type":"markdown","source":["The dependent and independent values are stored in different arrays. In case of multiple independent variables use `X = dataset.iloc[:,a:b].values ` where a is the starting range and b is the ending range (column indices). You can also specify the column indices in a list to select specific columns."],"metadata":{"id":"CVTi_DgZ9h1J"}},{"cell_type":"markdown","source":["## **Dealing with Missing Data**"],"metadata":{"id":"-8FQSg_i9rsm"}},{"cell_type":"markdown","source":["We have already noticed the missing fields in the data denoted by “nan”. Machine learning models cannot accommodate missing fields in the data they are provided with.So the missing fields must be filled with values that will not affect the variance of the data or make it more noisy."],"metadata":{"id":"_dbekcb89u-4"}},{"cell_type":"code","execution_count":174,"source":["from sklearn.impute import SimpleImputer\n","import numpy as np\n","imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","X = imp.fit_transform(X)\n","Y = Y.reshape(-1,1)\n","Y = imp.fit_transform(Y)\n","Y = Y.reshape(-1)"],"outputs":[],"metadata":{"id":"-FOTnhBW9v9g","executionInfo":{"status":"ok","timestamp":1621856800985,"user_tz":-330,"elapsed":40,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["The scikit-learn library’s SimpleImputer Class allows us to impute the missing fields in a dataset with valid data. In the above code, we have used the default strategy for filling missing values which is the mean. The imputer can not be applied on 1D arrays and since Y is a 1D array, it needs to be converted to a compatible shape.The reshape functions allows us to reshape any array.The `fit_transform()` method will fit the imputer object and then transforms the arrays."],"metadata":{"id":"A09HUeg29u2G"}},{"cell_type":"code","execution_count":175,"source":["X"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[44.        ],\n","       [27.        ],\n","       [30.        ],\n","       [38.        ],\n","       [40.        ],\n","       [35.        ],\n","       [38.77777778],\n","       [48.        ],\n","       [50.        ],\n","       [37.        ]])"]},"metadata":{"tags":[]},"execution_count":175}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUScBw-296Hp","executionInfo":{"status":"ok","timestamp":1621856800986,"user_tz":-330,"elapsed":41,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"5f761f5a-46d7-4005-9c6f-980be91eeda4"}},{"cell_type":"code","execution_count":176,"source":["Y"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([72000.        , 48000.        , 54000.        , 61000.        ,\n","       63777.77777778, 58000.        , 52000.        , 79000.        ,\n","       83000.        , 67000.        ])"]},"metadata":{"tags":[]},"execution_count":176}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaP-WdrF97oJ","executionInfo":{"status":"ok","timestamp":1621856800987,"user_tz":-330,"elapsed":38,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"93e1fa84-a8da-45e4-9113-500a0910549a"}},{"cell_type":"markdown","source":["## **Dealing with Categorical Data**"],"metadata":{"id":"XoWgVWjv-L4F"}},{"cell_type":"markdown","source":["When dealing with large and real-world datasets, categorical data is almost inevitable.Categorical variables represent types of data which may be divided into groups. Examples of categorical variables are race, sex, age group, educational level etc. These variables often has letters or words as its values. Since machine learning models are all about numbers and calculations , these categorical variables need to be coded in to numbers. Having coded the categorical variable into numbers may  just not be enough.\n","\n","For example, consider the dataset below with 2 categorical features nation and purchased_item. Let us assume that the dataset is a record of how age, salary and country of a person determine if an item is purchased or not.Thus purchased_item is the dependent factor and age, salary and nation are the independent factors."],"metadata":{"id":"YGNqjIrz-PBy"}},{"cell_type":"code","execution_count":177,"source":["dataset[\"Age\"] = X\n","dataset[\"Salary\"] = Y"],"outputs":[],"metadata":{"id":"paZ5DRNN-RVg","executionInfo":{"status":"ok","timestamp":1621856800988,"user_tz":-330,"elapsed":37,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":178,"source":["dataset"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","      <td>44.000000</td>\n","      <td>72000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Spain</td>\n","      <td>27.000000</td>\n","      <td>48000.000000</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Germany</td>\n","      <td>30.000000</td>\n","      <td>54000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Spain</td>\n","      <td>38.000000</td>\n","      <td>61000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany</td>\n","      <td>40.000000</td>\n","      <td>63777.777778</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>France</td>\n","      <td>35.000000</td>\n","      <td>58000.000000</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Spain</td>\n","      <td>38.777778</td>\n","      <td>52000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>France</td>\n","      <td>48.000000</td>\n","      <td>79000.000000</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Germany</td>\n","      <td>50.000000</td>\n","      <td>83000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>France</td>\n","      <td>37.000000</td>\n","      <td>67000.000000</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Country        Age        Salary Purchased\n","0   France  44.000000  72000.000000        No\n","1    Spain  27.000000  48000.000000       Yes\n","2  Germany  30.000000  54000.000000        No\n","3    Spain  38.000000  61000.000000        No\n","4  Germany  40.000000  63777.777778       Yes\n","5   France  35.000000  58000.000000       Yes\n","6    Spain  38.777778  52000.000000        No\n","7   France  48.000000  79000.000000       Yes\n","8  Germany  50.000000  83000.000000        No\n","9   France  37.000000  67000.000000       Yes"]},"metadata":{"tags":[]},"execution_count":178}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"xvWNtIbqF7q2","executionInfo":{"status":"ok","timestamp":1621856800989,"user_tz":-330,"elapsed":38,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"66a86e25-9b3b-45e0-83de-682e1aead4eb"}},{"cell_type":"markdown","source":["It has 3 countries listed. In a larger dataset, these may be large groups of data. Since countries don’t have a mathematical relation between them unless we are considering some known factors such as size or population etc , coding them in numbers will not work, as a number may be less than or greater than another number. Dummy variables are the solution. Using one hot encoding we will create a dummy variable for each of the category in the column. And uses binary encoding for each dummy variable. We do not need to create dummy variables for the feature purchased_item as it has only 2 categories either yes or no."],"metadata":{"id":"YQZs7-Qt-YPF"}},{"cell_type":"code","execution_count":179,"source":["X = pd.DataFrame(dataset.iloc[:,[0,1,2]].values)\n","Y = dataset.iloc[:,3].values\n","\n","from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n","le_X = LabelEncoder()\n","X.iloc[:,0] = le_X.fit_transform(X.iloc[:,0])\n","country_one_hot_encode = pd.get_dummies(dataset.Country, prefix='Country')"],"outputs":[],"metadata":{"id":"l8izy65r-bqo","executionInfo":{"status":"ok","timestamp":1621856800990,"user_tz":-330,"elapsed":37,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":180,"source":["X.drop([0], axis = 1, inplace=True)\n","X = pd.concat([X,country_one_hot_encode], axis=1)"],"outputs":[],"metadata":{"id":"wkVyYgbWMOA7","executionInfo":{"status":"ok","timestamp":1621856800991,"user_tz":-330,"elapsed":38,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":181,"source":["X"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>Country_France</th>\n","      <th>Country_Germany</th>\n","      <th>Country_Spain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>44</td>\n","      <td>72000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>27</td>\n","      <td>48000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30</td>\n","      <td>54000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>38</td>\n","      <td>61000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40</td>\n","      <td>63777.8</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>35</td>\n","      <td>58000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>38.7778</td>\n","      <td>52000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>48</td>\n","      <td>79000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>50</td>\n","      <td>83000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>37</td>\n","      <td>67000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         1        2  Country_France  Country_Germany  Country_Spain\n","0       44    72000               1                0              0\n","1       27    48000               0                0              1\n","2       30    54000               0                1              0\n","3       38    61000               0                0              1\n","4       40  63777.8               0                1              0\n","5       35    58000               1                0              0\n","6  38.7778    52000               0                0              1\n","7       48    79000               1                0              0\n","8       50    83000               0                1              0\n","9       37    67000               1                0              0"]},"metadata":{"tags":[]},"execution_count":181}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"EFsLTypKMj-h","executionInfo":{"status":"ok","timestamp":1621856800992,"user_tz":-330,"elapsed":38,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"4317dcc7-816e-42d7-c9dd-ebea8dc2f0a5"}},{"cell_type":"markdown","source":["The the last 3 columns are the dummy features representing Germany,India and Russia respectively.The 1’s in each column represent that the person belongs to that specific country."],"metadata":{"id":"KabzMuVLKkEC"}},{"cell_type":"code","execution_count":188,"source":["Y = le_X.fit_transform(Y)"],"outputs":[],"metadata":{"id":"_8zuP1bRIye8","executionInfo":{"status":"ok","timestamp":1621856824588,"user_tz":-330,"elapsed":363,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":189,"source":["Y"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"]},"metadata":{"tags":[]},"execution_count":189}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1w7mCPlZ_Q19","executionInfo":{"status":"ok","timestamp":1621856824939,"user_tz":-330,"elapsed":5,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"9d730619-7f29-42b3-b981-53045b6796b9"}},{"cell_type":"markdown","source":["## **Splitting the Dataset into Training and Testing sets**"],"metadata":{"id":"QVGheH1bKp1g"}},{"cell_type":"markdown","source":["All machine learning models require us to provide a training set for the machine so that the model can train from that data to understand the relations between features and can predict for new observations.When we are provided a single huge dataset with too much of observations ,it is a good idea to split the dataset into to two, a training_set and a test_set, so that we can test our model after its been trained with the training_set.\n","\n","Scikit-learn comes with a method called train_test_split to help us with this task."],"metadata":{"id":"rQLzIMIWKt1n"}},{"cell_type":"code","execution_count":190,"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)"],"outputs":[],"metadata":{"id":"RaFW6SWDKv4G","executionInfo":{"status":"ok","timestamp":1621856829225,"user_tz":-330,"elapsed":344,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["The above code will split X and Y into two subsets each.\n","\n","> * test_size: the desired size of the test_set. 0.3 denotes 30%.\n","> * random_state:  This is used to preserve the uniqueness. The split will happen uniquely for a random_state.\n"],"metadata":{"id":"hAec9bGeKx0o"}},{"cell_type":"markdown","source":["## **Scaling the features**"],"metadata":{"id":"sH91QPLMK2u4"}},{"cell_type":"markdown","source":["Since machine learning models rely on numbers to solve relations it is important to have similarly scaled data in a dataset. Scaling ensures that all data in a dataset falls in the same range.Unscaled data can cause inaccurate or false predictions.Some machine learning algorithms can handle feature scaling on its own and doesn’t require it explicitly."],"metadata":{"id":"ore-_BzYK7RU"}},{"cell_type":"markdown","source":["The StandardScaler class from the scikit-learn library can help us scale the dataset."],"metadata":{"id":"qk7XqmYZK9Oy"}},{"cell_type":"code","execution_count":191,"source":["from sklearn.preprocessing import StandardScaler\n","sc_X = StandardScaler()\n","X_train = sc_X.fit_transform(X_train)\n","X_test = sc_X.transform(X_test)\n","\n","sc_y = StandardScaler()\n","Y_train = Y_train.reshape((len(Y_train), 1))\n","Y_train = sc_y.fit_transform(Y_train)\n","Y_train = Y_train.ravel()"],"outputs":[],"metadata":{"id":"9_aUYn3qK_tN","executionInfo":{"status":"ok","timestamp":1621856835512,"user_tz":-330,"elapsed":347,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["To read more about it, refer [here](https://analyticsindiamag.com/data-pre-processing-in-python/)."],"metadata":{"id":"BDPaEbVe_qvR"}}]}