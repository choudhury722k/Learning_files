{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"LSTM Autoencoder.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **LSTM Autoencoder**"],"metadata":{"id":"fAmxp9psyQsP"}},{"cell_type":"markdown","source":["Simple Neural Network is feed-forward wherein info information ventures just in one direction.i.e. the information passes from input layers to hidden layers finally to the output layers. Recurrent Neural Network is the advanced type to the traditional Neural Network. It makes use of sequential information. Unlike conventional networks, the output and input layers are dependent on each other. RNNs are called recurrent because they play out a similar undertaking for each component of an arrangement, with the yield being relied upon the past calculations.LSTM or Long Short Term Memory are a type of RNNs that is useful in learning order dependence in sequence prediction problems."],"metadata":{"id":"QOXeKmmVyYFx"}},{"cell_type":"markdown","source":["In this notebook, we will cover a simple Long Short Term Memory autoencoder with the help of Keras and python"],"metadata":{"id":"HrkdO8V5yce0"}},{"cell_type":"markdown","source":["## **What is an LSTM autoencoder?**"],"metadata":{"id":"4t99XmlFyfYo"}},{"cell_type":"markdown","source":["\n","LSTM autoencoder is an encoder that makes use of LSTM encoder-decoder architecture to compress data using an encoder and decode it to retain original structure using a decoder."],"metadata":{"id":"Nf5Fijr-ylGl"}},{"cell_type":"markdown","source":["## **About the dataset**"],"metadata":{"id":"CwGxFb-lyoug"}},{"cell_type":"markdown","source":["The dataset can be downloaded from the following [link](https://www.kaggle.com/pdquant/sp500-daily-19862018). It gives the daily closing price of the S&P index"],"metadata":{"id":"afcW03aOyr0x"}},{"cell_type":"markdown","source":["## **Code Implementation With Keras**"],"metadata":{"id":"3qTTwRqIy9EC"}},{"cell_type":"markdown","source":["### **Import libraries required for this project**"],"metadata":{"id":"IkGMOIclzAzI"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels keras tensorflow torch --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"id":"86suwc_usyQC"}},{"cell_type":"markdown","source":["### **Read the data**"],"metadata":{"id":"ceOuusnms7o-"}},{"cell_type":"code","execution_count":null,"source":["df = pd.read_csv('spx.csv', parse_dates=['date'], index_col='date')"],"outputs":[],"metadata":{"id":"5Tk4If7nzic5"}},{"cell_type":"markdown","source":["### **Split the data**"],"metadata":{"id":"Q9WPGXbazkmX"}},{"cell_type":"code","execution_count":null,"source":["train_size = int(len(df) * 0.9)\n","test_size = len(df) - train_size\n","train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n","train.shape"],"outputs":[],"metadata":{"id":"l11DYA2azmBg"}},{"cell_type":"markdown","source":["### **Pre-Processing of Data**"],"metadata":{"id":"fuJ1mCF_zox5"}},{"cell_type":"markdown","source":["We need to pre-process the training and test data using the standardscaler library imported from sklearn."],"metadata":{"id":"4RGvYU7szsbk"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","scaler = scaler.fit(train[['close']])\n","train['close'] = scaler.transform(train[['close']])\n","test['close'] = scaler.transform(test[['close']])"],"outputs":[],"metadata":{"id":"mE99v4cszvZI"}},{"cell_type":"markdown","source":["### **Create a sequence with historical data**"],"metadata":{"id":"rL5G5MmVzyyQ"}},{"cell_type":"markdown","source":["Now we will split the time series data into subsequences and create a sequence of 30 days of historical data."],"metadata":{"id":"6Q9i7PYBz2V9"}},{"cell_type":"code","execution_count":null,"source":["def create_dataset(X, y, time_steps=1):\n","    X1, y1 = [], []\n","    for i in range(len(X) - time_steps):\n","        t = X.iloc[i:(i + time_steps)].values\n","        X1.append(t)\n","        y1.append(y.iloc[i + time_steps])\n","    return np.array(X1), np.array(y1)"],"outputs":[],"metadata":{"id":"K9wvTryrz4P9"}},{"cell_type":"code","execution_count":null,"source":["TIME_STEPS = 30\n","X_train, y_train = create_dataset(\n","  train[['close']],\n","  train.close,\n","  TIME_STEPS\n",")\n","X_test, y_test = create_dataset(\n","  test[['close']],\n","  test.close,\n","  TIME_STEPS\n",")\n","print(X_train.shape)"],"outputs":[],"metadata":{"id":"Mymj0PWDz6Tq"}},{"cell_type":"markdown","source":["### **Creating an LSTM Autoencoder Network**"],"metadata":{"id":"LPEIULvYz9t7"}},{"cell_type":"markdown","source":["The architecture will produce the same sequence as given as input. It will take the sequence data. The dropout removes inputs to a layer to reduce overfitting. Adding RepeatVector to the layer means it repeats the input n number of times. The TimeDistibuted layer takes the information from the previous layer and creates a vector with a length of the output layers."],"metadata":{"id":"knJ9vb8W0ZLb"}},{"cell_type":"code","execution_count":null,"source":["import keras\n","model = keras.Sequential()\n","model.add(keras.layers.LSTM(\n","    units=64,\n","    input_shape=(X_train.shape[1], X_train.shape[2])\n","))\n","model.add(keras.layers.Dropout(rate=0.2))\n","model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n","model.add(keras.layers.LSTM(units=64, return_sequences=True))\n","model.add(keras.layers.Dropout(rate=0.2))\n","model.add(\n","  keras.layers.TimeDistributed(\n","    keras.layers.Dense(units=X_train.shape[2])\n","  )\n",")\n","model.compile(loss='mae', optimizer='adam')\n","model.summary()"],"outputs":[],"metadata":{"id":"06HpaHUp0YJo"}},{"cell_type":"markdown","source":["### **Fitting the Model**"],"metadata":{"id":"mkwTc7240eiD"}},{"cell_type":"markdown","source":["Here, we train the model with epoch:20 and batch size 32."],"metadata":{"id":"qdm7C1lH2e5b"}},{"cell_type":"code","execution_count":null,"source":["history = model.fit(\n","    X_train, y_train,\n","    epochs=20,\n","    batch_size=32,\n","    validation_split=0.1,\n","    shuffle=False\n",")"],"outputs":[],"metadata":{"id":"xSUCUnsM0WLC"}},{"cell_type":"markdown","source":["### **Evaluation**"],"metadata":{"id":"GANVdi-30VL4"}},{"cell_type":"code","execution_count":null,"source":["plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend();"],"outputs":[],"metadata":{"id":"YhTIPWOq34jG"}},{"cell_type":"markdown","source":["From the above plot we can see the training and test error is decreasing. For better result, we can train the model with more epochs."],"metadata":{"id":"J7l82KlA37a4"}},{"cell_type":"markdown","source":["### **Actual Value of Test Data**"],"metadata":{"id":"YbvN2SRl39X7"}},{"cell_type":"code","execution_count":null,"source":["y_test"],"outputs":[],"metadata":{"id":"2xuWzahc3_3N"}},{"cell_type":"markdown","source":["### **Prediction on Test Data**"],"metadata":{"id":"B1gIAgB54Bxw"}},{"cell_type":"code","execution_count":null,"source":["pred = model.predict(X_test, verbose=0)"],"outputs":[],"metadata":{"id":"rRlqRCoA4EK2"}},{"cell_type":"markdown","source":["# **Related Articles --**\n","\n","> * [Introduction to LSTM Autoencoder](https://analyticsindiamag.com/introduction-to-lstm-autoencoder-using-keras/)\n","> * [Beginners Guide to MLP](https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/)\n","> * [Beginners Guide to PyTorch](https://analyticsindiamag.com/a-beginners-guide-pytorch/)\n","> * [Loss functions in PyTorch](https://analyticsindiamag.com/all-pytorch-loss-function/)\n","> * [Loss functions in Tensorflow Keras](https://analyticsindiamag.com/ultimate-guide-to-loss-functions-in-tensorflow-keras-api-with-python-implementation/)\n","> * [Loss function with examples](https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/)\n","> * [Optimizers in Tensorflow Keras](https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/)\n","> * [Deep Learning Frameworks](https://analyticsindiamag.com/deep-learning-frameworks/)\n","> * [Types of Activation Functions](https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/)\n","> * [Maths for Deep Learning](https://analyticsindiamag.com/beginners-guide-neural-network-math-python/)\n","> * [Deep Learning Using Tensorflow Keras](https://analyticsindiamag.com/deep-learning-using-tensorflow-keras/)"],"metadata":{"id":"fD68lYDPkZTw"}}]}