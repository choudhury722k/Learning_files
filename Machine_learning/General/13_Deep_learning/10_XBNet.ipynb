{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"XBNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMs8TY3HUHmGD2HbOC23YaB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# XBNet"],"metadata":{"id":"W9CkHDev_Rm-"}},{"cell_type":"markdown","source":["‘XBNet’, which stands for ‘Extremely Boosted Neural Network’, that combines gradient boosted tree with a feed-forward neural network, making the model robust for all performance metrics. In this approach, trees are being trained in every layer of the architecture, and feature importance given by the trees and weight determined by gradient descent is used to adjust the weights of layers where trees are trained.      \n","\n","XBNet takes raw tabular data as input, and the model is being trained using an optimization technique called Boosted Gradient Descent which is initialized with the help of feature importance of a gradient boosted trees further it updates the weights of each layer in the neural network in two steps as below:\n","\n","> * Updates Weight by Gradient descent.\n","> * Updates weights by using feature importance of gradient boosted trees.  "],"metadata":{"id":"ts8tu4w2_YFX"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/guide-to-xbnet-an-extremely-boosted-neural-network/) article."],"metadata":{"id":"2pqnbe3S_X-S"}},{"cell_type":"markdown","source":["# Code Implementation"],"metadata":{"id":"KLLNfEPf_ioH"}},{"cell_type":"markdown","source":["Here we will compare the performance of XBNet and custom neural networks maintaining the same training parameters.\n","XBNet:\n","\n","Install the architecture using pip as below"],"metadata":{"id":"35FjsEgP_pBR"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels keras tensorflow torch --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install --upgrade git+https://github.com/tusharsarkar3/XBNet.git --user -q --no-warn-script-location"],"outputs":[],"metadata":{"id":"8nZo5BmmtUhX"}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Import all the dependencies:"],"metadata":{"id":"eKQIiwht_q2E"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","import numpy as np\n","from sklearn.model_selection import train_test_split"],"outputs":[],"metadata":{"id":"umCcV7gvthAs"}},{"cell_type":"code","execution_count":null,"source":["from XBNet.training_utils import training,predict\n","from XBNet.models import XBNETClassifier\n","from XBNet.run import run_XBNET"],"outputs":[],"metadata":{"id":"vgOZnRP0tpGI"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.datasets import load_iris"],"outputs":[],"metadata":{"id":"Q5hn-OQQt9dG"}},{"cell_type":"code","execution_count":null,"source":["data = load_iris()"],"outputs":[],"metadata":{"id":"7xwkOwBIuFnP"}},{"cell_type":"code","execution_count":null,"source":["data.target"],"outputs":[],"metadata":{"id":"mCbLHITtuIie"}},{"cell_type":"markdown","source":["Set the input output features and train test split"],"metadata":{"id":"dIjUAHWX_spe"}},{"cell_type":"code","execution_count":null,"source":["x = data.data\n","y = data.target\n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.3, random_state= True)"],"outputs":[],"metadata":{"id":"Hj_7gF9duJSv"}},{"cell_type":"markdown","source":["Initialize the architecture with training data; while initializing you need to set input-output dimensions of each layer here. I have set the number of layers as two, so I need to set the dimension manually. Don’t worry; it is pretty straightforward; you will be prompted to do so, as shown below.  "],"metadata":{"id":"7mU2471S_3Xm"}},{"cell_type":"code","execution_count":null,"source":["model = XBNETClassifier(x_train,y_train,num_layers=2)"],"outputs":[],"metadata":{"id":"FO6JYkjzujpE"}},{"cell_type":"markdown","source":["Set the loss function and optimizer."],"metadata":{"id":"SFbO52Yu_5QR"}},{"cell_type":"code","execution_count":null,"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"],"outputs":[],"metadata":{"id":"xY0Fin6Wu-Bi"}},{"cell_type":"markdown","source":["Run the architecture using run_XBNET"],"metadata":{"id":"lK6nwiq5_7Au"}},{"cell_type":"code","execution_count":null,"source":["m,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=100,batch_size=32)"],"outputs":[],"metadata":{"id":"l3bpPq01wg-j"}},{"cell_type":"markdown","source":["Classification report for training and validation, respectively. "],"metadata":{"id":"-VSkw_WT_8_Y"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n"],"outputs":[],"metadata":{"id":"EjHrer295A6S"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc,label='training accuracy')\n","plt.plot(val_ac,label = 'validation accuracy')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(lo,label='training loss')\n","plt.plot(val_lo,label = 'validation loss')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend()"],"outputs":[],"metadata":{"id":"wopiPHPQ5Cky"}},{"cell_type":"markdown","source":["##Custom neural network"],"metadata":{"id":"EPCxn9NDHiZd"}},{"cell_type":"code","execution_count":null,"source":["from keras.models import Sequential\n","from keras.layers import Dense,Conv1D,Flatten"],"outputs":[],"metadata":{"id":"KmFfkP8l1lOa"}},{"cell_type":"code","execution_count":null,"source":["model2 = Sequential()"],"outputs":[],"metadata":{"id":"sWTgRhBh10Tn"}},{"cell_type":"code","execution_count":null,"source":["model2=Sequential()\n","model2.add(Conv1D(30,3, input_shape=(4,1), activation='relu'))                                        ## created simple neural network\n","model2.add(Dense(10,activation='relu'))  \n","model2.add(Flatten())                                                                 ## added hiden layer\n","model2.add(Dense(3,activation='softmax'))"],"outputs":[],"metadata":{"id":"4GD8mK8T10WH"}},{"cell_type":"code","execution_count":null,"source":["model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"12F7bGss10YH"}},{"cell_type":"code","execution_count":null,"source":["model2.summary()"],"outputs":[],"metadata":{"id":"NvEhqWvv3wop"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.preprocessing import OneHotEncoder"],"outputs":[],"metadata":{"id":"ShbfJH1N5cQU"}},{"cell_type":"code","execution_count":null,"source":["x=data.data\n","y=data.target.reshape(-1,1)\n","y=OneHotEncoder().fit_transform(y).toarray()"],"outputs":[],"metadata":{"id":"vcte6sKkGv9-"}},{"cell_type":"code","execution_count":null,"source":["x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=True,test_size=0.33)"],"outputs":[],"metadata":{"id":"X4Fgr_z9GwAW"}},{"cell_type":"code","execution_count":null,"source":["x_train = x_train.reshape(100, 4,1)\n","x_test = x_test.reshape(50, 4,1)"],"outputs":[],"metadata":{"id":"s4rsJjzSG3CF"}},{"cell_type":"code","execution_count":null,"source":["history = model2.fit(x_train,y_train,epochs=50,validation_split=0.2)"],"outputs":[],"metadata":{"id":"QSXbvDNl5jfQ"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"id":"1QnxhjJd5jjZ"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(history.history['accuracy'],label='training accuracy')\n","plt.plot(history.history['val_accuracy'],label = 'validation accuracy')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(history.history['loss'],label='training loss')\n","plt.plot(history.history['val_loss'],label = 'validation loss')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend()"],"outputs":[],"metadata":{"id":"j7t9FAkO5jlw"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.metrics import classification_report"],"outputs":[],"metadata":{"id":"OatQWPDfHahQ"}},{"cell_type":"code","execution_count":null,"source":["print(classification_report(y_test,model2.predict(x_test).round()))"],"outputs":[],"metadata":{"id":"k4XVFtx8JTgz"}},{"cell_type":"code","execution_count":null,"source":["print(classification_report(y_train,model2.predict(x_train).round()))"],"outputs":[],"metadata":{"id":"L_wAhplqKO3A"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"wgYOfWbQKgZS"}}]}