{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Beginners_Guide_PyTorch.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOwzWCQX3YVrNSGtaaBTDVK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**PyTorch**"],"metadata":{"id":"TWD3ReYef8ib"}},{"cell_type":"markdown","source":["\n","Pytorch is a deep learning library which has been created by Facebook AI in 2017. It is prominently being used by many companies like Apple, Nvidia, AMD etc. You can read more about the companies that are using it from [here](https://discovery.hgdata.com/product/pytorch).\n","\n","It is also often compared to TensorFlow, which was forged by Google in 2015, which is also a prominent deep learning library. \n","\n","You can read about how PyTorch is competing with TensorFlow from [here](https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b#:~:text=So%2C%20both%20TensorFlow%20and%20PyTorch,from%20which%20you%20may%20choose.).\n","\n","There are a lot of functions and explaining each of them is not always possible, so will be writing a brief code that would explain it and then would give a simple explanation for the same. If you want to read more about it, click on the link that is shared in each section."],"metadata":{"id":"SPw8ucmEf6DV"}},{"cell_type":"markdown","source":["## **Installation**"],"metadata":{"id":"oesydXmagoM7"}},{"cell_type":"markdown","source":["Installation command is different for different OS, you can check the best one for you from [here](https://pytorch.org/?utm_source=Google&utm_medium=PaidSearch&utm_campaign=%2A%2ALP+-+TM+-+General+-+HV+-+INDIA&utm_adgroup=PyTorch+Installation&utm_keyword=pytorch%20installation&utm_offering=AI&utm_Product=PyTorch&gclid=Cj0KCQiAhZT9BRDmARIsAN2E-J2aOHgldt9Jfd0pWHISa8UER7TN2aajgWv_TIpLHpt8MuaAlmr8vBcaAkgjEALw_wcB)."],"metadata":{"id":"uIy9Pd-4gsbG"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels keras tensorflow --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install torch --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#dependency \n","\n","import torch"],"outputs":[],"metadata":{"id":"MEs-wTxbg7ow"}},{"cell_type":"markdown","source":["We’d have a look at tensors first because they are really important.\n","\n","Let us understand what a tensor is."],"metadata":{"id":"nQSVTApZhASa"}},{"cell_type":"markdown","source":["##**Tensors**"],"metadata":{"id":"84NeZY9XhKdf"}},{"cell_type":"markdown","source":["Tensor is in simple words is a multidimensional array which is also generalised against vectors and matrices. Now let us see what all things can we do with it."],"metadata":{"id":"-KXgqXCBhIgt"}},{"cell_type":"code","execution_count":null,"source":["# a few lucid examples of tensor\n","a=torch.tensor(3)\n","a=torch.tensor([1,3])\n","a=torch.tensor([[1,2],[3,4]])\n","a"],"outputs":[],"metadata":{"id":"z27F3hH_fyVz"}},{"cell_type":"code","execution_count":null,"source":["# a tensor, a bit complex one\n","tensor = torch.Tensor(\n","    [\n","     [[1, 2], [3, 4]], \n","     [[5, 6], [7, 8]], \n","     [[9, 0], [1, 2]]\n","    ]\n",")"],"outputs":[],"metadata":{"id":"Suifx5AbhpHW"}},{"cell_type":"code","execution_count":null,"source":["tensor.shape #to find the shape of the tensor"],"outputs":[],"metadata":{"id":"K_1Jg3AEhn9M"}},{"cell_type":"code","execution_count":null,"source":["#Index of tensors\n","tensor[1]"],"outputs":[],"metadata":{"id":"JwFNIL02hx3w"}},{"cell_type":"markdown","source":["### **Initialising like tensors**"],"metadata":{"id":"f4stlWXLgfKf"}},{"cell_type":"markdown","source":["Like tensors are the ones which have the same shape as that of others."],"metadata":{"id":"SUVw3gDMjr7F"}},{"cell_type":"code","execution_count":null,"source":["torch.ones_like(tensor)"],"outputs":[],"metadata":{"id":"9H-oVc1qjrne"}},{"cell_type":"markdown","source":["Here the shape of this would be the same as that of our previous tensor and all the elements in this tensor would be 1."],"metadata":{"id":"RNKorVwrjxTS"}},{"cell_type":"code","execution_count":null,"source":["torch.zeros_like(tensor)"],"outputs":[],"metadata":{"id":"kfLanGm4jzgc"}},{"cell_type":"markdown","source":["All the elements of this tensor would be zero."],"metadata":{"id":"Bfr7BSaYj8L1"}},{"cell_type":"code","execution_count":null,"source":["#here we would be creating a tensor whose every element would be a normal distribution.\n","torch.randn_like(tensor)"],"outputs":[],"metadata":{"id":"59CCz8Fnj-zW"}},{"cell_type":"markdown","source":["Let us take a look at some basics operations on Tensors"],"metadata":{"id":"zVimEmoVlA_5"}},{"cell_type":"code","execution_count":null,"source":["(tensor - 5) * 2"],"outputs":[],"metadata":{"id":"FBypP12elDN9"}},{"cell_type":"markdown","source":["To read more about tensors, you can refer [here](https://pytorch.org/docs/stable/tensors.html)."],"metadata":{"id":"j4EgzOypoN53"}},{"cell_type":"markdown","source":["You can have a look at Pytorch’s official documentation from [here](https://pytorch.org/)."],"metadata":{"id":"FbokcFQooTuO"}},{"cell_type":"markdown","source":["We will see a few deep learning methods of PyTorch."],"metadata":{"id":"wRz9l3UnobSX"}},{"cell_type":"markdown","source":["## **Pytorch’s neural network module**"],"metadata":{"id":"7uBiecOToc95"}},{"cell_type":"markdown","source":["### **nn.Linear**"],"metadata":{"id":"jJGbrSpHouUL"}},{"cell_type":"code","execution_count":null,"source":["#dependency\n","import torch.nn as nn\n","nn.Linear"],"outputs":[],"metadata":{"id":"Rl6TmwnWohvB"}},{"cell_type":"markdown","source":["It is to create a linear layer. Here we pass the input and output dimensions as parameters."],"metadata":{"id":"vssNjrliokh6"}},{"cell_type":"markdown","source":["Here it is taking an input of nx10 and would return an output of nx2."],"metadata":{"id":"A3Edk_-JomL0"}},{"cell_type":"code","execution_count":null,"source":["linear = nn.Linear(10, 2)\n","example_input = torch.randn(3, 10)\n","example_output = linear(example_input)\n","example_output"],"outputs":[],"metadata":{"id":"E5gRoiOcokLe"}},{"cell_type":"markdown","source":["### **nn.Relu**"],"metadata":{"id":"_qByCwIiorZc"}},{"cell_type":"markdown","source":["It performs a relu activation function operation on the given output from linear."],"metadata":{"id":"Uertb6Mmoyxv"}},{"cell_type":"code","execution_count":null,"source":["relu = nn.ReLU()\n","relu_output = relu(example_output)\n","relu_output"],"outputs":[],"metadata":{"id":"dUZaoq0Oo1gf"}},{"cell_type":"markdown","source":["### **nn.BatchNorm1d**"],"metadata":{"id":"Vs0iMXXVo1EJ"}},{"cell_type":"markdown","source":["It is a normalisation technique which is used to maintain a consistent mean and standard dev among different batches of the of input."],"metadata":{"id":"MA_uVp8zo6s7"}},{"cell_type":"code","execution_count":null,"source":["batchnorm = nn.BatchNorm1d(2)\n","batchnorm_output = batchnorm(relu_output)\n","batchnorm_output"],"outputs":[],"metadata":{"id":"YnJbUZd_o9Pw"}},{"cell_type":"markdown","source":["You can read about [batchnorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) and [batchnorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) from their official doc."],"metadata":{"id":"cLYXIfyto_kD"}},{"cell_type":"markdown","source":["### **nn.Sequential**"],"metadata":{"id":"coM6Ine5pHYC"}},{"cell_type":"markdown","source":["It is to create a sequence of operations in one go."],"metadata":{"id":"J3WDnlF4pLBW"}},{"cell_type":"markdown","source":["mlp is the name of variable which stands for multilayer perceptron."],"metadata":{"id":"34L3oSs7pMwI"}},{"cell_type":"code","execution_count":null,"source":["mlp_layer = nn.Sequential(\n","    nn.Linear(5, 2),\n","    nn.BatchNorm1d(2),\n","    nn.ReLU()\n",")\n","test_example = torch.randn(5,5) + 1\n","print(\"input: \")\n","print(test_example)\n","print(\"output: \")\n","print(mlp_layer(test_example))"],"outputs":[],"metadata":{"id":"6d6HuS4LpPnU"}},{"cell_type":"markdown","source":["How nn.Sequential is important and why it is needed, read it from [here](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)."],"metadata":{"id":"VbAxLVuVpSPB"}},{"cell_type":"markdown","source":["## **Optimisers**"],"metadata":{"id":"2rDYXi_zpZak"}},{"cell_type":"code","execution_count":null,"source":["import torch.optim as optim\n","adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)\n","#Here lr stands for learning rate and 1e-1 means 0.1\n","#now let us look at the training loop\n","train_example = torch.randn(100,5) + 1\n","adam_opt.zero_grad()\n","# We'll use a simple loss function of mean distance from 1\n","# torch.abs takes the absolute value of a tensor\n","cur_loss = torch.abs(1 - mlp_layer(train_example)).mean()\n","cur_loss.backward()\n","adam_opt.step()\n","print(cur_loss)"],"outputs":[],"metadata":{"id":"KGSfHmkEpRgb"}},{"cell_type":"markdown","source":["A little bit of theory:"],"metadata":{"id":"l7ZoSq5Dpgiq"}},{"cell_type":"markdown","source":["### **requires_grad_()**"],"metadata":{"id":"qrl6bKyCpi4u"}},{"cell_type":"markdown","source":["This means that even if PyTorch wouldn’t normally store a grad for that particular tensor, it will for that specified tensor."],"metadata":{"id":"f1c1jVDRpmZy"}},{"cell_type":"markdown","source":["### **with torch.no_grad():**"],"metadata":{"id":"caLKnNyIpoFp"}},{"cell_type":"markdown","source":["PyTorch will usually calculate the gradients as it proceeds through a set of operations on tensors. This can often take up unnecessary computations and memory, especially if you’re performing an evaluation. However, you can wrap a piece of code with torch.no_grad() to prevent the gradients from being calculated in a piece of code."],"metadata":{"id":"l3-p_UCyprZy"}},{"cell_type":"markdown","source":["### **detach():**"],"metadata":{"id":"IidqM9B5ptdb"}},{"cell_type":"markdown","source":["Sometimes, you want to calculate and use a tensor’s value without calculating its gradients. For example, if you have two models, A and B, and you want to directly optimise the parameters of A with respect to the output of B, without calculating the gradients through B, then you could feed the detached output of B to A. There are many reasons you might want to do this, including efficiency or cyclical dependencies (i.e. A depends on B depends on A)."],"metadata":{"id":"RtO2sd9epx97"}},{"cell_type":"markdown","source":["We are now making the nn class."],"metadata":{"id":"GVK5CYCwpz0R"}},{"cell_type":"code","execution_count":null,"source":["class ExampleModule(nn.Module):\n","    def __init__(self, input_dims, output_dims):\n","        super(ExampleModule, self).__init__()\n","        self.linear = nn.Linear(input_dims, output_dims)\n","        self.exponent = nn.Parameter(torch.tensor(1.))\n","    def forward(self, x):\n","        x = self.linear(x)\n","        # This is the notation for element-wise exponentiation, \n","        # which matches python in general\n","        x = x ** self.exponent \n","        return x\n","example_model = ExampleModule(10, 2)\n","list(example_model.parameters())"],"outputs":[],"metadata":{"id":"42E-aOpip2R8"}},{"cell_type":"markdown","source":["This is the output of the class that we had created:\n"],"metadata":{"id":"tUvROWBZp9Td"}},{"cell_type":"code","execution_count":null,"source":["input = torch.randn(2, 10)\n","example_model(input)"],"outputs":[],"metadata":{"id":"zZMOn7aSp-3W"}},{"cell_type":"markdown","source":["# **Related Articles --**\n","> * [Beginners Guide to PyTorch](https://analyticsindiamag.com/a-beginners-guide-pytorch/)\n","> * [Loss functions in PyTorch](https://analyticsindiamag.com/all-pytorch-loss-function/)\n","> * [Loss functions in Tensorflow Keras](https://analyticsindiamag.com/ultimate-guide-to-loss-functions-in-tensorflow-keras-api-with-python-implementation/)\n","> * [Loss function with examples](https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/)\n","> * [Optimizers in Tensorflow Keras](https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/)\n","> * [Deep Learning Frameworks](https://analyticsindiamag.com/deep-learning-frameworks/)\n","> * [Types of Activation Functions](https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/)\n","> * [Maths for Deep Learning](https://analyticsindiamag.com/beginners-guide-neural-network-math-python/)\n","> * [Deep Learning Using Tensorflow Keras](https://analyticsindiamag.com/deep-learning-using-tensorflow-keras/)"],"metadata":{"id":"fD68lYDPkZTw"}}]}