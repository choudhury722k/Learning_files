{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_SupportVectorClassification_IRIS_DATA.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XXaDGNE_Fk5q"},"source":["# In this practice session, we will learn to code Support Vector Classification. \n","# We will perform the following steps to build a simple classifier using the popular Iris dataset.\n","\n"," \n"," \n","  - **Data Preprocessing**\n","\n","    - Importing the libraries.\n","    - Importing dataset (Dataset Link https://archive.ics.uci.edu/ml/datasets/iris).\n","    - Dealing with the categorical variable.\n","    - Classifying dependent and independent variables.\n","    - Splitting the data into a training set and test set.\n","    - Feature scaling.\n"," \n","\n","  -  **Support Vector Classification**\n","\n","    - Create a Support Vector classifier.\n","    - Feed the training data to the classifier.\n","    - Predicting the species for the test set.\n","    - Using the confusion matrix to find accuracy."]},{"cell_type":"markdown","metadata":{"id":"tA83xFpiFk5y"},"source":["# Load the Dependencies"]},{"cell_type":"code","metadata":{"id":"pwqsTzhqFk50"},"source":["import ipywidgets as widgets\n","from IPython.display import display\n","\n","style = {'description_width': 'initial'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BZ-jLOunczC"},"source":["#1 Importing essential libraries\n","import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","import seaborn as sns\n","iris = load_iris() \n","data = iris.data \n","target = iris.target \n","names = iris.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TqLtNC_Fk52"},"source":["# Load the Dataset"]},{"cell_type":"code","metadata":{"id":"WKtERLr9n09B"},"source":["#file_name = 'iris.data'\n","\n","dataset = pd.DataFrame(data, columns=['sepal length', 'sepal width', 'petal length', \"petal width\"])\n","dataset['Species']=target\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VQYV-kTFk53"},"source":["print(f\"Dataset has {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv-Mu3HSFk53"},"source":["#Plotting the relation between salary and experience\n","wig_col = widgets.Dropdown(\n","                options=[col for col in dataset.columns.tolist() if col.startswith(('sepal', 'petal'))],\n","                description='Choose a Column to Plot vs. Attributes',\n","                disabled=False,\n","                layout=widgets.Layout(width='40%', height='40px'),\n","                style=style)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImqEbMpNFk54"},"source":["# Plot Variables"]},{"cell_type":"code","metadata":{"id":"6r6B8PeQFk54"},"source":["display(wig_col)\n","\n","sns.catplot(x=\"Species\", y=wig_col.value, kind=\"boxen\", data=dataset, height=8.27, aspect=11.7/8.27);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QObH-xDlFk55"},"source":["g = sns.catplot(x=\"Species\", y=wig_col.value, kind=\"violin\", inner=None, data=dataset, height=8.27, aspect=11.7/8.27)\n","sns.swarmplot(x=\"Species\", y=wig_col.value, color=\"k\", size=3, data=dataset, ax=g.ax);\n","\n","display(wig_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5WuUWRFn4j8"},"source":["#3 classify dependent and independent variables\n","X = dataset.iloc[:,:-1].values  #independent variable YearsofExperience\n","y = dataset.iloc[:,-1].values  #dependent variable salary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bZ82kVbn7Ga"},"source":["print(\"\\nIdependent Variable (Sepal and Petal Attributes):\\n\\n\", X[:5])\n","print(\"\\nDependent Variable (Species):\\n\\n\", y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6__E1X3Fk59"},"source":["# Encode Classes"]},{"cell_type":"code","metadata":{"id":"WOJI6Sn6Fk59"},"source":["from sklearn.preprocessing import LabelEncoder\n","labelencoder = LabelEncoder()\n","dataset['Species'] = labelencoder.fit_transform(dataset['Species'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzv49C3fFk59"},"source":["dataset['Species'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fs_y1YPQFk5-"},"source":["# Create Train and Test Sets"]},{"cell_type":"code","metadata":{"id":"BFMIwI5Zn9MX"},"source":["#4 Creating training set and testing set\n","from sklearn.model_selection import train_test_split\n","test_size = widgets.FloatSlider(min=0.01, max=0.6, value=0.2, description=\"Test Size :\", tooltips=['Usually 20-30%'])\n","display(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ub975tYyFk5_"},"source":["#Divide the dataset into Train and Test sets\n","X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=test_size.value, random_state = 0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcY06YGDn_dz"},"source":["print(\"Training Set :\\n----------------\\n\")\n","print(\"X = \\n\", X_train[:5])\n","print(\"y = \\n\", y_train[:5])\n","\n","print(\"\\n\\nTest Set :\\n----------------\\n\")\n","print(\"X = \\n\",X_test[:5])\n","print(\"y = \\n\", y_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vk1SKgHXFk6A"},"source":["print(f\"Shape of Training set is {X_train.shape}\")\n","print(f\"Shape of Testing set is {X_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fIh44Up8Fk6A"},"source":["# Normalise Features\n","\n","As the Features are not in the range of 0-1, Let's normalize the features using Standard Scaler(Z-score) normalization and Label Encode the Class String Names."]},{"cell_type":"code","metadata":{"id":"RSncW_DUFk6A"},"source":["#Feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test) \n","\n","print(\"\\n-------------------------\\nDataset after Scaling:\\n-------------------------\\n\", )\n","\n","print(\"\\nX_train :\\n\", X_train[:5])\n","print(\"-------------------------\")\n","print(\"\\nX_test :\\n\", X_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"796mxvDmFk6A"},"source":["# Support Vector Classification"]},{"cell_type":"code","metadata":{"id":"ZaugTvv6Fk6A"},"source":["# import SLR library\n","from sklearn.svm import SVC\n","\n","# configure params for the model.\n","gamma_wig = widgets.Dropdown(options=[1e-1, 1e-2, 1e-3, 1e-5], \n","                             description=\"Coefficient \\n for Rbf, \\nPoly and Sigmoid = \", style=style)\n","\n","display(gamma_wig)\n","\n","kernel_wig = widgets.Dropdown(options=['linear', 'poly', 'rbf', 'sigmoid'], \n","                             description=\"Kernel Type = \", style=style)\n","\n","display(kernel_wig)\n","\n","c_wig = widgets.Dropdown(options=[10.0, 100.0, 1000.0, 10000.0], \n","                             description=\"Penalty parameter C = \", style=style)\n","\n","display(c_wig)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52C1gwKAFk6B"},"source":["# Predict and Evaluate the Model "]},{"cell_type":"code","metadata":{"id":"8XbYFyk8oCrH"},"source":["classifier = SVC(kernel=kernel_wig.value, gamma=gamma_wig.value,\n","                 C=c_wig.value, random_state=0)\n","\n","#Feed the training data to the classifier\n","classifier.fit(X_train,y_train)\n","\n","#Predicting the species for test set\n","y_pred = classifier.predict(X_test)\n","\n","print(\"\\n---------------------------\\n\")\n","print(\"Predicted Values for Test Set :\\n\",y_pred)\n","print(\"\\n---------------------------\\n\")\n","print(\"Actual Values for Test Set :\\n\",y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAwiJVWuoHEX"},"source":["#8 Claculating the Accuracy of the predictions\n","from sklearn import metrics\n","print(\"Prediction Accuracy = \", metrics.accuracy_score(y_test, y_pred))\n","\n","#9 Comparing Actual and Predicted Salaries for he test set\n","print(\"\\nActual vs Predicted Salaries \\n------------------------------\\n\")\n","error_df = pd.DataFrame({\"Actual\" : y_test,\n","                         \"Predicted\" : y_pred})\n","\n","error_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39gcez4PFk6D"},"source":["# Actual vs. Predicted "]},{"cell_type":"code","metadata":{"id":"ZL0vnbswFk6D"},"source":["\n","#Using confusion matrix to find the accuracy\n","from sklearn.metrics import confusion_matrix, classification_report\n","cm = confusion_matrix(y_test,y_pred)\n","\n","accuracy = cm.diagonal().sum()/cm.sum()\n","\n","print(\"\\n---------------------------\\n\")\n","print(\"Accuracy of Predictions = \",accuracy)\n","\n","print(\"\\n---------------------------\\n\")\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[]}]}