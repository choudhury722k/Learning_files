{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_RandomForestRegression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f9AHD7t5Iy5q"},"source":["# In this practice session, we will learn to code Random Forest Regression. \n","# We will perform the following steps to build a simple classifier using the popular Iris dataset.\n","\n"," \n"," \n","  - **Data Preprocessing**\n","\n","    - Importing the libraries.\n","    - Importing dataset (Dataset Link https://archive.ics.uci.edu/ml/datasets/iris).\n","    - Dealing with the categorical variable.\n","    - Classifying dependent and independent variables.\n","    - Splitting the data into a training set and test set.\n","    - Feature scaling.\n"," \n","\n","  -  **Random Forest Regression**\n","\n","    - Create a Random Forest Regressor.\n","    - Feed the training data to the regression model.\n","    - Predicting the species for the test set.\n","    - Using the RMSE to calculate the error metric."]},{"cell_type":"markdown","metadata":{"id":"D1pArOYwIy58"},"source":["# Load the Dependencies"]},{"cell_type":"code","metadata":{"id":"BjVxQp_SIy5_"},"source":["import ipywidgets as widgets\n","from IPython.display import display\n","\n","style = {'description_width': 'initial'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BZ-jLOunczC"},"source":["#1 Importing essential libraries\n","import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rK6Q35cHIy6E"},"source":["# Load the Dataset"]},{"cell_type":"code","metadata":{"id":"WKtERLr9n09B"},"source":["#2 Importing the dataset\n","\n","file_name = 'beer_data.csv'\n","dataset = pd.read_csv(file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqTYK79RIy6F"},"source":["#Displaying the dataset\n","dataset.head(8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7jP31o0Iy6G"},"source":["print(f\"Dataset has {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjdAKxDBIy6H"},"source":["## Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"QbRw2ihJIy6H"},"source":["#### Drop Nulls and Fill Nulls Based on Mean"]},{"cell_type":"code","metadata":{"id":"sX25bjwSIy6K"},"source":["#check nulls..\n","\n","dataset.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0T_GVSeIy6N"},"source":["dataset = dataset[~dataset['Cellar Temperature'].isna()]\n","dataset.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znPuuCBYIy6O"},"source":["dataset['ABV'].fillna(dataset['ABV'].mean(), inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyeS9UBoIy6O"},"source":["dataset['Ratings'] = dataset['Ratings'].apply(lambda x : np.float32(x.replace(\",\", \"\")))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNjatS6IIy6O"},"source":["# Dealing with the categorical data\n","\n","# Spliting Cellar Temperature into Maximum and Minimum based on the given data and converting the type from str to int\n","\n","dataset.loc[:, 'Minimum_Cellar_Temp'] = dataset['Cellar Temperature'].apply(lambda x : int(str(x).split('-')[0].strip()))\n","dataset.loc[:, 'Maximum_Cellar_Temp'] = dataset['Cellar Temperature'].apply(lambda x : int(str(x).split('-')[1].strip()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4xydWXkIy6Q"},"source":["dataset.drop('Cellar Temperature', inplace=True, axis=1)\n","dataset.columns.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5WuUWRFn4j8"},"source":["# classify dependent and independent variables\n","X = dataset[[col for col in dataset.columns if col not in ('Score')]].values  #independent variables \n","y = dataset['Score'].values  #dependent variable "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bZ82kVbn7Ga"},"source":["print(\"\\nIdependent Variables :\\n\\n\", X[:5])\n","print(\"\\nDependent Variable (Score):\\n\\n\", y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDwGtZzkIy6S"},"source":["# Create Train and Test Sets"]},{"cell_type":"code","metadata":{"id":"BFMIwI5Zn9MX"},"source":["#4 Creating training set and testing set\n","from sklearn.model_selection import train_test_split\n","test_size = widgets.FloatSlider(min=0.01, max=0.6, value=0.2, description=\"Test Size :\", tooltips=['Usually 20-30%'])\n","display(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_o-0ZC3sIy6T"},"source":["#Divide the dataset into Train and Test sets\n","X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=test_size.value, random_state = 0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcY06YGDn_dz"},"source":["print(\"Training Set :\\n----------------\\n\")\n","print(\"X = \\n\", X_train[:5])\n","print(\"y = \\n\", y_train[:5])\n","\n","print(\"\\n\\nTest Set :\\n----------------\\n\")\n","print(\"X = \\n\",X_test[:5])\n","print(\"y = \\n\", y_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ym5HiVajIy6U"},"source":["print(f\"Shape of Training set is {X_train.shape}\")\n","print(f\"Shape of Testing set is {X_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VN35MAlzIy6U"},"source":["# Apply Random Forest Regression "]},{"cell_type":"code","metadata":{"id":"3gZix10WIy6U"},"source":["# import random forest library\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# configure params for the model.\n","max_feat_wig = widgets.ToggleButtons(options=['log2', 'sqrt', 'auto'],\n","                                    description='Number of features for the best split :',\n","                                    disabled=False,\n","                                    style=style)\n","\n","display(max_feat_wig)\n","\n","max_depth_wig = widgets.Dropdown(options=[10, 20, 30, 50],\n","                            description='The maximum depth of the Tree. :',\n","                            style=style)\n","\n","display(max_depth_wig)\n","\n","min_split_wig = widgets.Dropdown(options=[100, 200, 300, 500],\n","                            description='Minimum Number of Splits. :',\n","                            style=style)\n","\n","display(min_split_wig)\n","\n","njobs_wig = widgets.Dropdown(options=[('One', 1), ('Two', 2), ('Three', 3), ('All Cores', -1)], \n","                             description=\"Number of CPU Cores :\", style=style)\n","\n","display(njobs_wig)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XK4ztcwvIy6V"},"source":["# Predict and Evaluate the Model "]},{"cell_type":"code","metadata":{"id":"8XbYFyk8oCrH"},"source":["# Train the Regressor with training set\n","regressor = RandomForestRegressor(max_features=max_feat_wig.value,\n","                                  max_depth=max_depth_wig.value,\n","                                  min_samples_split=min_split_wig.value,\n","                                  n_jobs=njobs_wig.value)\n","\n","#fit the linear model\n","regressor.fit(X_train, y_train)\n","\n","#7 predict the outcome of test sets\n","y_Pred = regressor.predict(X_test)\n","print(\"\\nPredictions = \", y_Pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAwiJVWuoHEX","scrolled":false},"source":["# Calculating score from Root Mean Log Squared Error\n","def rmlse(y_test, y_pred):\n","    error = np.square(np.log10(y_pred +1) - np.log10(y_test +1)).mean() ** 0.5\n","    score = 1 - error\n","    return score\n","\n","# Printing the score\n","print(\"\\n----------------------------\\nRMLSE Score = \", rmlse(y_test, y_Pred))\n","\n","#9 Comparing Actual and Predicted Salaries for he test set\n","print(\"\\nActual vs Predicted Scores \\n------------------------------\\n\")\n","error_df = pd.DataFrame({\"Actual\" : y_test,\n","                         \"Predicted\" : y_Pred,\n","                         \"Abs. Error\" : np.abs(y_test - y_Pred)})\n","\n","error_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9rIEtCL6Iy6W"},"source":["# Feature Importance"]},{"cell_type":"code","metadata":{"id":"QgJH4WsZIy6W"},"source":["feat_names = [col for col in dataset.columns if col not in ('Score')]\n","\n","pd.Series(regressor.feature_importances_, \\\n","          index=feat_names).sort_values(ascending=True).plot(kind='barh', figsize=(16,9));\n","\n","plt.title('Feature Importance Random Forest Regressor');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXU51VvOIy6X"},"source":["# Actual vs. Predicted "]},{"cell_type":"code","metadata":{"id":"zcokHiyFIy6X"},"source":["#Plotting Actual observation vs Predictions\n","plt.figure(figsize=(16, 9));\n","plt.scatter(y_test, y_Pred, s = 70)\n","plt.xlabel('Actual');\n","plt.ylabel('Predicted');\n","plt.grid();\n","plt.show();"],"execution_count":null,"outputs":[]}]}