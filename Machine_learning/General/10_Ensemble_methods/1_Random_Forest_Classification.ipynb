{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_Random_Forest.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMOOATMXPMzVPaw11TLWbH6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **What is Random Forest?**"],"metadata":{"id":"gK_UalSbp_18"}},{"cell_type":"markdown","source":["The forest is said to robust when there are a lot of trees in the forest. Random Forest is an ensemble technique that is a tree-based algorithm. The process of fitting no decision trees on different subsample and then taking out the average to increase the performance of the model is called “Random Forest”. Suppose we have to go on a vacation to someplace. Before going to the destination we vote for the place where we want to go. Once we have voted for the destination then we choose hotels, etc. And then come back with the final choice of hotel as well. The whole process of getting the vote for the place to the hotel is nothing but a Random Forest Algorithm. This is the way the algorithm works and the reason it is preferred over all other algorithms because of its ability to give high accuracy and to prevent overfitting by making use of more trees. There are several different hyperparameters like no trees, depth of trees, jobs, etc in this algorithm. Check here the Sci-kit documentation for the same. "],"metadata":{"id":"KJai_vSCqD56"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels scikit-image --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rfcl = RandomForestClassifier()"],"outputs":[],"metadata":{"id":"ALmZN_AEnOo3"}},{"cell_type":"markdown","source":["# **Build a Random Forest Classification Model**"],"metadata":{"id":"SlGOGGYbqL7E"}},{"cell_type":"markdown","source":["## **Import the required Libraries**"],"metadata":{"id":"l-j3zjAmqT5e"}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score,classification_report\n"],"outputs":[],"metadata":{"id":"c3nMAhWpvbT5"}},{"cell_type":"markdown","source":["##**Read the data**"],"metadata":{"id":"t3lgLn_2vp8U"}},{"cell_type":"code","execution_count":null,"source":["data = pd.read_csv('pimadiabetes.csv')"],"outputs":[],"metadata":{"id":"EZrsxWtkvy_W"}},{"cell_type":"markdown","source":["We will check what is there in the data and its shape. Refer to the below code for the same."],"metadata":{"id":"LgFQDonVwCr2"}},{"cell_type":"code","execution_count":null,"source":["data.head()"],"outputs":[],"metadata":{"id":"KzRQC5APwDaU"}},{"cell_type":"code","execution_count":null,"source":["data.shape"],"outputs":[],"metadata":{"id":"5p7jgAttwFYT"}},{"cell_type":"markdown","source":["## **Define Dependent and Independent Variables**"],"metadata":{"id":"G5NqSQHZwJx5"}},{"cell_type":"markdown","source":["\n","Now we will define the dependent and independent features X and y respectively. We will then divide the dataset into training and testing sets. Use the below code for the same"],"metadata":{"id":"XhPbEiYawPC1"}},{"cell_type":"code","execution_count":null,"source":["X = data.drop('Outcome',axis = 1)\n","y= data['Outcome']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n","print(X_train.shape)\n","print(X_test.shape)"],"outputs":[],"metadata":{"id":"Ne2CotftwOXO"}},{"cell_type":"markdown","source":["There are 514 rows in the training set and 254 rows in the testing set. Now we will fit the training data on both the model built by random forest and xgboost using default parameters. Then we will compute prediction over the testing data by both the models."],"metadata":{"id":"5QKfqwXCwalc"}},{"cell_type":"markdown","source":["## **Fit the Random Forest Classifier**"],"metadata":{"id":"a-zOxAdjwciB"}},{"cell_type":"code","execution_count":null,"source":["rfcl.fit(X_train,y_train)"],"outputs":[],"metadata":{"id":"ywvR4Be3wg-S"}},{"cell_type":"markdown","source":["## **Predict the Y values on Test Data**"],"metadata":{"id":"AMRUCV46wknO"}},{"cell_type":"code","execution_count":null,"source":["y_rfcl = rfcl.predict(X_test)"],"outputs":[],"metadata":{"id":"O5dXlm9Vwr1N"}},{"cell_type":"markdown","source":["## **Evaluation of the Model**"],"metadata":{"id":"I7PMrXkFwvA3"}},{"cell_type":"markdown","source":["We have stored the prediction on testing data for both the models in y_rfcl and Now we will evaluate the model performance to check how much the model is able to generalize. We will make use of evaluation metrics like accuracy score and classification report from sklearn. "],"metadata":{"id":"viE3GxGwwzFh"}},{"cell_type":"code","execution_count":null,"source":["print(\"Random Forest Accuracy: \", accuracy_score(y_rfcl,y_test))"],"outputs":[],"metadata":{"id":"IEknUShIwyQu"}},{"cell_type":"markdown","source":["We implemented a classification model for the Pima Indian Diabetes data set using Random Forest algorithm. We did not even normalize the data and directly fed it to the model still we were able to get 80%. If we work more on data and feature engineering then this accuracy can be improved further.\n","\n","The algorithms work efficiently even if we have missing values in the dateset and prevent the model from getting over fitted and easy to implement.  "],"metadata":{"id":"nayK4ii8w-FK"}},{"cell_type":"markdown","source":["# **Related Articles --**\n","\n",">* [Random Forest V/s XG Boost](https://analyticsindiamag.com/random-forest-vs-xgboost-comparing-tree-based-algorithms-with-codes/)\n","> * [Basics of Ensemble Learning](https://analyticsindiamag.com/basics-of-ensemble-learning-in-classification-techniques-explained/) \n","> * [Bagging V/S Boosting](https://analyticsindiamag.com/guide-to-ensemble-methods-bagging-vs-boosting/)\n","> * [Guide to Ensemble Learning](https://analyticsindiamag.com/a-hands-on-guide-to-hybrid-ensemble-learning-models-with-python-code/)\n","> * [Ensemble Methods](https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting/)"],"metadata":{"id":"80QsgDqknaj-"}}]}