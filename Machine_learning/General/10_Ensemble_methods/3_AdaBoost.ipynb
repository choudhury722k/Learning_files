{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"3_AdaBoost.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOC1iK7wYIPMA05MuwVmwmH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **What is AdaBoost?**"],"metadata":{"id":"yUWJuta8RKNm"}},{"cell_type":"markdown","source":["There have been many boosting algorithms that popped up recently, some of the popular ones being XGBoost, Gradient Boosting, LPBoost, TotalBoost, BrownBoost, LogitBoost etc.AdaBoost was the first successful boosting algorithm; all modern boosting algorithms build upon the success of boosting theory employed by AdaBoost. The premise of this theory is simple; combine multiple weak classifiers to create one strong classifier. This boosting approach can be used to boost the performance of any machine learning algorithm. But itâ€™s best paired with weak learners, which are models that perform just slightly better than a random guess on classification problems. The most commonly used weak learners are decision trees with a depth of one, also known as decision stumps"],"metadata":{"id":"FOp3Dc1HVDE9"}},{"cell_type":"markdown","source":["To know more about this algorithm, please refer this article [Introduction to AdaBoost](https://analyticsindiamag.com/introduction-to-boosting-implementing-adaboost-in-python/) and for the implementation of this algorithm from scratch please check [Demystifying AdaBoost](https://analyticsindiamag.com/demystifying-adaboost-the-origin-of-boosting/)."],"metadata":{"id":"BjAO_FYkVVH_"}},{"cell_type":"markdown","source":["## **Implementing Adaptive Boosting: AdaBoost in Python**"],"metadata":{"id":"8nO73yg-VoXp"}},{"cell_type":"markdown","source":["### **Importing the Required Libraries**"],"metadata":{"id":"sHc0a7unVtod"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels scikit-image --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import time\n","from sklearn import datasets\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split"],"outputs":[],"metadata":{"id":"qksJx90uQg1v"}},{"cell_type":"markdown","source":["### **Load the Dataset**"],"metadata":{"id":"-HR8rwqIVyqO"}},{"cell_type":"markdown","source":["Here we are using breast Cancer Dataset, an-inbuilt dataset in Sklearn"],"metadata":{"id":"nLXLWk72Qwl7"}},{"cell_type":"code","execution_count":null,"source":["X, y = datasets.load_breast_cancer(return_X_y=True)\n","#changing class 0 to -1\n","y[y == 0] = -1"],"outputs":[],"metadata":{"id":"nAWeZg59V-jz"}},{"cell_type":"markdown","source":["### **Split the dataset into Training and Testing**"],"metadata":{"id":"P5M1kl4iWMVX"}},{"cell_type":"code","execution_count":null,"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"],"outputs":[],"metadata":{"id":"Yx2aFnJhWQe4"}},{"cell_type":"markdown","source":["### **Fit the Adaboost Classifier and Evaluate the model**"],"metadata":{"id":"nRMgoWTaWVxo"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.ensemble import AdaBoostClassifier\n","import time\n","start = time.perf_counter()\n","sk_classifier = AdaBoostClassifier(n_estimators=5, random_state=42)\n","sk_classifier.fit(X_train, y_train)\n","sk_y_pred = sk_classifier.predict(X_test)\n","end = time.perf_counter()\n","\n","sk_accuracy = accuracy_score(y_test, sk_y_pred)\n","print (\"Accuracy:\", sk_accuracy)\n","print(f'Finished in {round(end-start, 2)} second(s)') "],"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9736842105263158\n","Finished in 0.02 second(s)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18TxE1R_Wauz","executionInfo":{"status":"ok","timestamp":1620903092966,"user_tz":-330,"elapsed":1146,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"68603c39-b90e-401f-dcf4-ea71edf7cc6a"}},{"cell_type":"markdown","source":["# **Related Articles --**\n","\n","\n","> * [Demystifying AdaBoost](https://analyticsindiamag.com/demystifying-adaboost-the-origin-of-boosting/)\n","> * [Introduction to AdaBoost](https://analyticsindiamag.com/introduction-to-boosting-implementing-adaboost-in-python/)\n","> * [Understand XG Boost](https://analyticsindiamag.com/xgboost-internal-working-to-make-decision-trees-and-deduce-predictions/)\n","> * [XGBoost v/s LightGBM](https://analyticsindiamag.com/comparing-the-gradient-boosting-decision-tree-packages-xgboost-vs-lightgbm/)\n","> * [Random Forest V/s XG Boost](https://analyticsindiamag.com/random-forest-vs-xgboost-comparing-tree-based-algorithms-with-codes/)\n","> * [Basics of Ensemble Learning](https://analyticsindiamag.com/basics-of-ensemble-learning-in-classification-techniques-explained/) \n","> * [Bagging V/S Boosting](https://analyticsindiamag.com/guide-to-ensemble-methods-bagging-vs-boosting/)\n","> * [Guide to Ensemble Learning](https://analyticsindiamag.com/a-hands-on-guide-to-hybrid-ensemble-learning-models-with-python-code/)\n","> * [Ensemble Methods](https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting/)"],"metadata":{"id":"80QsgDqknaj-"}}]}