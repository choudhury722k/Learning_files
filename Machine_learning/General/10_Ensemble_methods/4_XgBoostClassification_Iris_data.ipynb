{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_XgBoostClassification_Iris_data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UrKJ6c1iHQVx"},"source":["# In this practice session, we will learn to code Xtreme Gradient Boosting(XgBoost) Classification.\n","\n","# We will perform the following steps to build a simple classifier using the popular Iris dataset.\n","\n"," \n"," \n","  - **Data Preprocessing**\n","\n","    - Importing the libraries.\n","    - Importing dataset (Dataset Link https://archive.ics.uci.edu/ml/datasets/iris).\n","    - Dealing with the categorical variable.\n","    - Classifying dependent and independent variables.\n","    - Splitting the data into a training set and test set.\n","    - Feature scaling.\n"," \n","\n","  -  **XgBoost Classification**\n","\n","    - Create a XgBoost classifier.\n","    - Feed the training data to the classifier.\n","    - Predicting the species for the test set.\n","    - Using the confusion matrix to find accuracy."]},{"cell_type":"markdown","metadata":{"id":"RGQ2tr41HQV6"},"source":["# Load the Dependencies"]},{"cell_type":"code","metadata":{"id":"04Eyb2y1HQV8"},"source":["import ipywidgets as widgets\n","from IPython.display import display\n","\n","style = {'description_width': 'initial'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BZ-jLOunczC"},"source":["#1 Importing essential libraries\n","import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","import seaborn as sns\n","iris = load_iris() \n","data = iris.data \n","target = iris.target \n","names = iris.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"njmxLtcAHQV9"},"source":["# Load the Dataset"]},{"cell_type":"code","metadata":{"id":"WKtERLr9n09B"},"source":["#file_name = 'iris.data'\n","\n","dataset = pd.DataFrame(data, columns=['sepal length', 'sepal width', 'petal length', \"petal width\"])\n","dataset['Species']=target\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQECBNSXHQV9"},"source":["print(f\"Dataset has {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxeImgUWHQV_"},"source":["#Plotting the relation between salary and experience\n","wig_col = widgets.Dropdown(\n","                options=[col for col in dataset.columns.tolist() if col.startswith(('sepal', 'petal'))],\n","                description='Choose a Column to Plot vs. Attributes',\n","                disabled=False,\n","                layout=widgets.Layout(width='40%', height='40px'),\n","                style=style)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wuNlYe31HQV_"},"source":["# Plot Variables"]},{"cell_type":"code","metadata":{"id":"YTr5kVFKHQWA"},"source":["display(wig_col)\n","\n","sns.catplot(x=\"Species\", y=wig_col.value, kind=\"boxen\", data=dataset, height=8.27, aspect=11.7/8.27);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXbO2NYdHQWB"},"source":["g = sns.catplot(x=\"Species\", y=wig_col.value, kind=\"violin\", inner=None, data=dataset, height=8.27, aspect=11.7/8.27)\n","sns.swarmplot(x=\"Species\", y=wig_col.value, color=\"k\", size=3, data=dataset, ax=g.ax);\n","\n","display(wig_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5WuUWRFn4j8"},"source":["#3 classify dependent and independent variables\n","X = dataset.iloc[:,:-1].values  #independent variable YearsofExperience\n","y = dataset.iloc[:,-1].values  #dependent variable salary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bZ82kVbn7Ga"},"source":["print(\"\\nIdependent Variable (Sepal and Petal Attributes):\\n\\n\", X[:5])\n","print(\"\\nDependent Variable (Species):\\n\\n\", y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23HxDyZWHQWE"},"source":["# Encode Classes"]},{"cell_type":"code","metadata":{"id":"1dzQpXX6HQWE"},"source":["from sklearn.preprocessing import LabelEncoder\n","labelencoder = LabelEncoder()\n","dataset['Species'] = labelencoder.fit_transform(dataset['Species'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNmX9n-SHQWF"},"source":["dataset['Species'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ou43cDl7HQWF"},"source":["# Create Train and Test Sets"]},{"cell_type":"code","metadata":{"id":"BFMIwI5Zn9MX"},"source":["#4 Creating training set and testing set\n","from sklearn.model_selection import train_test_split\n","test_size = widgets.FloatSlider(min=0.01, max=0.6, value=0.2, description=\"Test Size :\", tooltips=['Usually 20-30%'])\n","display(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DtsZbiAVHQWG"},"source":["#Divide the dataset into Train and Test sets\n","X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=test_size.value, random_state = 0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcY06YGDn_dz"},"source":["print(\"Training Set :\\n----------------\\n\")\n","print(\"X = \\n\", X_train[:5])\n","print(\"y = \\n\", y_train[:5])\n","\n","print(\"\\n\\nTest Set :\\n----------------\\n\")\n","print(\"X = \\n\",X_test[:5])\n","print(\"y = \\n\", y_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWFGpF0IHQWG"},"source":["print(f\"Shape of Training set is {X_train.shape}\")\n","print(f\"Shape of Testing set is {X_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4fw-7nu1HQWG"},"source":["# Normalise Features\n","\n","As the Features are not in the range of 0-1, Let's normalize the features using Standard Scaler(Z-score) normalization and Label Encode the Class String Names."]},{"cell_type":"code","metadata":{"id":"BOkY5dkFHQWH"},"source":["#Feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test) \n","\n","print(\"\\n-------------------------\\nDataset after Scaling:\\n-------------------------\\n\", )\n","\n","print(\"\\nX_train :\\n\", X_train[:5])\n","print(\"-------------------------\")\n","print(\"\\nX_test :\\n\", X_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQ3aoiFhHQWH"},"source":["# XgBoost Classifier"]},{"cell_type":"code","metadata":{"id":"M6_1-rvbHQWH"},"source":["# import XgBoost library\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","\n","# configure params for the model.\n","learning_wig = widgets.ToggleButtons(options=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n","                                    description='Learning Rate :',\n","                                    disabled=False,\n","                                    style=style)\n","\n","display(learning_wig)\n","\n","max_depth_wig = widgets.Dropdown(options=[10, 20, 30, 50],\n","                            description='The maximum depth of the Tree :',\n","                            style=style)\n","\n","display(max_depth_wig)\n","\n","min_split_wig = widgets.Dropdown(options=[100, 200, 300, 500],\n","                            description='Minimum number of splits :',\n","                            style=style)\n","\n","display(min_split_wig)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clAmjKJiHQWH"},"source":["# Predict and Evaluate the Model "]},{"cell_type":"code","metadata":{"id":"8XbYFyk8oCrH"},"source":["classifier = GradientBoostingClassifier(learning_rate=learning_wig.value,\n","                                          max_depth=max_depth_wig.value,\n","                                          min_samples_split=min_split_wig.value\n","                                       )\n","\n","#Feed the training data to the classifier\n","classifier.fit(X_train,y_train)\n","\n","#Predicting the species for test set\n","y_pred = classifier.predict(X_test)\n","\n","print(\"\\n---------------------------\\n\")\n","print(\"Predicted Values for Test Set :\\n\",y_pred)\n","print(\"\\n---------------------------\\n\")\n","print(\"Actual Values for Test Set :\\n\",y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAwiJVWuoHEX"},"source":["#8 Claculating the Accuracy of the predictions\n","from sklearn import metrics\n","print(\"Prediction Accuracy = \", metrics.accuracy_score(y_test, y_pred))\n","\n","#9 Comparing Actual and Predicted Salaries for he test set\n","print(\"\\nActual vs Predicted Salaries \\n------------------------------\\n\")\n","error_df = pd.DataFrame({\"Actual\" : y_test,\n","                         \"Predicted\" : y_pred})\n","\n","error_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4rcKwWbHQWJ"},"source":["# Actual vs. Predicted "]},{"cell_type":"code","metadata":{"id":"f3DTU_g1HQWJ"},"source":["#Using confusion matrix to find the accuracy\n","from sklearn.metrics import confusion_matrix, classification_report\n","cm = confusion_matrix(y_test,y_pred)\n","\n","accuracy = cm.diagonal().sum()/cm.sum()\n","\n","print(\"\\n---------------------------\\n\")\n","print(\"Accuracy of Predictions = \",accuracy)\n","\n","print(\"\\n---------------------------\\n\")\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[]}]}