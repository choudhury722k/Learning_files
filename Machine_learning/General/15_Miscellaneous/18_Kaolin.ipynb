{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Kaolin.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPGDK44W/DfstsQ98NIxZ7V"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["# Kaolin"],"metadata":{"id":"6gT-Gs460WAz"}},{"cell_type":"markdown","source":["Kaolin, a one-stop solution for all of the 3D deep learning needs, was published, intending faster research and easy deployment. Kaolin supports 3D deep learning processes from data preprocessing to model building to model deployment with more efficiency. \n","\n","Though Kaolin was initially released with a great collection of pre-trained neural architectures, the recent optimized release expelled out all those pre-trained models to keep the library simple and focused towards future research. However, a separate repository with the pre-trained models collection could be expected to be officially released soon."],"metadata":{"id":"PktZLDvh0YLX"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/nvidias-kaolin-3d-deep-learning-library/) article."],"metadata":{"id":"Th1yTd6M2IW0"}},{"cell_type":"markdown","source":["# Code Implementation"],"metadata":{"id":"cE0sSb8K0d0l"}},{"cell_type":"markdown","source":["Install dependencies using the following command.\n","\n"],"metadata":{"id":"VR4BEGzO0gSJ"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels tensorflow keras --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install torch scipy cython Pillow usd-core --user -q"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhpphjrAvD9i","executionInfo":{"status":"ok","timestamp":1624278719199,"user_tz":-330,"elapsed":10703,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"5d35d25a-65f7-42cb-ddc9-fd7b6ba6fded"}},{"cell_type":"markdown","source":["Install Kaolin and the third-party dependencies from the official source code.\n"],"metadata":{"id":"Ddn_hguw0iwW"}},{"cell_type":"code","execution_count":null,"source":["!git clone --recursive https://github.com/NVIDIAGameWorks/kaolin "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CEHVYJiN0hI4","executionInfo":{"status":"ok","timestamp":1624278729021,"user_tz":-330,"elapsed":9835,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"62db24b5-0c5c-4833-d6e1-a024690b73e2"}},{"cell_type":"markdown","source":["Create the necessary environment using the following command.\n"],"metadata":{"id":"Hr8mfr2i0p19"}},{"cell_type":"code","execution_count":null,"source":["!python kaolin/setup.py develop"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAV0wraX0ksP","executionInfo":{"status":"ok","timestamp":1624278976043,"user_tz":-330,"elapsed":236415,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"684f2ff8-13f4-4509-aa11-320d1a20a74c"}},{"cell_type":"markdown","source":["Test correct version installation using the following command.\n","\n"],"metadata":{"id":"YD5bXeyU0s7n"}},{"cell_type":"code","execution_count":null,"source":["# test installation\n","!python -c \"import kaolin; print(kaolin.__version__)\" "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4kTnLdk0nrw","executionInfo":{"status":"ok","timestamp":1624278979646,"user_tz":-330,"elapsed":3614,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"5619a636-e37e-4923-d453-08a87096cb70"}},{"cell_type":"markdown","source":["Install the other requirements and perform a complete test of installation.\n","\n"],"metadata":{"id":"71Mb0mpW01Bq"}},{"cell_type":"code","execution_count":null,"source":["!pip install -r kaolin/tools/ci_requirements.txt\n","#!pytest tests/python/ "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lA4ILO4D0y5I","executionInfo":{"status":"ok","timestamp":1624279000671,"user_tz":-330,"elapsed":21038,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"53c7abb0-d02d-4836-b6f7-eb3012843921"}},{"cell_type":"markdown","source":["Letâ€™s perform an Image Rendering task. Most of this code implementation references [this](https://github.com/NVIDIAGameWorks/kaolin/blob/master/examples/tutorial/dibr_tutorial.ipynb) official notebook.\n","\n","Import necessary libraries and modules."],"metadata":{"id":"qkYGdGE405Lh"}},{"cell_type":"markdown","source":["Import necessary libraries and modules."],"metadata":{"id":"OMyP0ule0_gG"}},{"cell_type":"code","execution_count":null,"source":["import json\n","import os\n","import glob\n","import time\n","from PIL import Image\n","import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import kaolin as kal "],"outputs":[],"metadata":{"id":"6rKXYZ4h02hB","executionInfo":{"status":"ok","timestamp":1624279260797,"user_tz":-330,"elapsed":1295,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["# path to the rendered image (using the data synthesizer)\n","rendered_path = \"/kaolin/examples/samples/rendered_clock/\"\n","# path to the output logs (readable with the training visualizer in the omniverse app)\n","logs_path = './logs/'\n","# We initialize the timelapse that will store USD for the visualization apps\n","timelapse = kal.visualize.Timelapse(logs_path) "],"outputs":[],"metadata":{"id":"O9CPhUj11B2_","executionInfo":{"status":"ok","timestamp":1624279263698,"user_tz":-330,"elapsed":694,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Set hyperparameters necessary for image rendering.\n"],"metadata":{"id":"Pn-L2NdI1Igh"}},{"cell_type":"code","execution_count":null,"source":["# Hyperparameters\n","num_epoch = 40\n","batch_size = 2\n","laplacian_weight = 0.1\n","flat_weight = 0.001\n","image_weight = 0.1\n","mask_weight = 1.\n","lr = 5e-2\n","scheduler_step_size = 15\n","scheduler_gamma = 0.5\n","texture_res = 400\n","# select camera angle for best visualization\n","test_batch_ids = [2, 5, 10]\n","test_batch_size = len(test_batch_ids) "],"outputs":[],"metadata":{"id":"fItmLv-P1Eyc","executionInfo":{"status":"ok","timestamp":1624279265872,"user_tz":-330,"elapsed":526,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Sample data for image rendering is available with source code in zipped format. Unzip it using the following command.\n"],"metadata":{"id":"XKrvbdwf1Nnj"}},{"cell_type":"code","execution_count":null,"source":["!unzip kaolin/examples/samples/rendered_clock.zip -d kaolin/examples/samples/"],"outputs":[],"metadata":{"id":"sSMu2MGD1Mln","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624279459883,"user_tz":-330,"elapsed":20048,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"bef1713a-d855-48e3-b5b9-8c782f73ae09"}},{"cell_type":"markdown","source":["Check for proper extraction.\n"],"metadata":{"id":"-5t10jpA1XYK"}},{"cell_type":"code","execution_count":null,"source":["!ls kaolin/examples/samples/ -p"],"outputs":[],"metadata":{"id":"Bjg-LCzn1Wte","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624279467155,"user_tz":-330,"elapsed":395,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"ae6654c6-0b04-4d2a-9cb3-c5122277f122"}},{"cell_type":"markdown","source":["Load the data using the DataLoader method.\n"],"metadata":{"id":"wjBHBySq1a32"}},{"cell_type":"code","execution_count":null,"source":["os.path.join(rendered_path,'*_rgb.png')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Jjrheix6mjID","executionInfo":{"status":"ok","timestamp":1624279649410,"user_tz":-330,"elapsed":651,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"348568e6-5a91-4a06-a678-d855354a70d4"}},{"cell_type":"code","execution_count":null,"source":["num_views = len(glob.glob(os.path.join(rendered_path,'*_rgb.png')))\n","print(num_views)\n","train_data = []\n","for i in range(num_views):\n","    data = kal.io.render.import_synthetic_view(\n","        rendered_path, i, rgb=True, semantic=True)\n","    train_data.append(data)\n","dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                        shuffle=True, pin_memory=True) "],"outputs":[],"metadata":{"id":"s2aUw_XA1Q21","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1624279484029,"user_tz":-330,"elapsed":755,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"fea1062d-6e8e-416e-b5c2-319ee86129b6"}},{"cell_type":"markdown","source":["Load the in-built sphere template for training.\n"],"metadata":{"id":"vcPNoOIm1iDm"}},{"cell_type":"code","execution_count":null,"source":["mesh = kal.io.obj.import_mesh('kaolin/examples/samples/sphere.obj', with_materials=True)\n","# the sphere is usually too small (this is fine-tuned for the clock)\n","vertices = mesh.vertices.cuda().unsqueeze(0) * 75\n","vertices.requires_grad = True\n","faces = mesh.faces.cuda()\n","uvs = mesh.uvs.cuda().unsqueeze(0)\n","face_uvs_idx = mesh.face_uvs_idx.cuda()\n","face_uvs = kal.ops.mesh.index_vertices_by_faces(uvs, face_uvs_idx).detach()\n","face_uvs.requires_grad = False\n","texture_map = torch.ones((1, 3, texture_res, texture_res), dtype=torch.float, device='cuda',\n","                        requires_grad=True)\n","# The topology of the mesh and the uvs are constant\n","# so we can initialize them on the first iteration only\n","timelapse.add_mesh_batch(\n","    iteration=0,\n","    category='optimized_mesh',\n","    faces_list=[mesh.faces.cpu()],\n","    uvs_list=[mesh.uvs.cpu()],\n","    face_uvs_idx_list=[mesh.face_uvs_idx.cpu()],\n",") "],"outputs":[],"metadata":{"id":"9F0GOL7D1mZA"}},{"cell_type":"markdown","source":["Set up the losses and regularizer for training and evaluation.\n"],"metadata":{"id":"qm5WwCON1pZB"}},{"cell_type":"code","execution_count":null,"source":["## Separate vertices center as a learnable parameter\n","vertices_init = vertices.detach()\n","vertices_init.requires_grad = False\n","# This is the center of the optimized mesh, separating it as a learnable parameter helps the optimization. \n","vertice_shift = torch.zeros((3,), dtype=torch.float, device='cuda',\n","                            requires_grad=True)\n","def recenter_vertices(vertices, vertice_shift):\n","    \"\"\"Recenter vertices on vertice_shift for better optimization\"\"\"\n","    vertices_min = vertices.min(dim=1, keepdim=True)[0]\n","    vertices_max = vertices.max(dim=1, keepdim=True)[0]\n","    vertices_mid = (vertices_min + vertices_max) / 2\n","    vertices = vertices - vertices_mid + vertice_shift\n","    return vertices\n","nb_faces = faces.shape[0]\n","nb_vertices = vertices_init.shape[1]\n","face_size = 3\n","## Set up auxiliary connectivity matrix of edges to faces indexes for the flat loss\n","edges = torch.cat([faces[:,i:i+2] for i in range(face_size - 1)] +\n","                  [faces[:,[-1,0]]], dim=0)\n","edges = torch.sort(edges, dim=1)[0]\n","face_ids = torch.arange(nb_faces, device='cuda', dtype=torch.long).repeat(face_size)\n","edges, edges_ids = torch.unique(edges, sorted=True, return_inverse=True, dim=0)\n","nb_edges = edges.shape[0]\n","# edge to faces\n","sorted_edges_ids, order_edges_ids = torch.sort(edges_ids)\n","sorted_faces_ids = face_ids[order_edges_ids]\n","# indices of first occurences of each key\n","idx_first = torch.where(\n","    torch.nn.functional.pad(sorted_edges_ids[1:] != sorted_edges_ids[:-1],\n","                            (1,0), value=1))[0]\n","nb_faces_per_edge = idx_first[1:] - idx_first[:-1]\n","# compute sub_idx (2nd axis indices to store the faces)\n","offsets = torch.zeros(sorted_edges_ids.shape[0], device='cuda', dtype=torch.long)\n","offsets[idx_first[1:]] = nb_faces_per_edge\n","sub_idx = (torch.arange(sorted_edges_ids.shape[0], device='cuda', dtype=torch.long) -\n","          torch.cumsum(offsets, dim=0))\n","nb_faces_per_edge = torch.cat([nb_faces_per_edge,\n","                              sorted_edges_ids.shape[0] - idx_first[-1:]],\n","                              dim=0)\n","max_sub_idx = 2\n","edge2faces = torch.zeros((nb_edges, max_sub_idx), device='cuda', dtype=torch.long)\n","edge2faces[sorted_edges_ids, sub_idx] = sorted_faces_ids\n","## Set up auxiliary laplacian matrix for the laplacian loss\n","vertices_laplacian_matrix = kal.ops.mesh.uniform_laplacian(\n","    nb_vertices, faces)"],"outputs":[],"metadata":{"id":"SlaipxVh1oxA"}},{"cell_type":"markdown","source":["Set up Adam optimizer and learning rate scheduler.\n"],"metadata":{"id":"z3zeyrA11vo9"}},{"cell_type":"code","execution_count":null,"source":["optim  = torch.optim.Adam(params=[vertices, texture_map, vertice_shift],\n","                          lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=scheduler_step_size,\n","                                            gamma=scheduler_gamma) "],"outputs":[],"metadata":{"id":"NnZtWGP61xC-"}},{"cell_type":"markdown","source":["Perform training with above set up for 40 epochs.\n"],"metadata":{"id":"ceIE_TBq1ywp"}},{"cell_type":"code","execution_count":null,"source":["for epoch in range(num_epoch):\n","    for idx, data in enumerate(dataloader):\n","        optim.zero_grad()\n","        gt_image = data['rgb'].cuda()\n","        gt_mask = data['semantic'].cuda()\n","        cam_transform = data['metadata']['cam_transform'].cuda()\n","        cam_proj = data['metadata']['cam_proj'].cuda()\n","        ### Prepare mesh data with projection regarding to camera ###\n","        vertices_batch = recenter_vertices(vertices, vertice_shift)\n","        face_vertices_camera, face_vertices_image, face_normals = \\\n","            kal.render.mesh.prepare_vertices(\n","                vertices_batch.repeat(batch_size, 1, 1),\n","                faces, cam_proj, camera_transform=cam_transform\n","            )\n","        ### Perform Rasterization ###\n","        # Construct attributes that DIB-R rasterizer will interpolate.\n","        # the first is the UVS associated to each face\n","        # the second will make a hard segmentation mask\n","        face_attributes = [\n","            face_uvs.repeat(batch_size, 1, 1, 1),\n","            torch.ones((batch_size, nb_faces, 3, 1), device='cuda')\n","        ]\n","        image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n","            gt_image.shape[1], gt_image.shape[2], face_vertices_camera[:, :, :, -1],\n","            face_vertices_image, face_attributes, face_normals[:, :, -1])\n","        # image_features is a tuple in composed of the interpolated attributes of face_attributes\n","        texture_coords, mask = image_features\n","        image = kal.render.mesh.texture_mapping(texture_coords,\n","                                                texture_map.repeat(batch_size, 1, 1, 1), \n","                                                mode='bilinear')\n","        image = torch.clamp(image * mask, 0., 1.)\n","        ### Compute Losses ###\n","        image_loss = torch.mean(torch.abs(image - gt_image))\n","        mask_loss = kal.metrics.render.mask_iou(soft_mask,\n","                                                gt_mask.squeeze(-1))\n","        # laplacian loss\n","        vertices_mov = vertices - vertices_init\n","        vertices_mov_laplacian = torch.matmul(vertices_laplacian_matrix, vertices_mov)\n","        laplacian_loss = torch.mean(vertices_mov_laplacian ** 2) * nb_vertices * 3\n","        # flat loss\n","        mesh_normals_e1 = face_normals[:, edge2faces[:, 0]]\n","        mesh_normals_e2 = face_normals[:, edge2faces[:, 1]]\n","        faces_cos = torch.sum(mesh_normals_e1 * mesh_normals_e2, dim=2)\n","        flat_loss = torch.mean((faces_cos - 1) ** 2) * edge2faces.shape[0]\n","        loss = (\n","            image_loss * image_weight +\n","            mask_loss * mask_weight +\n","            laplacian_loss * laplacian_weight +\n","            flat_loss * flat_weight\n","        )\n","        ### Update the mesh ###\n","        loss.backward()\n","        optim.step()\n","    scheduler.step()\n","    print(f\"Epoch {epoch} - loss: {float(loss)}\")\n","    ### Write 3D Checkpoints ###\n","    pbr_material = [\n","        {'rgb': kal.io.materials.PBRMaterial(diffuse_texture=torch.clamp(texture_map[0], 0., 1.))}\n","    ]\n","    vertices_batch = recenter_vertices(vertices, vertice_shift)\n","    # We are now adding a new state of the mesh to the timelapse\n","    # we only modify the texture and the vertices position\n","    timelapse.add_mesh_batch(\n","        iteration=epoch,\n","        category='optimized_mesh',\n","        vertices_list=[vertices_batch[0]],\n","        materials_list=pbr_material\n","    ) "],"outputs":[],"metadata":{"id":"uEKl_i2b11Gt"}},{"cell_type":"markdown","source":["Visualize the rendered image.\n"],"metadata":{"id":"x8xz4d-214HA"}},{"cell_type":"code","execution_count":null,"source":["with torch.no_grad():\n","    # This is similar to a training iteration (without the loss part)\n","    data_batch = [train_data[idx] for idx in test_batch_ids]\n","    cam_transform = torch.stack([data['metadata']['cam_transform'] for data in data_batch], dim=0).cuda()\n","    cam_proj = torch.stack([data['metadata']['cam_proj'] for data in data_batch], dim=0).cuda()\n","    vertices_batch = recenter_vertices(vertices, vertice_shift)\n","    face_vertices_camera, face_vertices_image, face_normals = \\\n","        kal.render.mesh.prepare_vertices(\n","            vertices_batch.repeat(test_batch_size, 1, 1),\n","            faces, cam_proj, camera_transform=cam_transform\n","        )\n","    face_attributes = [\n","        face_uvs.repeat(test_batch_size, 1, 1, 1),\n","        torch.ones((test_batch_size, nb_faces, 3, 1), device='cuda')\n","    ]\n","    image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n","        256, 256, face_vertices_camera[:, :, :, -1],\n","        face_vertices_image, face_attributes, face_normals[:, :, -1])\n","    texture_coords, mask = image_features\n","    image = kal.render.mesh.texture_mapping(texture_coords,\n","                                            texture_map.repeat(test_batch_size, 1, 1, 1), \n","                                            mode='bilinear')\n","    image = torch.clamp(image * mask, 0., 1.)\n","    ## Display the rendered images\n","    f, axarr = plt.subplots(1, test_batch_size, figsize=(7, 22))\n","    f.subplots_adjust(top=0.99, bottom=0.79, left=0., right=1.4)\n","    f.suptitle('DIB-R rendering', fontsize=30)\n","    for i in range(test_batch_size):\n","        axarr[i].imshow(image[i].cpu().detach())\n","## Display the texture\n","plt.figure(figsize=(10, 10))\n","plt.title('2D Texture Map', fontsize=30)\n","plt.imshow(torch.clamp(texture_map[0], 0., 1.).cpu().detach().permute(1, 2, 0)) "],"outputs":[],"metadata":{"id":"5MuMUom51gNL"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"Ae-vvUSv174e"}}]}