{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Botorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSj4uqGScU9lbjWxreceO8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Botorch**"],"metadata":{"id":"uMLyF-24ArhL"}},{"cell_type":"markdown","source":["BoTorch is a library built on top of PyTorch for Bayesian Optimization. It combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques."],"metadata":{"id":"b5zCu6FeAraj"}},{"cell_type":"markdown","source":["Here are the salient features of Botorch according to the Readme of it’s repository\n","\n","> * Provides a modular and easily extensible interface for composing Bayesian optimization primitives, including probabilistic models, acquisition functions, and optimizers.\n","> * Harnesses the power of PyTorch, including auto-differentiation, native support for highly parallelized modern hardware (e.g. GPUs) using device-agnostic code, and a dynamic computation graph.\n","> * Supports Monte Carlo-based acquisition functions via the reparameterization trick, making it straightforward to implement new ideas without imposing restrictive assumptions about the underlying model.\n","> * Enables seamless integration with deep and/or convolutional architectures in PyTorch.\n","> * Has first-class support for state-of-the art probabilistic models in GPyTorch, including support for multi-task Gaussian Processes (GPs) deep kernel learning, deep GPs, and approximate inference."],"metadata":{"id":"PgTzyJqyCKbm"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/guide-to-bayesian-optimization-using-botorch/) article."],"metadata":{"id":"BSKXzXuVCOKN"}},{"cell_type":"markdown","source":["# Example"],"metadata":{"id":"wkYFBGwTCVKC"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels tensorflow keras torch torchvision --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python pip install botorch --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import os\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dtype = torch.double\n","SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"],"outputs":[],"metadata":{"id":"2Fes7FtNLX_c"}},{"cell_type":"markdown","source":["!pip install botorch can be used to do a quick install of botorch.\n","\n","Let’s see how to optimize the following function with added constraint of  ∥x∥−3≤0\n","\n","x∈[0,1]6  \n","\n","Following is the implementation of enforcing constraints on the above hartman function."],"metadata":{"id":"IHZXEdf7CZmA"}},{"cell_type":"code","execution_count":null,"source":["from botorch.test_functions import Hartmann\n","\n","\n","neg_hartmann6 = Hartmann(negate=True)\n","\n","\n","def outcome_constraint(X):\n","    \"\"\"L1 constraint; feasible if less than or equal to zero.\"\"\"\n","    return X.sum(dim=-1) - 3\n","\n","def weighted_obj(X):\n","    \"\"\"Feasibility weighted objective; zero if not feasible.\"\"\"\n","    return neg_hartmann6(X) * (outcome_constraint(X) <= 0).type_as(X)"],"outputs":[],"metadata":{"id":"IBYA6910MEil"}},{"cell_type":"markdown","source":["Now we need to initialize the surrogate model. MultiOutput Gaussian Process is used for this as we have objective function and a constraint."],"metadata":{"id":"FoRqYq1qCj7e"}},{"cell_type":"code","execution_count":null,"source":["from botorch.models import FixedNoiseGP, ModelListGP\n","from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n","\n","NOISE_SE = 0.5\n","train_yvar = torch.tensor(NOISE_SE**2, device=device, dtype=dtype)\n","\n","\n","def generate_initial_data(n=10):\n","    # generate training data\n","    train_x = torch.rand(10, 6, device=device, dtype=dtype)\n","    exact_obj = neg_hartmann6(train_x).unsqueeze(-1)  # add output dimension\n","    exact_con = outcome_constraint(train_x).unsqueeze(-1)  # add output dimension\n","    train_obj = exact_obj + NOISE_SE * torch.randn_like(exact_obj)\n","    train_con = exact_con + NOISE_SE * torch.randn_like(exact_con)\n","    best_observed_value = weighted_obj(train_x).max().item()\n","    return train_x, train_obj, train_con, best_observed_value\n","    \n","    \n","def initialize_model(train_x, train_obj, train_con, state_dict=None):\n","    # define models for objective and constraint\n","    model_obj = FixedNoiseGP(train_x, train_obj, train_yvar.expand_as(train_obj)).to(train_x)\n","    model_con = FixedNoiseGP(train_x, train_con, train_yvar.expand_as(train_con)).to(train_x)\n","    # combine into a multi-output GP model\n","    model = ModelListGP(model_obj, model_con)\n","    mll = SumMarginalLogLikelihood(model.likelihood, model)\n","    # load state dict if it is passed\n","    if state_dict is not None:\n","        model.load_state_dict(state_dict)\n","    return mll, model"],"outputs":[],"metadata":{"id":"nGtgS0pkMLUw"}},{"cell_type":"code","execution_count":null,"source":["from botorch.acquisition.objective import ConstrainedMCObjective\n","\n","\n","def obj_callable(Z):\n","    return Z[..., 0]\n","\n","\n","def constraint_callable(Z):\n","    return Z[..., 1]\n","\n","\n","# define a feasibility-weighted objective for optimization\n","constrained_obj = ConstrainedMCObjective(\n","    objective=obj_callable,\n","    constraints=[constraint_callable],\n",")\n"],"outputs":[],"metadata":{"id":"UZ04uCDpPKof"}},{"cell_type":"code","execution_count":null,"source":["from botorch.optim import optimize_acqf\n","\n","\n","bounds = torch.tensor([[0.0] * 6, [1.0] * 6], device=device, dtype=dtype)\n","\n","BATCH_SIZE = 3 if not SMOKE_TEST else 2\n","NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n","RAW_SAMPLES = 512 if not SMOKE_TEST else 32\n","\n","#Now we need a function to optimize the acquisition function and give us next observation.\n","def optimize_acqf_and_get_observation(acq_func):\n","    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation.\"\"\"\n","    # optimize\n","    candidates, _ = optimize_acqf(\n","        acq_function=acq_func,\n","        bounds=bounds,\n","        q=BATCH_SIZE,\n","        num_restarts=NUM_RESTARTS,\n","        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n","        options={\"batch_limit\": 5, \"maxiter\": 200},\n","    )\n","    # observe new values \n","    new_x = candidates.detach()\n","    exact_obj = neg_hartmann6(new_x).unsqueeze(-1)  # add output dimension\n","    exact_con = outcome_constraint(new_x).unsqueeze(-1)  # add output dimension\n","    new_obj = exact_obj + NOISE_SE * torch.randn_like(exact_obj)\n","    new_con = exact_con + NOISE_SE * torch.randn_like(exact_con)\n","    return new_x, new_obj, new_con\n","\n","\n","def update_random_observations(best_random):\n","    \"\"\"Simulates a random policy by taking a the current list of best values observed randomly,\n","    drawing a new random point, observing its value, and updating the list.\n","    \"\"\"\n","    rand_x = torch.rand(BATCH_SIZE, 6)\n","    next_random_best = weighted_obj(rand_x).max().item()\n","    best_random.append(max(best_random[-1], next_random_best))       \n","    return best_random"],"outputs":[],"metadata":{"id":"abP0DCoZPnz9"}},{"cell_type":"markdown","source":["Now that all initialization steps are done we can run the bayesian optimization."],"metadata":{"id":"OXwbG4p_CsJU"}},{"cell_type":"code","execution_count":null,"source":["from botorch import fit_gpytorch_model\n","from botorch.acquisition.monte_carlo import qExpectedImprovement, qNoisyExpectedImprovement\n","from botorch.sampling.samplers import SobolQMCNormalSampler\n","from botorch.exceptions import BadInitialCandidatesWarning\n","\n","import time\n","import warnings\n","\n","\n","warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n","warnings.filterwarnings('ignore', category=RuntimeWarning)\n","\n","\n","N_TRIALS = 3 if not SMOKE_TEST else 2\n","N_BATCH = 20 if not SMOKE_TEST else 2\n","MC_SAMPLES = 256 if not SMOKE_TEST else 32\n","\n","verbose = False\n","\n","best_observed_all_ei, best_observed_all_nei, best_random_all = [], [], []\n","\n","\n","# average over multiple trials\n","for trial in range(1, N_TRIALS + 1):\n","    \n","    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n","    best_observed_ei, best_observed_nei, best_random = [], [], []\n","    \n","    # call helper functions to generate initial training data and initialize model\n","    train_x_ei, train_obj_ei, train_con_ei, best_observed_value_ei = generate_initial_data(n=10)\n","    mll_ei, model_ei = initialize_model(train_x_ei, train_obj_ei, train_con_ei)\n","    \n","    train_x_nei, train_obj_nei, train_con_nei = train_x_ei, train_obj_ei, train_con_ei\n","    best_observed_value_nei = best_observed_value_ei\n","    mll_nei, model_nei = initialize_model(train_x_nei, train_obj_nei, train_con_nei)\n","    \n","    best_observed_ei.append(best_observed_value_ei)\n","    best_observed_nei.append(best_observed_value_nei)\n","    best_random.append(best_observed_value_ei)\n","    \n","    # run N_BATCH rounds of BayesOpt after the initial random batch\n","    for iteration in range(1, N_BATCH + 1):    \n","        \n","        t0 = time.time()\n","        \n","        # fit the models\n","        fit_gpytorch_model(mll_ei)\n","        fit_gpytorch_model(mll_nei)\n","        \n","        # define the qEI and qNEI acquisition modules using a QMC sampler\n","        qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n","        \n","        # for best_f, we use the best observed noisy values as an approximation\n","        qEI = qExpectedImprovement(\n","            model=model_ei, \n","            best_f=(train_obj_ei * (train_con_ei <= 0).to(train_obj_ei)).max(),\n","            sampler=qmc_sampler, \n","            objective=constrained_obj,\n","        )\n","        \n","        qNEI = qNoisyExpectedImprovement(\n","            model=model_nei, \n","            X_baseline=train_x_nei,\n","            sampler=qmc_sampler, \n","            objective=constrained_obj,\n","        )\n","        \n","        # optimize and get new observation\n","        new_x_ei, new_obj_ei, new_con_ei = optimize_acqf_and_get_observation(qEI)\n","        new_x_nei, new_obj_nei, new_con_nei = optimize_acqf_and_get_observation(qNEI)\n","                \n","        # update training points\n","        train_x_ei = torch.cat([train_x_ei, new_x_ei])\n","        train_obj_ei = torch.cat([train_obj_ei, new_obj_ei])\n","        train_con_ei = torch.cat([train_con_ei, new_con_ei])\n","\n","        train_x_nei = torch.cat([train_x_nei, new_x_nei])\n","        train_obj_nei = torch.cat([train_obj_nei, new_obj_nei])\n","        train_con_nei = torch.cat([train_con_nei, new_con_nei])\n","\n","        # update progress\n","        best_random = update_random_observations(best_random)\n","        best_value_ei = weighted_obj(train_x_ei).max().item()\n","        best_value_nei = weighted_obj(train_x_nei).max().item()\n","        best_observed_ei.append(best_value_ei)\n","        best_observed_nei.append(best_value_nei)\n","\n","        # reinitialize the models so they are ready for fitting on next iteration\n","        # use the current state dict to speed up fitting\n","        mll_ei, model_ei = initialize_model(\n","            train_x_ei, \n","            train_obj_ei, \n","            train_con_ei, \n","            model_ei.state_dict(),\n","        )\n","        mll_nei, model_nei = initialize_model(\n","            train_x_nei, \n","            train_obj_nei, \n","            train_con_nei, \n","            model_nei.state_dict(),\n","        )\n","        \n","        t1 = time.time()\n","        \n","        if verbose:\n","            print(\n","                f\"\\nBatch {iteration:>2}: best_value (random, qEI, qNEI) = \"\n","                f\"({max(best_random):>4.2f}, {best_value_ei:>4.2f}, {best_value_nei:>4.2f}), \"\n","                f\"time = {t1-t0:>4.2f}.\", end=\"\"\n","            )\n","        else:\n","            print(\".\", end=\"\")\n","   \n","    best_observed_all_ei.append(best_observed_ei)\n","    best_observed_all_nei.append(best_observed_nei)\n","    best_random_all.append(best_random)"],"outputs":[],"metadata":{"id":"exfQRfyZPzDm"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","\n","def ci(y):\n","    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n","\n","\n","GLOBAL_MAXIMUM = neg_hartmann6.optimal_value\n","\n","\n","iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n","y_ei = np.asarray(best_observed_all_ei)\n","y_nei = np.asarray(best_observed_all_nei)\n","y_rnd = np.asarray(best_random_all)\n","\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","ax.errorbar(iters, y_rnd.mean(axis=0), yerr=ci(y_rnd), label=\"random\", linewidth=1.5)\n","ax.errorbar(iters, y_ei.mean(axis=0), yerr=ci(y_ei), label=\"qEI\", linewidth=1.5)\n","ax.errorbar(iters, y_nei.mean(axis=0), yerr=ci(y_nei), label=\"qNEI\", linewidth=1.5)\n","plt.plot([0, N_BATCH * BATCH_SIZE], [GLOBAL_MAXIMUM] * 2, 'k', label=\"true best objective\", linewidth=2)\n","ax.set_ylim(bottom=0.5)\n","ax.set(xlabel='number of observations (beyond initial points)', ylabel='best objective value')\n","ax.legend(loc=\"lower right\")"],"outputs":[],"metadata":{"id":"d7_T4nWeTG52"}}]}