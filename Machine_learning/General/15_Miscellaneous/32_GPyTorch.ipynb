{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_GPyTorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPefH2YXjB4a4pQtrP630su"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GPyTorch"],"metadata":{"id":"2HjYdV-XR6zy"}},{"cell_type":"markdown","source":["GPyTorch is a PyTorch-based library designed for implementing Gaussian processes. It was introduced by Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger and Andrew Gordon Wilson – researchers at Cornel University (research paper).\n","\n","Before going into the details of GPyTorch, let us first understand what a Gaussian process means, in short."],"metadata":{"id":"CWfWU3fwR4-5"}},{"cell_type":"markdown","source":["To read more about this, you can refer to [this](https://analyticsindiamag.com/guide-to-gpytorch-a-python-library-for-gaussian-process-models/) article."],"metadata":{"id":"og61vppSR98V"}},{"cell_type":"markdown","source":["# Practical implementation\n","\n","Here’s a demonstration of training an RBF kernel Gaussian process on the following function:\n","\n","y = sin(2x) + E             …(i)\n","\n","E ~ (0, 0.04)\n","\n"," (where 0 is mean of the normal distribution and 0.04 is the variance)"],"metadata":{"id":"T-8YAx2bSJcX"}},{"cell_type":"markdown","source":["Install the GPyTorch library"],"metadata":{"id":"JlrOndJFSWeZ"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels tensorflow keras --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install gpytorch --user -q"],"outputs":[],"metadata":{"id":"DZPbAf0vJah7"}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Import required libraries"],"metadata":{"id":"Wt8O54GJSaoT"}},{"cell_type":"code","execution_count":null,"source":["import math\n","import torch\n","import gpytorch\n","from matplotlib import pyplot as plt\n","%matplotlib inline #for visualization plots to appear at the frontend "],"outputs":[],"metadata":{"id":"5AmTC0QLSYvt"}},{"cell_type":"markdown","source":["Prepare training data"],"metadata":{"id":"QAKdl976SgQR"}},{"cell_type":"code","execution_count":null,"source":["# Training data is 100 points in [0,1] inclusive regularly spaced\n","train_x = torch.linspace(0, 1, 100)\n","# True function is sin(2*pi*x) with Gaussian noise\n","train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04)"],"outputs":[],"metadata":{"id":"nIwIZii7SeIc"}},{"cell_type":"markdown","source":["Define the GP model\n","\n","We have used exact inference – the simplest form of GP model"],"metadata":{"id":"cQQFipzbSm4B"}},{"cell_type":"code","execution_count":null,"source":["# We will use the simplest form of GP model, exact inference\n","class ExactGPModel(gpytorch.models.ExactGP):\n","    def __init__(self, train_x, train_y, likelihood):\n","        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n","        self.mean_module = gpytorch.means.ConstantMean()\n","        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n","\n","    def forward(self, x):\n","        mean_x = self.mean_module(x)\n","        covar_x = self.covar_module(x)\n","        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"],"outputs":[],"metadata":{"id":"CehQhsTHSj3P"}},{"cell_type":"markdown","source":["For most of the GP regression models, following objects should be constructed:\n","\n","> * A ‘GP Model’ which handles most of the inference\n","> * A ‘Likelihood’\n","> * A ‘Mean’ defining prior mean of GP\n","> * A ‘Kernel’ defining covariance of GP\n","> * A Multivariate Normal Distribution\n","\n","The two methods defined above are components of the Exact (non-variational) GP model.\n","\n","The _init_ method takes a likelihood and the training data. It then constructs objects like mean module and kernel module required for the ‘forward’ method of the model. The ‘forward’ method takes in some data x. It returns a multivariate normal distribution with prior mean and covariance computed at x."],"metadata":{"id":"FO-zeF7dSshD"}},{"cell_type":"markdown","source":["  Initialize likelihood"],"metadata":{"id":"uLPrN3DZSy0g"}},{"cell_type":"code","execution_count":null,"source":["# initialize likelihood and model\n","likelihood = gpytorch.likelihoods.GaussianLikelihood()"],"outputs":[],"metadata":{"id":"AY3T-Y2XSqfj"}},{"cell_type":"markdown","source":["Initialize the GP model"],"metadata":{"id":"TW7bcrjLS2e8"}},{"cell_type":"code","execution_count":null,"source":["model = ExactGPModel(train_x, train_y, likelihood)"],"outputs":[],"metadata":{"id":"P4WZTUTVS1FL"}},{"cell_type":"markdown","source":["  Find optimal hyperparameters of the model"],"metadata":{"id":"FuupfppcS6cL"}},{"cell_type":"code","execution_count":null,"source":["# Find optimal model hyperparameters\n","model.train()\n","likelihood.train()"],"outputs":[],"metadata":{"id":"2EzwRE-6S4gG"}},{"cell_type":"markdown","source":["Use Adam optimization algorithm"],"metadata":{"id":"d9S6OAxyS-3H"}},{"cell_type":"code","execution_count":null,"source":["# Use the adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters"],"outputs":[],"metadata":{"id":"phnc7PGVS-ma"}},{"cell_type":"markdown","source":["  Define loss for GP"],"metadata":{"id":"PPdWSp_kTDuf"}},{"cell_type":"code","execution_count":null,"source":["# \"Loss\" for GPs - the marginal log likelihood\n","mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"],"outputs":[],"metadata":{"id":"9FWR8JWDS8im"}},{"cell_type":"markdown","source":["Compute loss, length scale (i.e. length of twists and turns in the function) and noise for each iteration of the GP"],"metadata":{"id":"be7CidJtWdlK"}},{"cell_type":"code","execution_count":null,"source":["for i in range(20):\n","    # Zero gradients from previous iteration\n","    optimizer.zero_grad()\n","    # Output from model\n","    output = model(train_x)\n","    # Calc loss and backprop gradients\n","    loss = -mll(output, train_y)\n","    loss.backward()\n","    print('Iteration %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n","        i + 1, 20, loss.item(),\n","        model.covar_module.base_kernel.lengthscale.item(),\n","        model.likelihood.noise.item()\n","    ))\n","    optimizer.step()"],"outputs":[],"metadata":{"id":"WMtkBq-CWdXc"}},{"cell_type":"markdown","source":["  Make prediction with the model"],"metadata":{"id":"otlynE0KWmkL"}},{"cell_type":"code","execution_count":null,"source":["# Get into evaluation (predictive posterior) mode\n","model.eval()\n","likelihood.eval()"],"outputs":[],"metadata":{"id":"AJ6dj5RwTGIL"}},{"cell_type":"markdown","source":["  Make predictions by feeding model through likelihood"],"metadata":{"id":"rTc6WZqfWqJ0"}},{"cell_type":"code","execution_count":null,"source":["# Test points are regularly spaced along [0,1]\n","# Make predictions by feeding model through likelihood\n","with torch.no_grad(), gpytorch.settings.fast_pred_var():\n","    test_x = torch.linspace(0, 1, 51)\n","    observed_pred = likelihood(model(test_x))"],"outputs":[],"metadata":{"id":"z6ykRqiVWow2"}},{"cell_type":"markdown","source":["Plot the fitted model"],"metadata":{"id":"KfoSQeTwWtif"}},{"cell_type":"code","execution_count":null,"source":["with torch.no_grad():\n","    # Initialize plot\n","    f, ax = plt.subplots(1, 1, figsize=(8, 8))\n","\n","    # Get upper and lower confidence bounds\n","    lower, upper = observed_pred.confidence_region()\n","    # Plot training data as black stars\n","    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n","    # Plot predictive means as blue line\n","    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n","    # Shade between the lower and upper confidence bounds\n","    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n","    ax.set_ylim([-3, 3])\n","    ax.legend(['Observed Data', 'Mean', 'Confidence'])"],"outputs":[],"metadata":{"id":"BStlrRqmWsSe"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"afrp9LYdWvuf"}}]}