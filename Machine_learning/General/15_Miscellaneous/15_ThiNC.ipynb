{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_ThiNC.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMg1Nw9LGDwLWmK6pvaxx33"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ThiNC"],"metadata":{"id":"AcExJN35xtJq"}},{"cell_type":"markdown","source":["THiNC is a deep learning framework that makes composing, configuring and deploying models easy. It provides a flexible yet simple approach to modelling by providing low-level abstractions of the training loop, evaluation loop etc. Moreover, it plays well with major deep learning frameworks like TensorFlow and PyTorch. The functional programming API of THiNC is fairly simple and elegant. It’s light weighted API makes THiNC a good option for quick prototyping and deployment of machine learning models."],"metadata":{"id":"Le9GZjq5xma-"}},{"cell_type":"markdown","source":["# Code Implementation"],"metadata":{"id":"E8VXQcmcxwlb"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"xlO3Njjh01lO"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels tensorflow keras torch torchvision --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install \"thinc==8.0.0rc6.dev0\" --user -q\n","#To use GPU we need to install cupy\n","!python -m pip install \"cupy-cuda101\" --user -q"],"outputs":[],"metadata":{"id":"bN110joIBn4F"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install \"ml_datasets>=0.2.0a0\" --user -q\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import ml_datasets\n","(train_X, train_Y), (dev_X, dev_Y) = ml_datasets.mnist()\n","print(f\"Training size={len(train_X)}, dev size={len(dev_X)}\")"],"outputs":[],"metadata":{"id":"PVZGELTDCHK1"}},{"cell_type":"code","execution_count":null,"source":["train_X.shape,train_Y.shape"],"outputs":[],"metadata":{"id":"emiNCWAgCwIR"}},{"cell_type":"markdown","source":["## Classifier Model\n","\n","To Build a model all the classes and functions must be imported from thin.api .Let’s build a simple FCNN with dropout to recognize the handwritten digits of MNIST dataset."],"metadata":{"id":"I7yhEF1n05hQ"}},{"cell_type":"code","execution_count":null,"source":["from thinc.api import Model,chain, Relu, Softmax\n"," \n","n_hidden1, n_hidden2, n_hidden3 = 64,32,10\n","dropout = 0.2\n","\n","model = chain(\n","    Relu(nO=n_hidden1, dropout=dropout),\n","    Relu(nO=n_hidden2, dropout=dropout), \n","    Relu(nO=n_hidden3, dropout=dropout), \n","    Softmax()\n",")"],"outputs":[],"metadata":{"id":"XuwPjK83C0F9"}},{"cell_type":"markdown","source":["## Operator Overloading\n","\n","Operator overloading can be used to build complex models in a concise manner.This FCNN model can be built in a single line by using operator overloading."],"metadata":{"id":"cD2DzRw00-2o"}},{"cell_type":"code","execution_count":null,"source":["with Model.define_operators({\">>\": chain}):\n","    model = Relu(nO=n_hidden1, dropout=dropout) >> Relu(nO=n_hidden2, dropout=dropout) >> Relu(nO=n_hidden3, dropout=dropout) >> Softmax()"],"outputs":[],"metadata":{"id":"XHkCRgsOFxPK"}},{"cell_type":"markdown","source":["## Model Initialization\n","\n","Input and output shapes, along with all the missing shape information in the model can be inferred automatically by initializing the model with sample inputs."],"metadata":{"id":"Ga7an6VJ1C6_"}},{"cell_type":"code","execution_count":null,"source":["# making sure the data is on the right device\n","train_X = model.ops.asarray(train_X)\n","train_Y = model.ops.asarray(train_Y)\n","dev_X = model.ops.asarray(dev_X)\n","dev_Y = model.ops.asarray(dev_Y)\n","\n","model.initialize(X=train_X[:5], Y=train_Y[:5])\n","nI = model.get_dim(\"nI\")\n","nO = model.get_dim(\"nO\")\n","print(f\"Initialized model with input dimension nI={nI} and output dimension nO={nO}\")"],"outputs":[],"metadata":{"id":"akE2dxVnG_9q"}},{"cell_type":"markdown","source":["## Training the Model\n","\n","Next, we need to build a training loop. THiNC provides low-level abstractions for batching the data and shuffling it. These help in building custom training loops. The following lines are the backbone of training."],"metadata":{"id":"eF1REHLg1Jo8"}},{"cell_type":"code","execution_count":null,"source":["from tqdm.notebook import tqdm\n","def train_model(data, model, optimizer, n_iter, batch_size):\n","    (train_X, train_Y), (dev_X, dev_Y) = data\n","    indices = model.ops.xp.arange(train_X.shape[0], dtype=\"i\")\n","    for i in range(n_iter):\n","        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n","        for X, Y in tqdm(batches, leave=False):\n","            Yh, backprop = model.begin_update(X)\n","            backprop(Yh - Y)\n","            model.finish_update(optimizer)\n","        # Evaluate and print progress\n","        correct = 0\n","        total = 0\n","        for X, Y in model.ops.multibatch(batch_size, dev_X, dev_Y):\n","            Yh = model.predict(X)\n","            correct += (Yh.argmax(axis=1) == Y.argmax(axis=1)).sum()\n","            total += Yh.shape[0]\n","        score = correct / total\n","        print(f\" {i} {float(score):.3f}\")"],"outputs":[],"metadata":{"id":"OKWzrqiwHT6y"}},{"cell_type":"code","execution_count":null,"source":["from thinc.api import Adam, fix_random_seed\n","fix_random_seed(0)\n","optimizer = Adam(0.001)\n","batch_size = 128\n","n_iter = 10\n","train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)"],"outputs":[],"metadata":{"id":"SKXyVortJTN0"}},{"cell_type":"markdown","source":["## Compatibility with Other Frameworks\n","\n","THiNC provides awesome wrappers to integrate with Tensorflow, PyTorch and MXNet.These wrappers help a lot when it comes to porting code amongst different frameworks. If some functionality you need is ready in a different framework, then you can use those special layers by wrapping them using the THiNC wrappers for that particular framework.\n","\n","Example: Let’s implement the same FCNN as above using layers from both TensorFlow and PyTorch."],"metadata":{"id":"NV8jQLkf1QMu"}},{"cell_type":"code","execution_count":null,"source":["from thinc.api import PyTorchWrapper, TensorFlowWrapper, chain, Linear, Adam\n","dropout=0.2\n","\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","tf_model = Sequential()\n","tf_model.add(Dense(64, activation=\"relu\", input_shape=(784,)))\n","tf_model.add(Dropout(dropout))\n","\n","import torch\n","import torch.nn\n","import torch.nn.functional as F\n","class PyTorchModel(torch.nn.Module):\n","    def __init__(self, nO, nI, dropout):\n","        super(PyTorchModel, self).__init__()\n","        self.dropout1 = torch.nn.Dropout2d(dropout)\n","        self.fc1 = torch.nn.Linear(nI, nO,)\n","\n","    def forward(self, x):\n","        x = F.relu(x)\n","        x = self.dropout1(x)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        return x\n","\n","\n","\n","model = chain(\n","    TensorFlowWrapper(tf_model),  \n","    PyTorchWrapper(PyTorchModel( 32, 64, dropout)),\n","    Relu(nO=10, dropout=dropout), \n","    Softmax()\n",")\n","model"],"outputs":[],"metadata":{"id":"p8UkKRoiJ1UK"}},{"cell_type":"code","execution_count":null,"source":["model.initialize(X=train_X[:5], Y=train_Y[:5])"],"outputs":[],"metadata":{"id":"kls_gFqbSIK5"}},{"cell_type":"code","execution_count":null,"source":["train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)"],"outputs":[],"metadata":{"id":"oFcU_sEVPqKG"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","preds,_=model(train_X,is_train=False)\n","print('Following image is predicted as {}'.format(np.argmax(preds[0])))\n","plt.imshow(train_X[0].reshape(28,28),cmap='gray')\n","plt.show()"],"outputs":[],"metadata":{"id":"VO0Vot4hRbv2"}},{"cell_type":"markdown","source":["## Config System\n","\n","Configuration is an important aspect of product development. You need models to be manageable. We might often need to expose lots of model components as hyperparameters.\n","\n","THiNC provides a great config system to do this.We can define trees of hyperparameters using simple json like structures. Then we can resolve these configurations and get the functions to run.Let’s create a config for the classifier model above."],"metadata":{"id":"V8KH3hrH1V7R"}},{"cell_type":"code","execution_count":null,"source":["from thinc.api import Config, registry\n","CONFIG = \"\"\"\n","[hyper_params]\n","n_hidden1 = 64\n","n_hidden2 = 32\n","n_hidden3 = 10\n","dropout = 0.2\n","\n","\n","[model]\n","@layers = \"chain.v1\"\n","\n","[model.*.relu1]\n","@layers = \"Relu.v1\"\n","nO = ${hyper_params:n_hidden1}\n","dropout = ${hyper_params:dropout}\n","\n","[model.*.relu2]\n","@layers = \"Relu.v1\"\n","nO = ${hyper_params:n_hidden2}\n","dropout = ${hyper_params:dropout}\n","\n","[model.*.relu3]\n","@layers = \"Relu.v1\"\n","nO = ${hyper_params:n_hidden3}\n","dropout = ${hyper_params:dropout}\n","\n","[model.*.softmax]\n","@layers = \"Softmax.v1\"\n","\n","[optimizer]\n","@optimizers = \"Adam.v1\"\n","\n","\n","[optimizer.learn_rate]\n","@schedules = \"warmup_linear.v1\"\n","initial_rate = 2e-5\n","warmup_steps = 1000\n","total_steps = 10000\n","\n","[training]\n","n_iter = 10\n","batch_size = 128\n","\"\"\"\n","\n","config = Config().from_str(CONFIG)\n","config"],"outputs":[],"metadata":{"id":"fen81SokTM4q"}},{"cell_type":"code","execution_count":null,"source":["loaded_config = registry.resolve(config)\n","loaded_config"],"outputs":[],"metadata":{"id":"x4kJ9umneUOv"}},{"cell_type":"code","execution_count":null,"source":["loaded_config = registry.resolve(config)\n","model = loaded_config[\"model\"]\n","optimizer = loaded_config[\"optimizer\"]\n","n_iter = loaded_config[\"training\"][\"n_iter\"]\n","batch_size = loaded_config[\"training\"][\"batch_size\"]\n","\n","model.initialize(X=train_X[:5], Y=train_Y[:5])\n","train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)"],"outputs":[],"metadata":{"id":"ROnXQWT5eyWW"}},{"cell_type":"code","execution_count":null,"source":["y_true=np.argmax(train_Y,axis=1)\n","y_pred=np.argmax(preds,axis=1)"],"outputs":[],"metadata":{"id":"o_Aid5EsfA8M"}},{"cell_type":"code","execution_count":null,"source":["def accuracy(y_true,y_pred):\n","  assert len(y_true)==len(y_pred),\"Lengths of labels mismatched\"\n","  assert len(y_pred)!=0,\"Predictions are empty\"\n","  return sum([1 if i==j else 0 for i,j in zip(y_true,y_pred)])/len(y_pred)\n","#Accuracy is on the scale of 0 to 1.\n","accuracy(y_true,y_pred)\n"],"outputs":[],"metadata":{"id":"uHtDY8ERW7IR"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.metrics import precision_score\n","precision_score(y_true,y_pred,average=None)"],"outputs":[],"metadata":{"id":"-3tcoDu0ZOsK"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","def multiclass_precision(y_true,y_pred):\n","  assert len(y_true)==len(y_pred),\"Lengths of labels mismatched\"\n","  assert len(y_pred)!=0,\"Predictions are empty\"\n","  precisions=[]\n","  for class_ in sorted(np.unique(y_true)):\n","    indices = np.array(y_pred==class_).nonzero()[0]\n","    try:\n","      precisions.append(sum(y_true[indices]==class_)/len(indices))\n","    except ZeroDivisionError as e:\n","      precisions.append(0)\n","  return precisions\n","#Accuracy is on the scale of 0 to 1.\n","def precision_score(y_true,y_pred,aggregator=None):\n","  try:\n","    return multiclass_precision(y_true,y_pred)\n","  except AssertionError as a:\n","    print(a)\n","    raise KeyError\n","  except TypeError:\n","    return accuracy(*list(zip(*list(filter(lambda x:x[1],zip(y_true,y_pred)))))) if sum(y_pred)!=0 else 0\n","\n","# Precision is on the scal eof 0 to 1\n","precision_score(y_true,y_pred)"],"outputs":[],"metadata":{"id":"IOCPNt4HY45-"}},{"cell_type":"code","execution_count":null,"source":["def multiclass_recall(y_true,y_pred):\n","  assert len(y_true)==len(y_pred),\"Lengths of labels mismatched\"\n","  assert len(y_pred)!=0,\"Predictions are empty\"\n","  recalls=[]\n","  for class_ in sorted(np.unique(y_true)):\n","    indices = np.array(y_true == class_).nonzero()[0]\n","    try:\n","      recalls.append(sum(y_pred[indices]==class_)/len(indices))\n","    except ZeroDivisionError as e:\n","      recalls.append(0)\n","  return recalls\n","#Accuracy is on the scale of 0 to 1.\n","def recall_score(y_true,y_pred,aggregator=None):\n","  try:\n","    return multiclass_recall(y_true,y_pred)\n","  except AssertionError as a:\n","    print(a)\n","    raise KeyError\n","  except TypeError:\n","    return accuracy(*list(zip(*list(filter(lambda x:x[0],zip(y_true,y_pred)))))) if sum(y_pred)!=0 else 0\n","\n","#Recall is on the scale of 0 to 1\n","recall_score(y_true,y_pred)"],"outputs":[],"metadata":{"id":"OhZJ0yi_bZOA"}},{"cell_type":"code","execution_count":null,"source":["import operator\n","def f1_score(y_true,y_pred):\n","  try:\n","    prcsn,rcl = precision_score(y_true,y_pred),recall_score(y_true,y_pred)\n","    return (2*np.multiply(prcsn,rcl))/np.add(prcsn,rcl)\n","  except ZeroDivisionError:\n","    return 0\n","#f1_score is on the scale of 0 to 1\n","f1_score(y_true,y_pred)"],"outputs":[],"metadata":{"id":"iXX0RvESol6o"}},{"cell_type":"code","execution_count":null,"source":["2*np.multiply([2,3],[3,4,])"],"outputs":[],"metadata":{"id":"zDxR9LBUzQXb"}},{"cell_type":"code","execution_count":null,"source":["np.add([2,3],[3,4,])"],"outputs":[],"metadata":{"id":"KwWFpjfPzrB4"}}]}