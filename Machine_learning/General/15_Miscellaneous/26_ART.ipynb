{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_ART.ipynb","provenance":[],"authorship_tag":"ABX9TyNr10PPY0EjuW8jvffs84sE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ART"],"metadata":{"id":"L_SCRc3azOrc"}},{"cell_type":"markdown","source":["The Adversarial Robustness Toolbox(ART) is a Python library which is one of the complete resources providing developers and researchers for evaluating the robustness of deep neural networks against adversarial attacks. Open-sourced by IBM, ART provides support to incorporate techniques to prevent adversarial attacks for deep neural networks written in TensorFlow, Keras, PyTorch, sci-kit-learn, MxNet, XGBoost, LightGBM, CatBoost and many more deep learning frameworks. It can be applied to all kinds of data from images, video, tables, to audio, and many more. It is cross-platform and supports various machine learning tasks such as classification, speech recognition, object detection, generation, certification, etcc."],"metadata":{"id":"DcUGexyazQac"}},{"cell_type":"markdown","source":["Please refer [this](https://analyticsindiamag.com/adversarial-robustness-toolbox-art/) article, to know about it more.\n"],"metadata":{"id":"n6FXbXg5zUYY"}},{"cell_type":"markdown","source":["# Adversarial Action Recognition Attack"],"metadata":{"id":"68hoWw3gzfE4"}},{"cell_type":"markdown","source":["This demonstrates the usage of ART library to impose an adversarial attack on video action recognition. First, it uses GluonCV and MXNet for video action recognition. MXNet pre-trained models are used for classification tasks. Specifically, the pre-trained i3d_resnet50_v1_ucf101 model is used. The video clip of a basketball action taken from the UCF101 dataset. To show how to classify the following short video clip correctly."],"metadata":{"id":"mTE081c6zh3Z"}},{"cell_type":"markdown","source":["# Initial working stages  \n","\n","> * the sample basketball to be downloaded \n","> * the pre-trained action recognition model is to be loaded\n","> * To show that the model can correctly classify the video action as playing basketball."],"metadata":{"id":"hgO-sH9gzjxu"}},{"cell_type":"markdown","source":["## Loading Model and Basketball Sample"],"metadata":{"id":"1tGFAlDCznK2"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels tensorflow keras --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install decord --user -q\n","!python -m pip install gluoncv --user -q\n","!python -m pip install mxnet --user -q\n","!python -m pip install adversarial-robustness-toolbox --user -q"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lSc91xV1zuLx","executionInfo":{"status":"ok","timestamp":1624451459999,"user_tz":-330,"elapsed":22488,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"940bb15b-d302-4d9c-9d4b-217ba0a532e1"}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import os\n","import tempfile\n","import decord\n","from gluoncv import utils\n","from gluoncv.data.transforms import video\n","from gluoncv.model_zoo import get_model\n","from gluoncv.utils.filesystem import try_import_decord\n","import imageio\n","from matplotlib.image import imsave\n","import matplotlib.pyplot as plt\n","import mxnet as mx\n","from mxnet import gluon, nd, image\n","from mxnet.gluon.data.vision import transforms\n","import numpy as np\n","from art.attacks.evasion import FastGradientMethod, FrameSaliencyAttack\n","from art import config\n","from art.defences.preprocessor import VideoCompression\n","from art.estimators.classification import MXClassifier "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIIn5BM5zKL7","executionInfo":{"status":"ok","timestamp":1624451861903,"user_tz":-330,"elapsed":5560,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"d6cdac1c-5874-4587-ec96-69d377a86bbd"}},{"cell_type":"code","execution_count":null,"source":["# setting global variables\n","\n","PRETRAINED_MODEL_NAME = 'i3d_resnet50_v1_ucf101'\n","VIDEO_SAMPLE_URI = 'https://github.com/bryanyzhu/tiny-ucf101/raw/master/v_Basketball_g01_c01.avi' "],"outputs":[],"metadata":{"id":"FxULsYRlzrFA","executionInfo":{"status":"ok","timestamp":1624451865923,"user_tz":-330,"elapsed":590,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Setting seed"],"metadata":{"id":"mwKz_WgI2gO8"}},{"cell_type":"code","execution_count":null,"source":["np.random.seed(123)\n","def predict_top_k(video_input, model, k=5, verbose=True):\n","    pred = model(nd.array(video_input))\n","    classes = model.classes    \n","    ind = nd.topk(pred, k=k)[0].astype('int')\n","    if verbose:\n","        msg = \"The sample video clip is\"\n","        for i in range(k):\n","            msg += f\"\\n\\t[{classes[ind[i].asscalar()]}], with probability {nd.softmax(pred)[0][ind[i]].asscalar():.3f}.\"\n","        print(msg)\n","    return ind\n","def sample_to_gif(sample, output=\"sample.gif\", path=config.ART_DATA_PATH, postprocess=None):\n","    frame_count = sample.shape[1]\n","    output_path = os.path.join(path, output)\n","    with tempfile.TemporaryDirectory() as tmpdir, imageio.get_writer(output_path, mode='I') as writer:\n","        for frame in range(frame_count):\n","            file_path = os.path.join(tmpdir, f\"{frame}.png\")\n","            imsave(file_path, np.transpose(sample[:,frame,:,:], (1,2,0)))\n","            writer.append_data(imageio.imread(file_path))\n","    return output_path "],"outputs":[],"metadata":{"id":"2AAYFHfI2hlP","executionInfo":{"status":"ok","timestamp":1624451866597,"user_tz":-330,"elapsed":3,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["### Downloading sample video"],"metadata":{"id":"j59-jD-X2m3R"}},{"cell_type":"code","execution_count":null,"source":["decord = try_import_decord()\n","video_fname = utils.download(VIDEO_SAMPLE_URI, path=config.ART_DATA_PATH);\n","video_reader = decord.VideoReader(video_fname)\n","frame_id_list = range(0, 64, 2)\n","video_data = video_reader.get_batch(frame_id_list).asnumpy()\n","video_sample_lst = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)] "],"outputs":[],"metadata":{"id":"z3_isU0l2lYQ","executionInfo":{"status":"ok","timestamp":1624451867230,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["### Preprocessing the benign video sample"],"metadata":{"id":"gW81H_eF2sGh"}},{"cell_type":"code","execution_count":null,"source":["transform_fn = video.VideoGroupValTransform(size=224, mean=[0.475, 0.465, 0.475], std=[0.220, 0.200, 0.225])\n","sample = np.stack(transform_fn(video_sample_lst),  axis=0)\n","sample = sample.reshape((-1,) + (32, 3, 224, 224))\n","sample = np.transpose(sample, (0, 2, 1, 3, 4))\n","print(f\"`{video_fname}` has been downloaded and preprocessed.\") "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9Qx-Prn2qhi","executionInfo":{"status":"ok","timestamp":1624451867231,"user_tz":-330,"elapsed":8,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"c6f92bd3-9634-4e9b-c5d1-14f461ca4d62"}},{"cell_type":"code","execution_count":null,"source":["# loading pretrained model\n","\n","model = get_model(PRETRAINED_MODEL_NAME, nclass=101, pretrained=True)\n","print(f\"`{PRETRAINED_MODEL_NAME}` model was successfully loaded.\") "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6eQFA9r2vA9","executionInfo":{"status":"ok","timestamp":1624451869049,"user_tz":-330,"elapsed":1824,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"e48ebead-3497-481d-b49e-fa31c54e53c4"}},{"cell_type":"code","execution_count":null,"source":["# evaluating model on basketball video sample\n","\n","_ = predict_top_k(sample, model)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"toqQ2VjG2xWI","executionInfo":{"status":"ok","timestamp":1624451871066,"user_tz":-330,"elapsed":2019,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"8cefe082-131f-4794-e52f-f6e41e8c121c"}},{"cell_type":"markdown","source":["For the given video sample, it is seen that the model correctly classified it as playing basketball."],"metadata":{"id":"OBsQy3fb22Sr"}},{"cell_type":"markdown","source":["Now we can include the ART library for the adversarial attack via the Fast Gradient Method. The attack is incorporated to corrupt the video sample so that it could be misclassified. Also, the adversarial example is converted into a GIF "],"metadata":{"id":"ZIPUXGea24hI"}},{"cell_type":"markdown","source":["Adversarial Basketball\n","\n"],"metadata":{"id":"AM6kMd-u27KV"}},{"cell_type":"code","execution_count":null,"source":["# preprocessing the adversarial sample video input\n","\n","transform_fn_unnormalized = video.VideoGroupValTransform(size=224, mean=[0, 0, 0], std=[1, 1, 1])\n","adv_sample_input = np.stack(transform_fn_unnormalized(video_sample_lst),  axis=0)\n","adv_sample_input = adv_sample_input.reshape((-1,) + (32, 3, 224, 224))\n","adv_sample_input = np.transpose(adv_sample_input, (0, 2, 1, 3, 4)) "],"outputs":[],"metadata":{"id":"Kle06tV82z-H","executionInfo":{"status":"ok","timestamp":1624451871068,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["# wrapping model in a ART classifier\n","\n","model_wrapper = gluon.nn.Sequential()\n","with model_wrapper.name_scope():\n","    model_wrapper.add(model) \n","\n","# preparing the mean and std arrays for ART classifier preprocessing\n","\n","mean = np.array([0.485, 0.456, 0.406] * (32 * 224 * 224)).reshape((3, 32, 224, 224), order='F')\n","std = np.array([0.229, 0.224, 0.225] * (32 * 224 * 224)).reshape((3, 32, 224, 224), order='F')\n","classifier_art = MXClassifier(\n","    model=model_wrapper,\n","    loss=gluon.loss.SoftmaxCrossEntropyLoss(),\n","    input_shape=(3, 32, 224, 224),\n","    nb_classes=101,\n","    preprocessing=(mean, std),\n","    clip_values=(0, 1),\n","    channels_first=True,\n",") "],"outputs":[],"metadata":{"id":"5c6cncTJ3CCR","executionInfo":{"status":"ok","timestamp":1624451871719,"user_tz":-330,"elapsed":658,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["# verifying whether the ART classifier predictions are consistent with the original model:\n","\n","pred = nd.array(classifier_art.predict(adv_sample_input))\n","ind = nd.topk(pred, k=5)[0].astype('int')\n","msg = \"The video sample clip is classified\"\n","for i in range(len(ind)):\n","    msg += f\"\\n\\t[{model.classes[ind[i].asscalar()]}], with probability {nd.softmax(pred)[0][ind[i]].asscalar():.3f}.\"\n","print(msg) "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95B9M_f43FS9","executionInfo":{"status":"ok","timestamp":1624451874346,"user_tz":-330,"elapsed":2629,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"ddfaa6a0-5873-44cc-e1f1-c3fd3ae8f895"}},{"cell_type":"code","execution_count":null,"source":["# crafting adversarial attack with FGM\n","\n","epsilon = 8/255\n","fgm = FastGradientMethod(\n","    classifier_art,\n","    eps=epsilon,\n",")"],"outputs":[],"metadata":{"id":"OJS2Dd-73HaL","executionInfo":{"status":"ok","timestamp":1624451874347,"user_tz":-330,"elapsed":10,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["adv_sample = fgm.generate(\n","    x=adv_sample_input\n",") "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"id":"l_B748tA3Ke8","executionInfo":{"status":"error","timestamp":1624451878700,"user_tz":-330,"elapsed":4359,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"920408c4-0cc8-4f52-ae48-805246f8c400"}},{"cell_type":"code","execution_count":null,"source":["#printing results\n","\n","_ = predict_top_k((adv_sample-mean)/std, model)"],"outputs":[],"metadata":{"id":"1l_g4xxU3PKQ","executionInfo":{"status":"aborted","timestamp":1624451878691,"user_tz":-330,"elapsed":18,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"code","execution_count":null,"source":["# saving adversarial example to gif:\n","\n","adversarial_gif = sample_to_gif(np.squeeze(adv_sample), \"adversarial_basketball.gif\")\n","print(f\"`{adversarial_gif}` has been successfully created.\") "],"outputs":[],"metadata":{"id":"r07r2co73UeB"}},{"cell_type":"markdown","source":["\n","\n","Creating Sparse Adversarial Attack\n","\n","Using the Frame Saliency Attack, now it’s time to create a sparse adversarial example. The final result is shown in the GIF. Here only one frame is needed to be perturbed to achieve a misclassification.\n","\n","adversarial_basketball_sparse.gif"],"metadata":{"id":"mvRPExis3Xen"}},{"cell_type":"code","execution_count":null,"source":["# Frame Saliency Attack. Note: we specify here the frame axis, which is 2.\n","\n","fsa = FrameSaliencyAttack(\n","    classifier_art,\n","    fgm,\n","    \"iterative_saliency\",\n","    frame_index = 2\n",")\n"],"outputs":[],"metadata":{"id":"hxRPe6fJ28gc"}},{"cell_type":"code","execution_count":null,"source":["%%time\n","adv_sample_sparse = fsa.generate(\n","    x=adv_sample_input\n",") "],"outputs":[],"metadata":{"id":"f-LMx1b-3Z7w"}},{"cell_type":"code","execution_count":null,"source":["_ = predict_top_k((adv_sample_sparse-mean)/std, model)"],"outputs":[],"metadata":{"id":"_kwOapSf3kJn"}},{"cell_type":"code","execution_count":null,"source":["# Again saving the adversarial example to gif:\n","\n","adversarial_sparse_gif = sample_to_gif(np.squeeze(adv_sample_sparse), \"adversarial_basketball_sparse.gif\")\n","print(f\"`{adversarial_sparse_gif}` has been successfully created.\") "],"outputs":[],"metadata":{"id":"5gDQZC9s3lZE"}},{"cell_type":"code","execution_count":null,"source":["# counting the number of perturbed frames:\n","\n","x_diff = adv_sample_sparse - adv_sample_input\n","x_diff = np.swapaxes(x_diff, 1, 2)\n","x_diff = np.reshape(x_diff, x_diff.shape[:2] + (np.prod(x_diff.shape[2:]), ))\n","x_diff_norm = np.sign(np.round(np.linalg.norm(x_diff, axis=-1), decimals=4))\n","print(f\"Number of perturbed frames: {int(np.sum(x_diff_norm))}\") "],"outputs":[],"metadata":{"id":"fDwVroeS3odU"}},{"cell_type":"markdown","source":["\n","Applying H.264 compression defence\n","\n","Next VideoCompression is applied as a simple input preprocessing defence mechanism. This defence is intended to correct predictions when applied to both the original and the adversarial video input.\n","\n","Initializing VideoCompression defense"],"metadata":{"id":"qNZ5rAFc3sbN"}},{"cell_type":"code","execution_count":null,"source":["video_compression = VideoCompression(video_format=\"avi\", constant_rate_factor=30, channels_first=True)\n","# applying defense to the original input\n","adv_sample_input_compressed = video_compression(adv_sample_input * 255)[0] / 255\n","# applying defense to the sparse adversarial sample\n","adv_sample_sparse_compressed = video_compression(adv_sample_sparse * 255)[0] / 255\n","# printing the resulting predictions on compressed original input\n","_ = predict_top_k((adv_sample_input_compressed-mean)/std, model)\n","# printing the resulting predictions on sparse adversarial sample\n","_ = predict_top_k((adv_sample_sparse_compressed-mean)/std, model)"],"outputs":[],"metadata":{"id":"a6MKlygG3ht9"}}]}