{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_Imbalance_Classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO240/d2WAkLSwYDE1TENO8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Imbalance Classification**"],"metadata":{"id":"5VYDLhEgUYUc"}},{"cell_type":"markdown","source":["In classification machine learning problems(binary and multiclass), datasets are often imbalanced which means that one class has a higher number of samples than others. This will lead to bias during the training of the model, the class containing a higher number of samples will be preferred more over the classes containing a lower number of samples. Having bias will, in turn, increase the true-negative and false-positive rates. Hence to overcome this bias of the model we need to make the dataset balanced containing an approximately equal number of samples in all the classes."],"metadata":{"id":"E41bve1aUhQc"}},{"cell_type":"markdown","source":["## **Implementation**"],"metadata":{"id":"omL0WaV8ZSkU"}},{"cell_type":"markdown","source":["For demonstration, We've taken the Pima Indians Diabetes Database by UCI Machine Learning from Kaggle. Get the dataset from [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database). This is a binary classification dataset. Dataset consists of various factors related to diabetes – Pregnancies, Glucose, blood pressure, Skin Thickness, Insulin, BMI, Diabetes Pedigree, Age, Outcome(1 for positive, 0 for negative). ‘Outcome’ is the dependent variable, rest are independent variables. \n","\n","\n","Python provides a package imbalance-learn for handling imbalanced datasets "],"metadata":{"id":"QI6gWjZDZWF_"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn imbalanced-learn --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### **Exploring the dataset**"],"metadata":{"id":"9dzMrXU7ZkYB"}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd \n","import matplotlib.pyplot as plt\n","df = pd.read_csv('diabetes.csv')\n","df['Outcome'].value_counts()"],"outputs":[],"metadata":{"id":"LAgDX7ylazKa"}},{"cell_type":"markdown","source":["It clearly shows an imbalance dataset wherein class 0 is 500 and class 1 is 268 which is nearly half of class 0. \n","\n","For better understanding, we do some visualization."],"metadata":{"id":"t8x7GwMma7RO"}},{"cell_type":"code","execution_count":null,"source":["count_classes = pd.value_counts(df['Outcome'], sort = True)\n","count_classes.plot(kind = 'bar', rot=0)\n","plt.title(\"Class Distribution\")\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Frequency\")"],"outputs":[],"metadata":{"id":"ppF2c-jta9gV"}},{"cell_type":"markdown","source":["We separate out the dependent and independent variables into variable X and Y respectively."],"metadata":{"id":"JAlwiBXVbA-d"}},{"cell_type":"code","execution_count":null,"source":["X = df.drop('Outcome',axis = 1)\n","Y = df['Outcome']"],"outputs":[],"metadata":{"id":"684HHPftbDx8"}},{"cell_type":"markdown","source":["Original dataset size"],"metadata":{"id":"q8o5LfoEbHBk"}},{"cell_type":"code","execution_count":null,"source":["X.shape,Y.shape"],"outputs":[],"metadata":{"id":"Ml3gI4BxbFaO"}},{"cell_type":"markdown","source":["###**Undersampling**"],"metadata":{"id":"zMG7crmKbDJu"}},{"cell_type":"markdown","source":["This technique samples down or reduces the samples of the class containing more data equivalent to the class containing the least samples. Suppose class A has 900 samples and class B has 100 samples, then the imbalance ratio is 9:1. Using the undersampling technique we keep class B as 100 samples and from class A we randomly select 100 samples out of 900. Then the ratio becomes 1:1 and we can say it’s balanced.\n","\n","From the imblearn library, we have the under_sampling module which contains various libraries to achieve undersampling. Out of those, I’ve shown the performance of the NearMiss module."],"metadata":{"id":"da2fRMoabQAF"}},{"cell_type":"code","execution_count":null,"source":["from imblearn.under_sampling import NearMiss\n","nm = NearMiss()\n","X_res,y_res=nm.fit_sample(X,Y)\n","X_res.shape,y_res.shape"],"outputs":[],"metadata":{"id":"LetUvuiHbTMV"}},{"cell_type":"markdown","source":["After undersampling the size of the dataset is 536 which is reduced by 232. "],"metadata":{"id":"zecYFqXUbVwz"}},{"cell_type":"code","execution_count":null,"source":["from collections import Counter\n","print('Original dataset shape {}'.format(Counter(Y)))\n","print('Resampled dataset shape {}'.format(Counter(y_res)))"],"outputs":[],"metadata":{"id":"fTQ32BRmbXjm"}},{"cell_type":"markdown","source":["We can see the comparison between original target values and undersampled values. It is seen that both the classes have an equal number and it is the number of the original class containing lower samples(class 1 in this case).\n","\n","Undersampling is also referred to as downsampling as it reduces the number of samples. This method should only be used for large datasets as otherwise there’s a huge loss of data, which is not good for the model. Losing out on data is not appropriate as it could hold important information regarding the dataset."],"metadata":{"id":"bBTobgm6bgXl"}},{"cell_type":"markdown","source":["### **Oversampling**\n"],"metadata":{"id":"rIk3yChybpW0"}},{"cell_type":"markdown","source":["\n","Oversampling is just the opposite of undersampling. Here the class containing less data is made equivalent to the class containing more data. This is done by adding more data to the least sample containing class. Let’s take the same example of undersampling, then, in this case, class A will remain 900 and class B will also be 900 (which was previously 100). Hence the ratio will be 1:1 and it’ll be balanced.\n","\n","The imblearn library contains an over_sampling module which contains various libraries to achieve oversampling. Out of those, I’ve shown the performance of the RandomOverSampler module."],"metadata":{"id":"9men9gsFbofm"}},{"cell_type":"code","execution_count":null,"source":["from imblearn.over_sampling import RandomOverSampler\n","os =  RandomOverSampler()\n","X_train_res, y_train_res = os.fit_sample(X, Y)\n","X_train_res.shape,y_train_res.shape"],"outputs":[],"metadata":{"id":"174hQ9RvbndI"}},{"cell_type":"markdown","source":["After oversampling, size of dataset becomes 1000 which was originally 768"],"metadata":{"id":"qyEoycRGbvH7"}},{"cell_type":"code","execution_count":null,"source":["print('Original dataset shape {}'.format(Counter(Y)))\n","print('Resampled dataset shape {}'.format(Counter(y_train_res)))"],"outputs":[],"metadata":{"id":"LL29rP7-bx3z"}},{"cell_type":"markdown","source":["We can see the comparison between original target values and oversampled values. It is seen that both the classes have an equal number and it is the number of the original class containing higher samples(class 0 in this case).\n","\n","Oversampling is also referred to as upsampling as it increases the number of samples. This method should primarily be used in the small or medium-sized dataset. It is better than undersampling as there is no loss of data instead more data is added, which can prove to be good for the model. "],"metadata":{"id":"e67bulQob0gM"}},{"cell_type":"markdown","source":["### **SMOTETomek**"],"metadata":{"id":"RI63ZW0-b7Es"}},{"cell_type":"markdown","source":["SMOTETomek is somewhere upsampling and downsampling. SMOTETomek is a hybrid method which is a mixture of the above two methods, it uses an under-sampling method (Tomek) with an oversampling method (SMOTE). This is present within imblearn.combine module"],"metadata":{"id":"Qozt6holdNQn"}},{"cell_type":"code","execution_count":null,"source":["from imblearn.combine import SMOTETomek\n","smk = SMOTETomek()\n","X_res,y_res=smk.fit_sample(X,Y)\n","X_res.shape,y_res.shape"],"outputs":[],"metadata":{"id":"YeDiYUiNdOIZ"}},{"cell_type":"markdown","source":["Here the dataset size after resampling is increased to 944 from 768."],"metadata":{"id":"WW1CzRdJdiTR"}},{"cell_type":"code","execution_count":null,"source":["print('Original dataset shape {}'.format(Counter(Y)))\n","print('Resampled dataset shape {}'.format(Counter(y_res)))"],"outputs":[],"metadata":{"id":"xOEqUdRYdlFk"}},{"cell_type":"markdown","source":["Class 0 has been downsampled from 500 to 472 and class 1 has been upsampled from 268 to 472.\n","\n","Note that using both techniques together in smotetomek we get the same ratio of samples as 1:1 which is balanced"],"metadata":{"id":"Sdt3M96Gb7B_"}},{"cell_type":"markdown","source":["### **SMOTEEENN**"],"metadata":{"id":"6OkwkzTRdvaR"}},{"cell_type":"markdown","source":["SMOTEENN another library present within imblearn.combine module. This performs similar to SMOTETomek, there is some difference in results between the two methods."],"metadata":{"id":"1T5rQc3VdxyX"}},{"cell_type":"code","execution_count":null,"source":["from imblearn.combine import SMOTEENN\n","smk = SMOTEENN()\n","X_res,y_res=smk.fit_sample(X,Y)\n","X_res.shape,y_res.shape"],"outputs":[],"metadata":{"id":"QJFKWDJ5b66b"}},{"cell_type":"markdown","source":["Here the dataset size after resampling is decreased to 532 from 768"],"metadata":{"id":"9mkiR1h_d5NH"}},{"cell_type":"code","execution_count":null,"source":["print('Original dataset shape {}'.format(Counter(Y)))\n","print('Resampled dataset shape {}'.format(Counter(y_res)))"],"outputs":[],"metadata":{"id":"QbHNFvZ3b64Y"}},{"cell_type":"markdown","source":["Class 0 has been downsampled from 500 to 223 and class 1 has been upsampled from 268 to 309.\n","\n","Unlike SMOTETomek the ratio is not 1:1 but the difference between the samples is not very large.\n","\n","In all these above 4 techniques there is an important parameter called sampling_strategy, this simply means how the resampling should be done. By default, it’s set to ‘auto’.\n","\n","The other available options are:\n","\n","> ‘minority’ – resampling done only to the minority class.\n","\n","> ‘not majority’ – resample all classes except the majority class. This is the same as ‘auto’.\n","\n","> ‘not minority’ – resample all classes except minority class.\n","\n","> ‘all’ – resample all classes."],"metadata":{"id":"A0zjp2AQd-9V"}},{"cell_type":"markdown","source":["#**Please refer related articles -**\n","> * [Techniques to deal with Imbalance Dataset](https://analyticsindiamag.com/top-ways-to-manage-imbalanced-classes-in-dataset/)\n","> * [Correcting Class Imbalanced Data For Binary Classification Problems](https://analyticsindiamag.com/correcting-class-imbalanced-data-for-binary-classification-problems-demonstrations-using-animated-videos/)\n","\n","> * To know more about imblearn technique, refer [here](https://analyticsindiamag.com/what-is-imblearn-technique-everything-to-know-for-class-imbalance-issues-in-machine-learning/)."],"metadata":{"id":"WqcM1-9dTOBx"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"o58hkP56UWAj"}}]}