{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_MultiClass_Classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMjDsULZ+S9/ntzU0+XBxTE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multiclass Classification"],"metadata":{"id":"buWmntQCuBAu"}},{"cell_type":"markdown","source":["A classification problem including more than two classes, such as classifying a series of dog breed photographs which may be a pug, bulldog, or teabetain mastiff. Multi-class classification assumes that each sample is assigned to one class, e.g. a dog can be either a breed of pug or a bulldog but not both simultaneously.\n","\n","Many approaches are used to solve this problem, such as converting the N number of classes to N number binary columns representing each class. By doing so, we can use a binary classifier for Multi Classification problems. Pandas built-in get_dummies attribute provides you with   this functionality to get a binary representation of each class. "],"metadata":{"id":"n_DttEtluDeV"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/guide-to-multi-class-classification/) article."],"metadata":{"id":"C4tnllKBuEEZ"}},{"cell_type":"markdown","source":["# Code Implementation of Multi-Class Classification"],"metadata":{"id":"DcVymaztuOs5"}},{"cell_type":"markdown","source":["Importing all dependencies:"],"metadata":{"id":"ROsregcquQpw"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q \n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.datasets import load_digits\n","from sklearn.metrics import plot_confusion_matrix,accuracy_score,classification_report"],"outputs":[],"metadata":{"id":"rvsCTksP5G7i"}},{"cell_type":"markdown","source":["This demonstration focuses on performing multi-classification uniquely to avoid class imbalance and uncertainty in the dataset; I have used the built-in digits dataset provided by sklearn. Which has 64 different pixel values of particular digits as input feature columns and targeted digits range from 0 to 9. "],"metadata":{"id":"sZMY9T9guSsf"}},{"cell_type":"markdown","source":["Loading the dataset "],"metadata":{"id":"Q3MdULdtuWdx"}},{"cell_type":"code","execution_count":null,"source":["# loading dataset into X, y frame \n","x, y = load_digits(return_X_y=True)  "],"outputs":[],"metadata":{"id":"Ssd-g6kT5aHn"}},{"cell_type":"markdown","source":["Selecting input-output features and train test split:\n","\n","As the number of instances is around 1800, only to ensure more data is used for training here, we have set the test size to be 25%"],"metadata":{"id":"BTuiPZYnuamu"}},{"cell_type":"code","execution_count":null,"source":["# as the no of instances are only 1800 therefore test_size is limited to 25%\n","# so that more data can be made available for training\n","x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = True, test_size = 0.2)"],"outputs":[],"metadata":{"id":"WM-rZz6M6m-m"}},{"cell_type":"markdown","source":["### Lets Findout optimum value of K using Elbow method"],"metadata":{"id":"PbJttZ6M6see"}},{"cell_type":"markdown","source":["To set the optimum value of K here, I have used the elbow method; the plot below shows the error rate against 100 values of K. Error rate is calculated by taking the mean of where the predicted value is not equal to the actual value."],"metadata":{"id":"jDmRJSGPxERz"}},{"cell_type":"code","execution_count":null,"source":["error_rate=[]\n","for i in range(1,100):\n","    knn = KNeighborsClassifier(n_neighbors=i)\n","    model = knn.fit(x_train,y_train)\n","    pred_i = knn.predict(x_test)\n","    error_rate.append(np.mean(pred_i != y_test)) "],"outputs":[],"metadata":{"id":"pFQ4dm8b6ptm"}},{"cell_type":"code","execution_count":null,"source":["plt.figure(figsize=(13,8))\n","plt.plot(range(1,100), error_rate, linestyle = 'dotted', marker = 'o',color = 'g')\n","plt.xlabel('K value')\n","plt.ylabel('Error Rate')\n","plt.title('K value Vs Error Rate')\n","plt.show()"],"outputs":[],"metadata":{"id":"pZug4ZF56w5e"}},{"cell_type":"markdown","source":["From the above graph, we can see that from the initial point to the K values equals to 8 and 9, the error rate decreases; afterwards, it tends to increase and saturate for some K values, and this pattern is being followed throughout the graph. "],"metadata":{"id":"9OUfRKIYxIPV"}},{"cell_type":"markdown","source":["From the graph, we can see that for K value equal 8 and 9, the error given by the model is nearly zero, so we can choose K value as 8 or 9 for our final model.\n","\n","\n","\n","Final Model and performance metrics :"],"metadata":{"id":"qRnEBAadxJ6C"}},{"cell_type":"code","execution_count":null,"source":["model = KNeighborsClassifier(n_neighbors=8).fit(x_train,y_train)\n","pred = model.predict(x_test)"],"outputs":[],"metadata":{"id":"0bFIW9yb60ae"}},{"cell_type":"code","execution_count":null,"source":["plot_confusion_matrix(model,x_test,y_test,cmap=plt.cm.Blues)"],"outputs":[],"metadata":{"id":"7-EgM9pS60eP"}},{"cell_type":"code","execution_count":null,"source":["print(classification_report(y_test,pred))"],"outputs":[],"metadata":{"id":"QCFjHiuu65V2"}},{"cell_type":"markdown","source":["Almost all the samples are predicted correctly. The classification report has also given evidence that our model is perfectly fit for the dataset. "],"metadata":{"id":"xMr5HOMixQq-"}}]}