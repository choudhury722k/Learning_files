{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"3_Logistic_Regression_Breast_Cancer_Dataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYA0UzNdB5luBdjrZEUzIz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Beginners Guide to Logistic Regression in Python\n","This notebook discusses Logistic Regression and the math behind it with a practical example and Python codes. Logistic regression is one of the fundamental algorithms meant for classification. Logistic regression is meant exclusively for binary classification problems. Nevertheless, multi-class classification can also be performed with this algorithm with some modifications.\n","\n","References:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","\n","https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit\n"],"metadata":{"id":"d7tEto53KHGb"}},{"cell_type":"markdown","source":["# Define a Binary Classification Problem"],"metadata":{"id":"-xZ1Vl-j5cqK"}},{"cell_type":"markdown","source":["Create Environment by importing necessary libraries"],"metadata":{"id":"aP6RUjp5ifz0"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","\n","sns.set_style('darkgrid')"],"outputs":[],"metadata":{"id":"3U_k26X3KCh5"}},{"cell_type":"markdown","source":["Load a binary classification problem from SciKit-Learn’s in-built datasets. The breast cancer data is a binary classification problem with two classes. Download the data and metadata using the following code."],"metadata":{"id":"dj2XVj_b6gbu"}},{"cell_type":"code","execution_count":null,"source":["raw_data = load_breast_cancer()\n","\n","raw_data.keys()"],"outputs":[],"metadata":{"id":"6OKy30fVhPEB"}},{"cell_type":"markdown","source":["Read Description"],"metadata":{"id":"kktY1wh6ittI"}},{"cell_type":"markdown","source":["We can read more about the loaded data using the DESCR file."],"metadata":{"id":"9W2Dexzb6kQp"}},{"cell_type":"code","execution_count":null,"source":["print(raw_data['DESCR'])"],"outputs":[],"metadata":{"id":"U21S1rSnivay"}},{"cell_type":"markdown","source":["The dataset contains 30 features and one target. Target has two classes: Malignant (cancerous state) and Benign (non-cancerous state).\n","\n","Create a pandas dataframe for the features and a pandas series for the target."],"metadata":{"id":"XrZBOnm-6nw_"}},{"cell_type":"code","execution_count":null,"source":["data = pd.DataFrame(raw_data['data'], columns=raw_data['feature_names'])\n","target = pd.Series(raw_data['target'], name='target')\n","data.head()"],"outputs":[],"metadata":{"id":"Gs6LdjHPkMqm"}},{"cell_type":"markdown","source":["For more clarity, we proceed with only five selected features."],"metadata":{"id":"OwYdr4n76ps2"}},{"cell_type":"code","execution_count":null,"source":["merge = pd.concat([data, target], axis=1)\n","merge.head()"],"outputs":[],"metadata":{"id":"Nxp9Y1G7lrPP"}},{"cell_type":"markdown","source":["Plot the data to understand inter-relationship"],"metadata":{"id":"TfixSPK8w8CH"}},{"cell_type":"code","execution_count":null,"source":["columns = list(data.columns)\n","plt.figure(figsize=(10,12))\n","k=1\n","for col in columns:\n","  plt.subplot(6,5,k)\n","  sns.scatterplot(x=col, y='mean radius', hue='target', data=merge)\n","  k+=1\n","plt.tight_layout()\n","plt.show()"],"outputs":[],"metadata":{"id":"KN-7QfEul4gi"}},{"cell_type":"markdown","source":["Select a few independent features to proceed"],"metadata":{"id":"4Y0tbPSdxCF5"}},{"cell_type":"code","execution_count":null,"source":["# selected features\n","features = ['mean radius', 'mean texture', 'mean smoothness', 'mean compactness', 'mean concavity']\n","\n","X = data[features]\n","y = target.copy()\n","X.head()"],"outputs":[],"metadata":{"id":"ZSjGn2iBuus1"}},{"cell_type":"markdown","source":["X has all the features and y has the target. If we model with all of the available data, we could not evaluate our model. Hence, it is mandatory to split the available data into training and validation sets. The training set is used to train the model and the validation set will be used to evaluate the trained model."],"metadata":{"id":"1mJNGo6YxIZZ"}},{"cell_type":"code","execution_count":null,"source":["X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=6)\n"],"outputs":[],"metadata":{"id":"Jx2mJT6BxM2C"}},{"cell_type":"markdown","source":["# Logistic Regression using statsmodels library"],"metadata":{"id":"24zHPzSpYTXt"}},{"cell_type":"markdown","source":["Logistic Regression can be performed using either SciKit-Learn library or statsmodels library. However, the  math concepts can be explored clearly with statsmodels. "],"metadata":{"id":"q2XEQ1oZ7AJl"}},{"cell_type":"code","execution_count":null,"source":["from statsmodels.api import Logit, add_constant\n","\n","# add intercept manually\n","X_train_const = add_constant(X_train)\n","# build model and fit training data\n","model_1 = Logit(y_train, X_train_const).fit()\n","# print the model summary\n","model_1.summary()"],"outputs":[],"metadata":{"id":"iPv1LVPyYYmb"}},{"cell_type":"markdown","source":["The bias and coefficients of the Logit function are calculated by the Logit Regression using Maximum Likelihood Estimation (MLE). The coefficients in the above output are the bias and the five weights respectively.\n","\n","The probability distribution of the logit function for training data can be obtained and visualized using the following codes."],"metadata":{"id":"zozq-7ss68lY"}},{"cell_type":"code","execution_count":null,"source":["# Probability Distribution for Training data\n","prob_train = model_1.predict(X_train_const)\n","\n","# sort the prob dist for visualization\n","sorted_train = sorted(prob_train.values)\n","index_train = np.arange(len(sorted_train))\n","# plot it\n","plt.plot(index_train, sorted_train, '+r')\n","plt.title('Training Data: Probability Distribution', size=14, color='orange')\n","plt.xlabel('Examples (sorted by output value)')\n","plt.ylabel('Probability of Logit function')\n","plt.show()"],"outputs":[],"metadata":{"id":"XflWVkBIcIcj"}},{"cell_type":"markdown","source":["It can be observed that the probability values are pushed close to either 0 or 1. Most of the points are close to 0 or 1, while a few points make the shift from 0 to 1. Moreover, the shift from 0 to 1 is sudden. It helps the model make decisions with more confidence. By default, 0.5 is the decision boundary (or technically called the threshold). Even if this threshold is shifted a little above or below, hardly any point will be differently classified. Let’s predict the probability distribution for the validation data and plot it."],"metadata":{"id":"azEg1PVX7MjR"}},{"cell_type":"code","execution_count":null,"source":["# Probability Distribution for Validation data\n","X_val_const = add_constant(X_val)\n","prob_val = model_1.predict(X_val_const)\n","\n","sorted_val = sorted(prob_val.values)\n","index_val = np.arange(len(sorted_val))\n","plt.plot(index_val, sorted_val, '+g')\n","plt.title('Validation Data: Probability Distribution', size=14, color='orange')\n","plt.xlabel('Examples (sorted by output value)')\n","plt.ylabel('Probability of Logit function')\n","plt.show()"],"outputs":[],"metadata":{"id":"8b9mwHWjcp8v"}},{"cell_type":"markdown","source":["Because of this continuous transition of predicted values from 0 to 1, Logistic Regression is called so, but not Logistic Classification.\n","\n","Let’s perform classification using the probability distribution. Define 0.5 as threshold and classify data points either as 0 or 1."],"metadata":{"id":"nrc5zC0v7Qbw"}},{"cell_type":"code","execution_count":null,"source":["threshold = 0.5\n","y_pred = (prob_val > threshold).astype(np.int8)"],"outputs":[],"metadata":{"id":"PrKfaHIsoKxa"}},{"cell_type":"markdown","source":["Evaluate the model using Accuracy score."],"metadata":{"id":"58M5UL_u7SkY"}},{"cell_type":"code","execution_count":null,"source":["metrics.accuracy_score(y_val,y_pred)"],"outputs":[],"metadata":{"id":"EgZrQJYGoiqw"}},{"cell_type":"code","execution_count":null,"source":["print(metrics.classification_report(y_val, y_pred))"],"outputs":[],"metadata":{"id":"ZPt399qYJheK"}},{"cell_type":"markdown","source":["The confusion matrix may give a better insight on performance."],"metadata":{"id":"joRD4AYm7Z66"}},{"cell_type":"code","execution_count":null,"source":["conf = pd.DataFrame(metrics.confusion_matrix(y_val,y_pred), \n","                    index=['Actual Malignant', 'Actual Benign'], \n","                    columns=['Predicted Malignant', 'Predicted Benign'])\n","conf"],"outputs":[],"metadata":{"id":"8HYLTdn_orAt"}},{"cell_type":"markdown","source":["It is observed that totally 9 data points are misclassified among 114."],"metadata":{"id":"odB2tuzU7cs1"}},{"cell_type":"markdown","source":["We can try different threshold values manually to check model performance."],"metadata":{"id":"885VDhcy7ftg"}},{"cell_type":"code","execution_count":null,"source":["accuracies = []\n","thresholds = np.arange(0.0, 1.01, 0.05)\n","for th in thresholds:\n","  y_preds = (prob_val > th).astype(np.int8)\n","  acc = metrics.accuracy_score(y_val,y_preds)\n","  accuracies.append(acc)\n"],"outputs":[],"metadata":{"id":"IOSZ2NoovBkR"}},{"cell_type":"code","execution_count":null,"source":["# plot the accuracy values\n","plt.plot(thresholds, accuracies, '*m')\n","plt.xlabel('Threshold')\n","plt.ylabel('Accuracy')\n","plt.show()"],"outputs":[],"metadata":{"id":"PEizexJnvzBO"}},{"cell_type":"markdown","source":["# Using SciKit-Learn Library\n"],"metadata":{"id":"u3PTiFQbH2mG"}},{"cell_type":"markdown","source":["Logistic Regression is performed with a few lines of code using the SciKit-Learn library."],"metadata":{"id":"by3F2U6P7kxW"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.linear_model import LogisticRegression\n","\n","model_2 = LogisticRegression(penalty='none')\n","model_2.fit(X_train, y_train)\n","y_pred_2 = model_2.predict(X_val)\n","metrics.accuracy_score(y_val, y_pred_2)"],"outputs":[],"metadata":{"id":"8liv4_q3H72m"}},{"cell_type":"markdown","source":["Evaluate the model with validation data. Infer predictions with X_train and calculate the accuracy."],"metadata":{"id":"nCL0Jlt7QEeN"}},{"cell_type":"code","execution_count":null,"source":["print(metrics.classification_report(y_val, y_pred_2))"],"outputs":[],"metadata":{"id":"dfHKzw0YLCyD"}},{"cell_type":"markdown","source":["Find the probability distribution"],"metadata":{"id":"NYzYljbeQGhp"}},{"cell_type":"code","execution_count":null,"source":["model_2.predict_proba(X_val)"],"outputs":[],"metadata":{"id":"LW_rALaeLqR4"}},{"cell_type":"markdown","source":["# Compare both libraries"],"metadata":{"id":"hDAIZIOGQMBd"}},{"cell_type":"code","execution_count":null,"source":["# y_pred is the prediction of statsmodels library\n","# y_pred_2 is the prediction of sklearn libray\n","\n","# Compare both libraries\n","\n","(y_pred == y_pred_2).all()"],"outputs":[],"metadata":{"id":"9tryhmGhPJTn"}},{"cell_type":"markdown","source":["Please refer these articles:\n","\n","> * [Beginners Guide to Logistic Regression](https://analyticsindiamag.com/beginners-guide-to-logistic-regression-in-python/)\n","\n","> * [Important Regression Techniques](https://analyticsindiamag.com/a-beginners-guide-to-regression-techniques/) \n","\n","\n","> * [Fake News Classification](https://analyticsindiamag.com/hands-on-guide-to-predict-fake-news-using-logistic-regression-svm-and-naive-bayes-methods/)"],"metadata":{"id":"vCAZDemxiPHd"}}]}