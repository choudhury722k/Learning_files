{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"4_Regularization_Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **What is Regularization?**"],"metadata":{"id":"fsEH0PqYwunn"}},{"cell_type":"markdown","source":["\n","Overfitting is one of the most annoying things about a Machine Learning model. After all those time-consuming processes that took to gather the data, clean and preprocess it, the model is still incapable to give out an optimised result.  There can be lots of noises in data which may be the variance in the target variable for the same and exact predictors or irrelevant features or it can be corrupted data points. The ML model is unable to identify the noises and hence uses them as well to train the model. This can have a negative impact on the predictions of the model. This is called overfitting.\n","\n","In simple words, overfitting is the result of an ML model trying to fit everything that it gets from the data including noises."],"metadata":{"id":"uyqIOnqRw02E"}},{"cell_type":"markdown","source":["Please refer [this](https://analyticsindiamag.com/lasso-regression-in-python-with-machinehack-data-science-hackathon/) post to read about it more."],"metadata":{"id":"broLkmv8xsWY"}},{"cell_type":"markdown","source":["## **Ridge Regression**"],"metadata":{"id":"9CAv7K7Py4Mk"}},{"cell_type":"markdown","source":["It is also called an L2 regularization that is used to get rid of overfitting. The goal while building a machine learning model is to develop a model that can generalize patterns well in training as well as in testing."],"metadata":{"id":"HJvc52KRy8bV"}},{"cell_type":"markdown","source":["Ridge Regression is done to improve the generalizability of the model. This is done by tweaking the slope of the best fit line. Maybe the model does not perform much well in the training because now the line does not pass exactly to the data points but it will give fairly good results in testing. The slope is changed or the line is titled a bit by making use of the penalty term called Alpha which is a hyperparameter. Linear regression aims to reduce the sum of squared errors whereas in ridge regression it also reduces the sum of squared error but adds this penalty term by multiplying it with slope square."],"metadata":{"id":"1I_CFaDQ04u4"}},{"cell_type":"markdown","source":["**Linear regression = min(Sum of squared errors)**\n","\n","**Ridge regression = min(Sum of squared errors + alpha * slope)square)**\n","\n","As the value of alpha increases, the lines gets horizontal and slope reduces as shown in the below graph."],"metadata":{"id":"CW3DjzZV07H3"}},{"cell_type":"markdown","source":["## **Lasso Regression**"],"metadata":{"id":"-WUjCVMd1EvU"}},{"cell_type":"markdown","source":["It is also called as l1 regularization. Similar to ridge regression, lasso regression also works in a similar fashion the only difference is of the penalty term. In ridge, we multiply it by slope and take the square whereas in lasso we just multiply the alpha with absolute of slope. \n","\n","**Lasso Regression = min(sum of squared error + alpha * | slope| )**"],"metadata":{"id":"xS3oOui82d8J"}},{"cell_type":"markdown","source":["Similar to ridge regression as you increase the value of the penalty term the slope will get reduced and the line will become horizontal. As this term is increased it becomes less responsive to the independent variable. "],"metadata":{"id":"0eQ7HpFN2jxh"}},{"cell_type":"markdown","source":["## **Implementation**"],"metadata":{"id":"r_mUB30b9LQy"}},{"cell_type":"markdown","source":["First, we will import all the required libraries and the data set. After importing we will explore a bit data like shape and about missing values present in the data set. Use the below code to do the same."],"metadata":{"id":"wf76jjvk9sgT"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q \n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\n","\n","from sklearn.linear_model import LinearRegression\n","\n","from sklearn.linear_model import Ridge\n","\n","from sklearn.linear_model import Lasso\n","\n","from sklearn.datasets import load_boston\n","boston_data = load_boston()\n","df = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n","df['target'] = pd.Series(boston_data.target)\n","df.head()"],"outputs":[],"metadata":{"id":"dkKWwYbfNxhR"}},{"cell_type":"code","execution_count":null,"source":["print(df.shape)\n","\n","print(df.isnull().sum())"],"outputs":[],"metadata":{"id":"WtpP-FPY_GLg"}},{"cell_type":"markdown","source":["The data set contains 506 rows and 14 columns. There are no missing values that are found in the data. We will not divide the dependent variable and independent variable X and y respectively followed by scaling the data and then dividing it into training and testing sets. Use the below code to do so."],"metadata":{"id":"wpO0cvBB_ZMZ"}},{"cell_type":"code","execution_count":null,"source":["X = df.drop('target', axis=1)\n","\n","y = df['target']\n","\n","from sklearn import preprocessing\n","\n","X = preprocessing.scale(X)\n","\n","y = preprocessing.scale(y)\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n","\n","print(X_train.shape)\n","\n","print(y_train.shape)\n","\n","print(X_test.shape)\n","\n","print(y_test.shape)"],"outputs":[],"metadata":{"id":"jouT1X9L_z_T"}},{"cell_type":"markdown","source":["There are a total of 354 rows in the training data set and 152 are present in the testing data. We now build three models using simple linear regression, ridge regression and lasso regression and fit the data for training. After the model gets trained we will compute the scores for testing and training. Use the below code for the same."],"metadata":{"id":"2yz3uMW2_8WK"}},{"cell_type":"code","execution_count":null,"source":["regression_model = LinearRegression()\n","\n","regression_model.fit(X_train, y_train)\n","\n","ridge = Ridge(alpha=.3)\n","\n","ridge.fit(X_train,y_train)\n","\n","print (\"Ridge model:\", (ridge.coef_))"],"outputs":[],"metadata":{"id":"YPOAnnV8AJhI"}},{"cell_type":"code","execution_count":null,"source":["lasso = Lasso(alpha=0.1)\n","\n","lasso.fit(X_train,y_train)\n","\n","print (\"Lasso model:\", (lasso.coef_))"],"outputs":[],"metadata":{"id":"xsZmm4PkANFS"}},{"cell_type":"code","execution_count":null,"source":["print(\"Linear Regression Model Training Score: \", regression_model.score(X_train, y_train))\n","\n","print(\"Linear Regression Model Testing Score: \",regression_model.score(X_test, y_test))\n","\n","print(\"Ridge Regression Model Training Score: \",ridge.score(X_train, y_train))\n","\n","print(\"Ridge Regression Model Testing Score: \",ridge.score(X_test, y_test))\n","\n","print(\"Lasso Regression Model Training Score: \",lasso.score(X_train, y_train))\n","\n","print(\"Lasso Regression Model Testing Score: \",lasso.score(X_test, y_test))"],"outputs":[],"metadata":{"id":"pC71gTYsAPKi"}},{"cell_type":"markdown","source":["The results are almost identical but with less complexity of the models. We will now create a polynomial regression model by creating new features from the features followed by transforming the data and dividing it into training and testing. Use the below code to do so. "],"metadata":{"id":"HTfx8gDqAYv8"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","poly = PolynomialFeatures(degree = 2, interaction_only=True)\n","\n","X_poly = poly.fit_transform(X)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.30, random_state=1)\n","\n","regression_model.fit(X_train, y_train)\n","\n","print(regression_model.coef_[0])"],"outputs":[],"metadata":{"id":"4NnywAQMAZpT"}},{"cell_type":"code","execution_count":null,"source":["ridge = Ridge(alpha=.3)\n","\n","ridge.fit(X_train,y_train)\n","\n","print (\"Ridge model:\", (ridge.coef_))"],"outputs":[],"metadata":{"id":"M2kJeQSSAlz5"}},{"cell_type":"code","execution_count":null,"source":["lasso = Lasso(alpha=0.003)\n","\n","lasso.fit(X_train,y_train)\n","\n","print (\"Lasso model:\", (lasso.coef_))"],"outputs":[],"metadata":{"id":"8EU69ZBlAni5"}},{"cell_type":"markdown","source":["We will now check the scores of the polynomial model and compute the training and testing scores. Use the below code to do so."],"metadata":{"id":"5kewkwX4At4m"}},{"cell_type":"code","execution_count":null,"source":["print(\"Linear Regression Model Training Score: \", regression_model.score(X_train, y_train))\n","\n","print(\"Linear Regression Model Testing Score: \",regression_model.score(X_test, y_test))\n","\n","print(\"Ridge Regression Model Training Score: \",ridge.score(X_train, y_train))\n","\n","print(\"Ridge Regression Model Testing Score: \",ridge.score(X_test, y_test))\n","\n","print(\"Lasso Regression Model Training Score: \",lasso.score(X_train, y_train))\n","\n","print(\"Lasso Regression Model Testing Score: \",lasso.score(X_test, y_test))"],"outputs":[],"metadata":{"id":"lAjEj8gAAuik"}},{"cell_type":"markdown","source":["Regularization is done to control the performance of the model and to avoid the model to get overfitted. In this article, we discussed the overfitting of the model and two well-known regularization techniques that are Lasso and Ridge Regression. Lasso regression transforms the coefficient values to 0 which means it can be used as a feature selection method and also dimensionality reduction technique. The feature whose coefficient becomes equal to 0 is less important in predicting the target variable and hence it can be dropped.  Ridge regression transforms the coefficient values to close to 0 and not completely equal to 0. "],"metadata":{"id":"2BPBCgT-A3W_"}},{"cell_type":"markdown","source":["Please refer [this](https://analyticsindiamag.com/hands-on-implementation-of-lasso-and-ridge-regression/) to get the complete overview of implementation and refer [this](https://analyticsindiamag.com/ridge-regression-vs-lasso-how-these-2-popular-ml-regularisation-techniques-work/) story to learn more about theoretical aspects of Ridge and Lasso Regularization."],"metadata":{"id":"SxSCpEo5yDH9"}}]}