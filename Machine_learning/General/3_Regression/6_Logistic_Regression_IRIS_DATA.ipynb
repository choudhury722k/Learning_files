{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_Logistic_Regression_IRIS_DATA.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"siYQU2cKe7zN"},"source":["# In this practice session, we will learn to code Logistic Regression Classifier. \n","# We will perform the following steps to build a simple classifier using the popular Iris dataset.\n","\n"," \n"," \n","  - **Data Preprocessing**\n","\n","    - Importing the libraries.\n","    - Importing dataset (Dataset Link https://archive.ics.uci.edu/ml/datasets/iris).\n","    - Dealing with the categorical variable.\n","    - Classifying dependent and independent variables.\n","    - Splitting the data into a training set and test set.\n","    - Feature scaling.\n"," \n","\n","  -  **Logistic Regression Classification**\n","\n","    - Create a Logistic Regression classifier.\n","    - Feed the training data to the classifier.\n","    - Predicting the species for the test set.\n","    - Using the confusion matrix to find accuracy."]},{"cell_type":"markdown","metadata":{"id":"4jDb3eRKe7zU"},"source":["# Load the Dependencies"]},{"cell_type":"code","metadata":{"id":"ucejirLue7zV"},"source":["import ipywidgets as widgets\n","from IPython.display import display\n","\n","style = {'description_width': 'initial'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BZ-jLOunczC"},"source":["#1 Importing essential libraries\n","import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","import seaborn as sns\n","iris = load_iris() \n","data = iris.data \n","target = iris.target \n","names = iris.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnvkpvNoe7zW"},"source":["# Load the Dataset"]},{"cell_type":"code","metadata":{"id":"WKtERLr9n09B"},"source":["#file_name = 'iris.data'\n","\n","dataset = pd.DataFrame(data, columns=['sepal length', 'sepal width', 'petal length', \"petal width\"])\n","dataset['Species']=target\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TuLujj0ye7zY"},"source":["print(f\"Dataset has {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c33jxPWge7zY"},"source":["#Plotting the relation between salary and experience\n","wig_col = widgets.Dropdown(\n","                options=[col for col in dataset.columns.tolist() if col.startswith(('sepal', 'petal'))],\n","                description='Choose a Column to Plot vs. Attributes',\n","                disabled=False,\n","                layout=widgets.Layout(width='40%', height='40px'),\n","                style=style)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBqIhHGze7zZ"},"source":["# Plot Variables"]},{"cell_type":"code","metadata":{"id":"oltCmUXYe7zb"},"source":["display(wig_col)\n","\n","sns.catplot(x=\"Species\", y=wig_col.value, kind=\"boxen\", data=dataset, height=8.27, aspect=11.7/8.27);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbCEfY34e7zb"},"source":["g = sns.catplot(x=\"Species\", y=wig_col.value, kind=\"violin\", inner=None, data=dataset, height=8.27, aspect=11.7/8.27)\n","sns.swarmplot(x=\"Species\", y=wig_col.value, color=\"k\", size=3, data=dataset, ax=g.ax);\n","\n","display(wig_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5WuUWRFn4j8"},"source":["#3 classify dependent and independent variables\n","X = dataset.iloc[:,:-1].values  #independent variable YearsofExperience\n","y = dataset.iloc[:,-1].values  #dependent variable salary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bZ82kVbn7Ga"},"source":["print(\"\\nIdependent Variable (Sepal and Petal Attributes):\\n\\n\", X[:5])\n","print(\"\\nDependent Variable (Species):\\n\\n\", y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCpCDwJXe7zd"},"source":["# Encode Classes"]},{"cell_type":"code","metadata":{"id":"DwuYyNNze7ze"},"source":["from sklearn.preprocessing import LabelEncoder\n","labelencoder = LabelEncoder()\n","dataset['Species'] = labelencoder.fit_transform(dataset['Species'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4Dq7IIEe7ze"},"source":["dataset['Species'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rJuQ4oNe7ze"},"source":["# Create Train and Test Sets"]},{"cell_type":"code","metadata":{"id":"BFMIwI5Zn9MX"},"source":["#4 Creating training set and testing set\n","from sklearn.model_selection import train_test_split\n","test_size = widgets.FloatSlider(min=0.01, max=0.6, value=0.2, description=\"Test Size :\", tooltips=['Usually 20-30%'])\n","display(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-qOUtSRe7zf"},"source":["#Divide the dataset into Train and Test sets\n","X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=test_size.value, random_state = 0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcY06YGDn_dz"},"source":["print(\"Training Set :\\n----------------\\n\")\n","print(\"X = \\n\", X_train[:5])\n","print(\"y = \\n\", y_train[:5])\n","\n","print(\"\\n\\nTest Set :\\n----------------\\n\")\n","print(\"X = \\n\",X_test[:5])\n","print(\"y = \\n\", y_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgHlt2s6e7zf"},"source":["print(f\"Shape of Training set is {X_train.shape}\")\n","print(f\"Shape of Testing set is {X_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jNaw6Sne7zf"},"source":["# Normalise Features\n","\n","As the Features are not in the range of 0-1, Let's normalize the features using Standard Scaler(Z-score) normalization and Label Encode the Class String Names."]},{"cell_type":"code","metadata":{"id":"5fAgLnP0e7zg"},"source":["#Feature scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test) \n","\n","print(\"\\n-------------------------\\nDataset after Scaling:\\n-------------------------\\n\", )\n","\n","print(\"\\nX_train :\\n\", X_train[:5])\n","print(\"-------------------------\")\n","print(\"\\nX_test :\\n\", X_test[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQ5p1f_Qe7zg"},"source":["# Logistic Regression"]},{"cell_type":"code","metadata":{"id":"PmDDhoone7zg"},"source":["# import Logistic Regression library\n","from sklearn.linear_model import LogisticRegression\n","\n","# configure params for the model.\n","penalty_wig = widgets.Dropdown(options=[\"l2\", \"l1\"], \n","                             description=\"Penalty Type = \", style=style)\n","\n","display(penalty_wig)\n","\n","njobs_wig = widgets.Dropdown(options=[('One', 1), ('Two', 2), ('Three', 3), ('All Cores', -1)], \n","                             description=\"Number of CPU Cores = \", style=style)\n","\n","display(njobs_wig)\n","\n","c_wig = widgets.Dropdown(options=[1.0, 10.0, 100.0, 1000.0, 10000.0], \n","                             description=\"Penalty parameter C = \", style=style)\n","\n","display(c_wig)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJwtGktie7zh"},"source":["# Predict and Evaluate the Model "]},{"cell_type":"code","metadata":{"id":"8XbYFyk8oCrH"},"source":["classifier = LogisticRegression(C=c_wig.value, penalty=penalty_wig.value,\n","                                n_jobs=njobs_wig.value, random_state=0)\n","\n","#Feed the training data to the classifier\n","classifier.fit(X_train,y_train)\n","\n","#Predicting the species for test set\n","y_pred = classifier.predict(X_test)\n","\n","print(\"\\n---------------------------\\n\")\n","print(\"Predicted Values for Test Set :\\n\",y_pred)\n","print(\"\\n---------------------------\\n\")\n","print(\"Actual Values for Test Set :\\n\",y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAwiJVWuoHEX"},"source":["#8 Claculating the Accuracy of the predictions\n","from sklearn import metrics\n","print(\"Prediction Accuracy = \", metrics.accuracy_score(y_test, y_pred))\n","\n","#9 Comparing Actual and Predicted Salaries for he test set\n","print(\"\\nActual vs Predicted Salaries \\n------------------------------\\n\")\n","error_df = pd.DataFrame({\"Actual\" : y_test,\n","                         \"Predicted\" : y_pred})\n","\n","error_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8C45CJNZe7zh"},"source":["# Actual vs. Predicted "]},{"cell_type":"code","metadata":{"id":"VeJQ_ygQe7zh"},"source":["#Using confusion matrix to find the accuracy\n","from sklearn.metrics import confusion_matrix, classification_report\n","cm = confusion_matrix(y_test,y_pred)\n","\n","accuracy = cm.diagonal().sum()/cm.sum()\n","\n","print(\"\\n---------------------------\\n\")\n","print(\"Accuracy of Predictions = \",accuracy)\n","\n","print(\"\\n---------------------------\\n\")\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[]}]}