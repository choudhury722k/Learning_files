{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"3_Univariate_and_Multivariate_LinearRegression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM1JXJ5VjVb37GoBofSb+YV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Beginners Guide to Linear Regression  "],"metadata":{"id":"ycFufNkFvOtn"}},{"cell_type":"markdown","source":["Supervised learning is called regression if the dependent variable (aka target) is continuous. Supervised learning is called classification if the dependent variable is discrete. In other words, a regression model outputs a numerical value (a real floating value), but a classification model outputs a class (among two or more classes).\n","\n","In this practice session, we will discuss linear regression and its implementation with python codes. Regression analysis can be specifically termed linear regression if the dependent variable (target) has a linear relationship with the independent variables (features)"],"metadata":{"id":"EWq7x8Gb9nkp"}},{"cell_type":"markdown","source":["To understand the math behind it, please refer [this](https://analyticsindiamag.com/beginners-guide-to-linear-regression-in-python/) article."],"metadata":{"id":"E-dDE1Nw9sWI"}},{"cell_type":"markdown","source":["# Code Implementation"],"metadata":{"id":"ppc-Nlf390hj"}},{"cell_type":"markdown","source":["## Load a Regression Data"],"metadata":{"id":"mKkRfgfI92nJ"}},{"cell_type":"markdown","source":["Import necessary libraries and modules."],"metadata":{"id":"TwE-Pj2i95yp"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import load_diabetes"],"outputs":[],"metadata":{"id":"mcUDwYT0xLah"}},{"cell_type":"markdown","source":["Load a regression problem dataset from SciKit-Learn’s in-built datasets. Data is already preprocessed and normalized, and is ready to use."],"metadata":{"id":"UiMjce7txqbM"}},{"cell_type":"code","execution_count":null,"source":["data = load_diabetes()\n","data.keys()"],"outputs":[],"metadata":{"id":"mv1ZT8z30Kj4"}},{"cell_type":"markdown","source":["Generate features and target. Visualize the top 5 rows of the data."],"metadata":{"id":"NeI5UCEJ9-dV"}},{"cell_type":"code","execution_count":null,"source":["features = pd.DataFrame(data['data'], columns=data['feature_names'])\n","target = pd.Series(data['target'], name='target')\n","features.head()"],"outputs":[],"metadata":{"id":"LafBsECB0sim"}},{"cell_type":"markdown","source":["## Simple Linear Regression"],"metadata":{"id":"gta6AjeJ8lMw"}},{"cell_type":"markdown","source":["Simple linear regression is performed with one dependent variable and one independent variable. In our data, we declare the feature ‘bmi’ to be the independent variable.\n","\n","Prepare X and y."],"metadata":{"id":"WoPQGuuh-DWt"}},{"cell_type":"code","execution_count":null,"source":["X = features['bmi'].values.reshape(-1,1)\n","y = target.values.reshape(-1,1)"],"outputs":[],"metadata":{"id":"LZ9LUUk28kqI"}},{"cell_type":"markdown","source":["Fit the data to the model"],"metadata":{"id":"YB2vq7OQxt6f"}},{"cell_type":"code","execution_count":null,"source":["simple = LinearRegression()\n","simple.fit(X,y)"],"outputs":[],"metadata":{"id":"RE_ZB-iu9Skp"}},{"cell_type":"markdown","source":["The training is completed. We can explore the weight (coefficient) and bias (intercept) of the trained model."],"metadata":{"id":"GUAtb9NRxypA"}},{"cell_type":"code","execution_count":null,"source":["simple.intercept_"],"outputs":[],"metadata":{"id":"RfFW_nMq-ZCS"}},{"cell_type":"code","execution_count":null,"source":["simple.coef_"],"outputs":[],"metadata":{"id":"eCMtWcLP-fDc"}},{"cell_type":"markdown","source":["Calculate the predictions following the formula, y = intercept + X*coefficient."],"metadata":{"id":"q1uvnyfLx4TY"}},{"cell_type":"markdown","source":["Predictions can also be calculated using the trained model."],"metadata":{"id":"kB8eQffz-NfZ"}},{"cell_type":"code","execution_count":null,"source":["calc_pred = simple.intercept_ + (X*simple.coef_)\n","\n","pred = simple.predict(X)"],"outputs":[],"metadata":{"id":"7-L05bJlTQkR"}},{"cell_type":"code","execution_count":null,"source":["(calc_pred == pred).all()"],"outputs":[],"metadata":{"id":"KpSLFR88Tdnw"}},{"cell_type":"markdown","source":["Plot the actual values and predicted values to get a better understanding."],"metadata":{"id":"WUon1r5D-PlR"}},{"cell_type":"code","execution_count":null,"source":["plt.scatter(X,y, label='Actual')\n","plt.plot(X,pred, '-r', label='Prediction')\n","plt.xlabel('Feature X')\n","plt.ylabel('Target y')\n","plt.title('Simple Linear Regression', color='orange', size=14)\n","plt.legend()\n","plt.show()"],"outputs":[],"metadata":{"id":"4GA3_8DB_RDv"}},{"cell_type":"markdown","source":["According to SciKit-Learn’s LinearRegression method, the above red line is the best possible fit with minimal error value. \n","\n","We can calculate the mean squared error value for the above regression using the following code."],"metadata":{"id":"RiCQS961-SHc"}},{"cell_type":"markdown","source":["Mean Squared Error"],"metadata":{"id":"GvXeROQqx6v7"}},{"cell_type":"code","execution_count":null,"source":["mean_squared_error(y, pred)"],"outputs":[],"metadata":{"id":"AGBsINqETKnr"}},{"cell_type":"markdown","source":["R-Squared value"],"metadata":{"id":"WG-tjOpLx9P4"}},{"cell_type":"markdown","source":["CoD gives the ratio of the regression sum of square to the total sum of the square. Total sum of squares (SST) is the sum of deviations of each y value from the mean value of y. Regression sum of squares (SSR) is the difference between the total sum of squares and the sum of squared error (SSE). When there is no error (MSE = 0), CoD becomes unity. When the sum of squared error equals the total sum of squares (SSE = SST), CoD becomes zero.\n","\n","CoD = 1 refers to the best prediction\n","\n","CoD = 0 refers to the worst prediction\n","\n","CoD gives a limit [0,1], thus makes the predictions comparable. CoD is also called R-squared value. It can be calculated using the following code."],"metadata":{"id":"SuYxPpdi-doL"}},{"cell_type":"code","execution_count":null,"source":["simple.score(X,y)"],"outputs":[],"metadata":{"id":"tjhzMn_bRs-F"}},{"cell_type":"markdown","source":["## Multiple Linear Regression "],"metadata":{"id":"XI-dZMCVd6W8"}},{"cell_type":"markdown","source":["Multiple linear regression is performed with more than one independent variable. We choose the following columns as our features."],"metadata":{"id":"LTBbWyYY-g7k"}},{"cell_type":"code","execution_count":null,"source":["columns = ['age', 'bmi', 'bp', 's3', 's5']\n","columns"],"outputs":[],"metadata":{"id":"zYke77Ug1a_W"}},{"cell_type":"markdown","source":["Visuaize the data"],"metadata":{"id":"sIUzyWc1yD08"}},{"cell_type":"code","execution_count":null,"source":["for i in columns:\n","  plt.scatter(features[i], y)\n","  plt.xlabel(str(i))\n","  plt.show() "],"outputs":[],"metadata":{"id":"VK24VTH_yjHR"}},{"cell_type":"markdown","source":["It is observed that each individual feature has scatteredness in nature. But, the variation in target values for a single input feature value may be explained by some other features. In other words, the target value may find difficulty in fitting a linear regression model with a single feature. Nevertheless, it may yield an improved fit with multiple features by exploring the true pattern in the data."],"metadata":{"id":"exvwkR38-l7j"}},{"cell_type":"markdown","source":["In the simple linear regression implementation, we have used all our data to fit the model. But, how can we test our model? How far will our model perform on unforeseen data? This is where the train-test-split comes into play. We split our dataset into two sets: a training set and a validation set. We train our model with training data only and evaluate it with the validation set."],"metadata":{"id":"CyeHY_OY-oXu"}},{"cell_type":"markdown","source":["Perform Train-Validation split"],"metadata":{"id":"DGJjVYVCyHR5"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\n","\n","X = features[columns]\n","# 70% training data, 30% validation data\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=6)"],"outputs":[],"metadata":{"id":"LM5cUcMTkQQa"}},{"cell_type":"markdown","source":["Build a linear regression model and fit the data."],"metadata":{"id":"9LUckZUt-sjb"}},{"cell_type":"code","execution_count":null,"source":["multi = LinearRegression()\n","multi.fit(X_train, y_train)"],"outputs":[],"metadata":{"id":"ZS3byeAck50S"}},{"cell_type":"markdown","source":["What are the weights (coefficients) of our model? There should be five coefficients each corresponding to each feature."],"metadata":{"id":"_71ju0JQyOi0"}},{"cell_type":"code","execution_count":null,"source":["multi.coef_"],"outputs":[],"metadata":{"id":"KZb7k2bklOIG"}},{"cell_type":"code","execution_count":null,"source":["multi.intercept_"],"outputs":[],"metadata":{"id":"H3-dtc9dl7X8"}},{"cell_type":"markdown","source":["Predictions, error and R-squared value"],"metadata":{"id":"WctJNmX0yUnk"}},{"cell_type":"code","execution_count":null,"source":["pred = multi.predict(X_val)\n"],"outputs":[],"metadata":{"id":"q1AXp29LmsRq"}},{"cell_type":"code","execution_count":null,"source":["mean_squared_error(y_val, pred)"],"outputs":[],"metadata":{"id":"zOyYSyiJntD5"}},{"cell_type":"code","execution_count":null,"source":["multi.score(X_train, y_train), multi.score(X_val, y_val)"],"outputs":[],"metadata":{"id":"xJfqehskn9iv"}},{"cell_type":"markdown","source":["## Using statsmodels Library "],"metadata":{"id":"HKmhilMnqVdm"}},{"cell_type":"markdown","source":["We have used the SciKit-Learn library so far to perform linear regression. However, we can use the statsmodels library to perform the same task. Fit the training data on the OLS (Ordinary Least Squares) model available in the statsmodels library."],"metadata":{"id":"OMDIqMYMyaLq"}},{"cell_type":"code","execution_count":null,"source":["import statsmodels.api as sm\n","X_train = sm.add_constant(X_train)\n","model = sm.OLS(y_train, X_train).fit()\n","model.summary()"],"outputs":[],"metadata":{"id":"dMg-Ra-rrCTc"}},{"cell_type":"markdown","source":["Prediction, error calculation"],"metadata":{"id":"NxwVHFx3yfvG"}},{"cell_type":"markdown","source":["It can be observed that the model weights, intercept and the R-squared value are all identical to the Linear Regression method of the SciKit-Learn library.\n","\n","The model can be implemented to make predictions on validation data too."],"metadata":{"id":"WOEMCYIc-4RP"}},{"cell_type":"code","execution_count":null,"source":["# Constant (intercept) must be added manually\n","X_val = sm.add_constant(X_val)\n","preds = model.predict(X_val)\n"],"outputs":[],"metadata":{"id":"l9pxNBVHtbLC"}},{"cell_type":"code","execution_count":null,"source":["mean_squared_error(y_val, preds)"],"outputs":[],"metadata":{"id":"VXa3qFzPvNjO"}},{"cell_type":"markdown","source":["Both methods perform identically"],"metadata":{"id":"5COODKtQymj-"}}]}