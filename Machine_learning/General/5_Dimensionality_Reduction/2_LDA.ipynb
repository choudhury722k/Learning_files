{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2_LDA.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMu8iotfnd/f6rQ2/Jatk9w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Latent Discriminant Allocation(LDA)**"],"metadata":{"id":"8uUJQPr6dq84"}},{"cell_type":"markdown","source":["Linear Discriminant Analysis (LDA) is used to find a linear combination of features that characterizes or separates two or more classes of objects or events. It explicitly attempts to model the difference between the classes of data. It works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis."],"metadata":{"id":"jsxfz16FdxxJ"}},{"cell_type":"markdown","source":["## **Practical Implementation**"],"metadata":{"id":"FiThpD3zeFVK"}},{"cell_type":"markdown","source":["In this implementation, we have used the wine classification dataset, which is publicly available on Kaggle. Follow the steps below:-"],"metadata":{"id":"afkjLOvteJsq"}},{"cell_type":"markdown","source":["### **Import the required libraries**"],"metadata":{"id":"xMmpb1fweLzV"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q\n","!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels scikit-image --user -q"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#1. Import the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"outputs":[],"metadata":{"id":"Z1bh6x2gMvZB"}},{"cell_type":"markdown","source":["### **Loading the dataset**"],"metadata":{"id":"muJdQOBnVSQT"}},{"cell_type":"code","execution_count":null,"source":["#2. Import the dataset\n","dataset = pd.read_csv('Wine.csv', header=None)\n","\n","\n","dataset.columns = [  'name'\n","                 ,'alcohol'\n","             \t,'malicAcid'\n","             \t,'ash'\n","            \t,'ashalcalinity'\n","             \t,'magnesium'\n","            \t,'totalPhenols'\n","             \t,'flavanoids'\n","             \t,'nonFlavanoidPhenols'\n","             \t,'proanthocyanins'\n","            \t,'colorIntensity'\n","             \t,'hue'\n","             \t,'od280_od315'\n","             \t,'proline'\n","                ]\n","dataset.head()"],"outputs":[],"metadata":{"id":"qahT_EW0VWw5"}},{"cell_type":"markdown","source":["We need to store the independent and dependent variables by using the iloc method."],"metadata":{"id":"Hhjv-9xmWcH6"}},{"cell_type":"code","execution_count":null,"source":["X = dataset.iloc[:, 1:].values\n","y = dataset.iloc[:, 0].values"],"outputs":[],"metadata":{"id":"XTG9UyjqWc0c"}},{"cell_type":"markdown","source":["Split the training and testing data in the 80:20 ratio."],"metadata":{"id":"E9d11D0bWfsf"}},{"cell_type":"code","execution_count":null,"source":["#3. Split the dataset into Training set and Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"],"outputs":[],"metadata":{"id":"Npcs7fdaWgmT"}},{"cell_type":"markdown","source":["### **Feature Scaling**"],"metadata":{"id":"xX6oookZWi2P"}},{"cell_type":"markdown","source":["It is important to convert all the data into numerical format. We need to standardize data for converting features of different units to the same unit."],"metadata":{"id":"ejmNjvRbWovE"}},{"cell_type":"code","execution_count":null,"source":["#4. Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"],"outputs":[],"metadata":{"id":"K2ljxrK5Wink"}},{"cell_type":"markdown","source":["### **Apply LDA**"],"metadata":{"id":"W89DpwsjfpU2"}},{"cell_type":"code","execution_count":null,"source":["#5. Apply LDA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","lda = LDA(n_components = 2)\n","X_train = lda.fit_transform(X_train, y_train)\n","X_test = lda.transform(X_test)"],"outputs":[],"metadata":{"id":"uLFt2KfeWigi"}},{"cell_type":"markdown","source":["### **Fitting Logistic Regression To the training set**"],"metadata":{"id":"BB-kzjx_XKBu"}},{"cell_type":"markdown","source":["As we are solving a classification problem, we can use the Logistic Regression for model prediction."],"metadata":{"id":"m9kMNVGuXQCu"}},{"cell_type":"code","execution_count":null,"source":["#6. Fit Logistic Regression to the Training set\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(random_state = 0)\n","classifier.fit(X_train, y_train)"],"outputs":[],"metadata":{"id":"ihE9JhV8XAgV"}},{"cell_type":"markdown","source":["### **Predict the test Result**"],"metadata":{"id":"iwvZmsg5dCaB"}},{"cell_type":"code","execution_count":null,"source":["#7. Predict the Test set results\n","\n","y_pred = classifier.predict(X_test)"],"outputs":[],"metadata":{"id":"t63TEMIcdGCZ"}},{"cell_type":"markdown","source":["### **Evaluating the Algorithm**"],"metadata":{"id":"G250juP3XqQD"}},{"cell_type":"markdown","source":["For classification tasks, we will use a confusion matrix to check the accuracy of our machine learning model."],"metadata":{"id":"1NZsaXIyXtTi"}},{"cell_type":"code","execution_count":null,"source":["\n","#8. Make the Confusion Matrix\n","\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(y_test, y_pred)\n","cm"],"outputs":[],"metadata":{"id":"bC0UiBqzXt5k"}},{"cell_type":"markdown","source":["### **Plot the training set**"],"metadata":{"id":"taE3kRvVXw4C"}},{"cell_type":"code","execution_count":null,"source":["#9. Visualize the Training set results\n","\n","from matplotlib.colors import ListedColormap\n","X_set, y_set = X_train, y_train\n","X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n","                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n","plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n","             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n","plt.xlim(X1.min(), X1.max())\n","plt.ylim(X2.min(), X2.max())\n","for i, j in enumerate(np.unique(y_set)):\n","    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n","                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n","plt.title('Logistic Regression (Training set)')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.legend()\n","plt.show()"],"outputs":[],"metadata":{"id":"z-0uRUmIX1zk"}},{"cell_type":"markdown","source":["### **Plot the Test Set**"],"metadata":{"id":"vSFfxqj7dWL6"}},{"cell_type":"code","execution_count":null,"source":["#10.Visualize the Test set results\n","\n","from matplotlib.colors import ListedColormap\n","X_set, y_set = X_test, y_test\n","X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n","                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n","plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n","             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n","plt.xlim(X1.min(), X1.max())\n","plt.ylim(X2.min(), X2.max())\n","for i, j in enumerate(np.unique(y_set)):\n","    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n","                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n","plt.title('Logistic Regression (Test set)')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.legend()\n","plt.show()\n","\n"],"outputs":[],"metadata":{"id":"zBDfo5GadaLy"}},{"cell_type":"markdown","source":["#**Related Articles:**\n","\n","> * [PCA in Python](https://analyticsindiamag.com/principal-component-analysis-in-python/)\n","> * [Comparing PCA, LDA and PCA-kernel](https://analyticsindiamag.com/practical-approach-to-dimensionality-reduction-using-pca-lda-and-kernel-pca/)\n","> * [Mathematical Practical Approach to PCA](https://analyticsindiamag.com/principal-component-analysis-on-matrix-using-python/)"],"metadata":{"id":"TJlAo65ERmJj"}}]}