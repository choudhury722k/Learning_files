{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Comparison_of_PCA_LDA_Kernel_PCA_Theory.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMX2jeeeivLk3iB/GC29uqg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-NK1dcOLgn6D"},"source":["# **Comparison of PCA, LDA & Kernel-PCA**"]},{"cell_type":"markdown","metadata":{"id":"s0k1JNJSgl6F"},"source":["All of these dimensionality reduction techniques are used to maximize the variance in the data but these all three have a different characteristic and approach of working. \n"]},{"cell_type":"markdown","metadata":{"id":"zM_VHZLtgyLi"},"source":["##**The difference in Strategy:**\n","\n","The PCA and LDA are applied in dimensionality reduction when we have a linear problem in hand that means there is a linear relationship between input and output variables. On the other hand, the Kernel PCA is applied when we have a nonlinear problem in hand that means there is a nonlinear relationship between input and output variables. So the PCA and LDA can be applied together to see the difference in their result. But the Kernel PCA uses a different dataset and the result will be different from LDA and PCA. Although PCA and LDA work on linear problems, they further have differences. The LDA models the difference between the classes of the data while PCA does not work to find any such difference in classes. In PCA, the factor analysis builds the feature combinations based on differences rather than similarities in LDA. The discriminant analysis as done in LDA is different from the factor analysis done in PCA where eigenvalues, eigenvectors and covariance matrix are used."]},{"cell_type":"markdown","metadata":{"id":"jUW3aO5Cg28H"},"source":["##**The difference in Results:**\n","\n","As we have seen in the above practical implementations, the results of classification by the logistic regression model after PCA and LDA are almost similar. The main reason for this similarity in the result is that we have used the same datasets in these two implementations. Because there is a linear relationship between input and output variables. The task was to reduce the number of input features. Both dimensionality reduction techniques are similar but they both have a different strategy and different algorithms. On the other hand, a different dataset was used with Kernel PCA because it is used when we have a nonlinear relationship between input and output variables. The result of classification by the logistic regression model re different when we have used Kernel PCA for dimensionality reduction."]}]}