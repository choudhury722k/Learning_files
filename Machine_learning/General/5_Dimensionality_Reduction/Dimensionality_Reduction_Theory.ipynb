{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dimensionality_Reduction_Theory.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMDe673mhZxMRyEzIczkKe0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bIRyTBkKSY4o"},"source":["# **Dimensionality Reduction**"]},{"cell_type":"markdown","metadata":{"id":"DNOdTJ4fSm9b"},"source":["Real-world data may not be as simple as predicting a person’s salary against his or her experience. While this is just one example, there can be so many factors that may affect an employee’s salary. Real-world data is much more complex and therefore identifying and predicting a dependent factor against so many independent factors can reduce the probability of getting a correct prediction. That is why it is important to identify strong independent variables. Dimensionality Reduction is a technique that allows us to understand the independent variables and their variance thus helping to identify a minimum number of independent variables that has the highest variance with respect to the dependent variables.\n","\n","Dimensionality reduction is an important approach in machine learning. A large number of features available in the dataset may result in overfitting of the learning model. To identify the set of significant features and to reduce the dimension of the dataset, there are three popular dimensionality reduction techniques that are used. In the following series, we will discuss the practical implementation of these three dimensionality reduction techniques:-\n","\n","> * Principal Component Analysis (PCA)\n","> * Linear Discriminant Analysis (LDA), and\n","> * Kernel PCA (KPCA)"]},{"cell_type":"code","metadata":{"id":"6vpPtmyLSVln"},"source":[""],"execution_count":null,"outputs":[]}]}