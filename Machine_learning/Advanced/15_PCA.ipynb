{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnNZVTTTRZGa"
   },
   "source": [
    "# **Principal Component Analysis(PCA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W00kLGujU1KB"
   },
   "source": [
    "In this simple tutorial, we will learn how to implement a dimensionality reduction technique called  Principal Component Analysis (PCA) that helps to reduce the number to independent variables in a problem by identifying Principle Components.We will take a step by step approach to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Cd2X5oU2eI"
   },
   "source": [
    "## **Dataset**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZgJiMaRVH6R"
   },
   "source": [
    "The dataset can be downloaded from the following [link](https://archive.ics.uci.edu/ml/datasets/wine). The dataset gives the details of breast cancer patients. It has 32 features with 569 rows.\n",
    "\n",
    "Letâ€™s get started.Import all the libraries required for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels scikit-image --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSqPfHauMs3x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muJdQOBnVSQT"
   },
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qahT_EW0VWw5"
   },
   "outputs": [],
   "source": [
    "#2. Import the dataset\n",
    "dataset = pd.read_csv('Wine.csv', header=None)\n",
    "\n",
    "\n",
    "dataset.columns = [  'name'\n",
    "                 ,'alcohol'\n",
    "             \t,'malicAcid'\n",
    "             \t,'ash'\n",
    "            \t,'ashalcalinity'\n",
    "             \t,'magnesium'\n",
    "            \t,'totalPhenols'\n",
    "             \t,'flavanoids'\n",
    "             \t,'nonFlavanoidPhenols'\n",
    "             \t,'proanthocyanins'\n",
    "            \t,'colorIntensity'\n",
    "             \t,'hue'\n",
    "             \t,'od280_od315'\n",
    "             \t,'proline'\n",
    "                ]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhjv-9xmWcH6"
   },
   "source": [
    "We need to store the independent and dependent variables by using the iloc method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTG9UyjqWc0c"
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:].values\n",
    "y = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9d11D0bWfsf"
   },
   "source": [
    "Split the training and testing data in the 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Npcs7fdaWgmT"
   },
   "outputs": [],
   "source": [
    "#3. Split the dataset into Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX6oookZWi2P"
   },
   "source": [
    "## **PCA Standardization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejmNjvRbWovE"
   },
   "source": [
    "PCA can only be applied to numerical data. So,it is important to convert all the data into numerical format. We need to standardize data for converting features of different units to the same unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2ljxrK5Wink"
   },
   "outputs": [],
   "source": [
    "#4. Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l133CmpBWr1F"
   },
   "source": [
    "## **Covariance Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T63gFMcWu00"
   },
   "source": [
    "Based on standardized data we will build the covariance matrix. It gives the variance between each feature in our original dataset. The negative value in the result below represents are inversely dependent on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0tgBtRTWilf"
   },
   "outputs": [],
   "source": [
    "mean_vec=np.mean(X_train,axis=0)\n",
    "cov_mat=(X_train-mean_vec).T.dot((X_train-mean_vec))/(X_train.shape[0]-1)\n",
    "mean_vect=np.mean(X_test,axis=0)\n",
    "cov_matt=(X_test-mean_vec).T.dot((X_test-mean_vec))/(X_test.shape[0]-1)\n",
    "print(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqU2w2ktWyJt"
   },
   "source": [
    "## **Eigen Decomposition on Covariance Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNlyHc_QW1gF"
   },
   "source": [
    "Each eigenvector will have an eigenvalue and sum of the eigenvalues represent the variance in the dataset. We can get the location of maximum variance by calculating eigenvalue. The eigenvector with lowest eigenvalue will give the lowest amount of variation in the dataset. These values need to be dropped off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drPTmKzfWijK"
   },
   "outputs": [],
   "source": [
    "cov_mat=np.cov(X_train.T)\n",
    "eig_vals,eig_vecs=np.linalg.eig(cov_mat)\n",
    "cov_matt=np.cov(X_test.T)\n",
    "eig_vals,eig_vecs=np.linalg.eig(cov_mat)\n",
    "print(eig_vals)\n",
    "print(eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSHPfqlzW5OS"
   },
   "source": [
    "We need to specify how many components we want to keep. The result gives a reduction of dimension from 13 to 2 features. The first and second PCA will capture the most variance in the original dataset\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLFt2KfeWigi"
   },
   "outputs": [],
   "source": [
    "#5. Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVgeSL_EW-Dz"
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QXIIX5yXBP3"
   },
   "source": [
    "In this matrix array, each column represents the original data, and each row represents a PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB-kzjx_XKBu"
   },
   "source": [
    "## **Fitting Logistic Regression To the training set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9kMNVGuXQCu"
   },
   "source": [
    "As we are solving a classification problem, we can use the Logistic Regression for model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihE9JhV8XAgV"
   },
   "outputs": [],
   "source": [
    "#6. Fit the Logistic Regression to the Training set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwvZmsg5dCaB"
   },
   "source": [
    "## **Predict the test Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t63TEMIcdGCZ"
   },
   "outputs": [],
   "source": [
    "#7. Predict the Test set results\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G250juP3XqQD"
   },
   "source": [
    "## **Evaluating the Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NZsaXIyXtTi"
   },
   "source": [
    "For classification tasks, we will use a confusion matrix to check the accuracy of our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC0UiBqzXt5k"
   },
   "outputs": [],
   "source": [
    "\n",
    "#8. Make the Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taE3kRvVXw4C"
   },
   "source": [
    "## **Plot the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-0uRUmIX1zk"
   },
   "outputs": [],
   "source": [
    "#9. Visualize the Training set results\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSFfxqj7dWL6"
   },
   "source": [
    "## **Plot the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBDfo5GadaLy"
   },
   "outputs": [],
   "source": [
    "#10.Visualize the Test set results\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ekl2mveZ7bz"
   },
   "source": [
    "##**Important things to note:**\n",
    "\n",
    "> * PCA will take all the original training set variables and decompose them in a manner to make a new set of variables with high explained variance.\n",
    "> * Principal component analysis involves extracting linear composites of observed variables.\n",
    "> * PCA can be used to determine what amount of variability the independent variables can explain for the dependent variable and cannot be used to see whIch independent variables are more important for predictio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJlAo65ERmJj"
   },
   "source": [
    "#**Related Articles:**\n",
    "\n",
    "> * [PCA in Python](https://analyticsindiamag.com/principal-component-analysis-in-python/)\n",
    "> * [Comparing PCA, LDA and PCA-kernel](https://analyticsindiamag.com/practical-approach-to-dimensionality-reduction-using-pca-lda-and-kernel-pca/)\n",
    "> * [Mathematical Practical Approach to PCA](https://analyticsindiamag.com/principal-component-analysis-on-matrix-using-python/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOn2Q9h74ZIFT6hfFboTpkS",
   "collapsed_sections": [],
   "name": "1_PCA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
