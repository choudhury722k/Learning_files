{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wzMbWYwPxEV"
   },
   "source": [
    "# **Optimizers in Tensorflow Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nSv-SEBP1X7"
   },
   "source": [
    "Optimizers are the expanded class, which includes the method to train your machine/deep learning model. Right optimizers are necessary for your model as they improve training speed and performance, Now there are many optimizers algorithms we have in PyTorch and TensorFlow library but today we will be discussing how to initiate TensorFlow Keras optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_1_9xsaR-a4"
   },
   "source": [
    "## **Now how the loss functions and optimizers are related?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxyLUGN2SAqg"
   },
   "source": [
    "During the training of the model, we tune the parameters(also known as hyperparameter tuning) and weights to minimize the loss and try to make our prediction accuracy as correct as possible. Now to change these parameters the optimizer’s role came in, which ties the model parameters with the loss function by updating the model in response to the loss function output. Simply optimizers shape the model into its most accurate form by playing with model weights. The loss function just tells the optimizer when it’s moving in the right or wrong direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XroSVP3dRQMI"
   },
   "source": [
    "TensorFlow mainly supports 9 optimizer classes, consisting of algorithms like Adadelta, FTRL, NAdam, Adadelta, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwgvrYohRVHJ"
   },
   "source": [
    "> * **Adadelta**: Optimizer that implements the Adadelta algorithm.\n",
    "> * **Adagrad**: Optimizer that implements the Adagrad algorithm.\n",
    "> * **Adam**: Optimizer that implements the Adam algorithm.\n",
    "> * **Adamax**: Optimizer that implements the Adamax algorithm.\n",
    "> * **Ftrl**: Optimizer that implements the FTRL algorithm.\n",
    "> * **Nadam**: Optimizer that implements the NAdam algorithm.\n",
    "> * **Optimizer class**: Base class for Keras optimizers.\n",
    "> * **RMSprop**: Optimizer that implements the RMSprop algorithm.\n",
    "> * **SGD**: Gradient descent (with momentum) optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvjSXzG9SP1I"
   },
   "source": [
    "You can refer this article [Optimizers in Tensorflow Keras](https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/) for the theoretical aspect of Optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DgCJUvNSOfo"
   },
   "source": [
    "## **Initialize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJZfPis-ShCw"
   },
   "source": [
    "For initialization you can simply use google colab or for implementation in a local machine you can download anaconda that integrates all the major data science pages into one. Use below import command to initialize tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels keras tensorflow --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN9cqYVmNkpo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5dpg-5XSw5A"
   },
   "source": [
    "### **AdaGrad Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhzbLoNwS2D6"
   },
   "source": [
    "Adagrad adapts the learning rate specifically with individual features: it means that some of the weights in your dataset have different learning rates than others. It always works best in a sparse dataset where a lot of inputs are missing. In TensorFlow, you can call the optimizer using the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIVrMgarS7fF"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.001,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name=\"Adagrad\",\n",
    "    **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BunNvaFS_B-"
   },
   "source": [
    "It is a parameter specific learning rate, adapts with how frequently a parameter gets updated during training. Parameters we pass with these optimizers are learning_rate, initial_accumulator_value, epsilon, name, and **kwargs you can read more about them at [Keras documentation](https://keras.io/api/optimizers/adagrad/) or [TensorFlow docs](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cybG0rAzTLKo"
   },
   "source": [
    "### **RMSprop Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM-WebNLTZc4"
   },
   "source": [
    "It is an exclusive version of Adagrad developed by Geoffrey Hinton( [learn more](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)), now the thinking behind this optimizer was pretty straight forward: instead of letting all of the gradients accumulate for momentum, it only accumulates gradients in a specific fix window. It is exactly like Adaprop(an updated version of Adagrad with some improvement), you can call this in the TensorFlow framework using the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH3hMCo9TmkK"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "rho=0.9, momentum=0.0, \n",
    "epsilon=1e-07, \n",
    "centered=False,\n",
    "    name='RMSprop', **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZQ4BA3aTpR3"
   },
   "source": [
    "Learn more about RMSprop [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMxj1y33T7HL"
   },
   "source": [
    "### **Adadelta(adaptive delta) Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piRq0GPsT62G"
   },
   "source": [
    "Now like the RMSprop optimizer, Adadelta(Read paper: [Zeiler, 2012](https://arxiv.org/abs/1212.5701)) is another more improved optimization algorithm, here delta refers to the difference between the current weight and the newly updated weight. Adadelta removed the use of the learning rate parameter completely and replaced it with an exponential moving average of squared deltas. You can call it in your machine learning project using the below command with basic parameters like epsilon, learning_rate, rho, and **kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nseS2SmUWhv"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta',\n",
    "    **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qm7-eDeYUgqE"
   },
   "source": [
    "### **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pT02niSUlNn"
   },
   "source": [
    "Adam stands for adaptive moment estimation, which is another way of using past gradients to calculate current gradients, for the deep mathematical explanation you can read its official paper([Kingma & Ba, 2014](https://arxiv.org/abs/1412.6980)) here, Adam utilizes the concept of momentum by adding fractions of previous gradients to the current one, it is practically accepted in many projects during training neural nets.\n",
    "\n",
    "You can call it using Tensorflow by leveraging the below commands into your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaEn_f7JUrVv"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam', **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nulmzgAfUqst"
   },
   "source": [
    "Here is the standalone usage for the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdVWL_9zUtwj"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "var1 = tf.Variable(10.0)\n",
    "loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "step_count = opt.minimize(loss, [var1]).numpy()\n",
    "# The first step is `-learning_rate*sign(grad)`\n",
    "var1.numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzfcIpk1Uzb6"
   },
   "source": [
    "###**AdaMax Optimizer Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_seetcqVZal"
   },
   "source": [
    "As the name suggests AdaMax is an adaption of Adam optimizer, by the same researchers who wrote the Adam algorithm, you can read about AdaMax([Kingma & Ba, 2015](https://arxiv.org/abs/1412.6980)) [here](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJqJberVVppc"
   },
   "source": [
    "You can call it using below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD_0DhhcVrwK"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax', **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY-fj7x_VvjY"
   },
   "source": [
    "It is a variant of Adam based on the infinity norm. Sometimes it is considered superior to Adam, especially in models with embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBe_Q2hBV0db"
   },
   "source": [
    "### **NAdam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5DEhu1VWnTu"
   },
   "source": [
    "NAdam optimizer is an acronym for Nesterov and Adam optimizer. Its official research paper was published in 2015 [here](http://cs229.stanford.edu/proj2015/054_report.pdf), now this Nesterov component is way more efficient than its previous implementations. Nadam used Nesterov to update the gradient. You can call NAdam optimizer class during training your model in Tensorflow by leveraging the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "we-AM6tPW7Ky"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Nadam', **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqwlc-1-W_ek"
   },
   "source": [
    "###**Ftrl Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er9AMIfHWnJe"
   },
   "source": [
    "According to algorithm 1 of the [research paper by google](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf), This version has support for both online L2 (the L2 penalty given in the paper above) and shrinkage-type L2 (which is the addition of an L2 penalty to the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s9ofwiiVvIN"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.001,\n",
    "    learning_rate_power=-0.5,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.0,\n",
    "    l2_regularization_strength=0.0,\n",
    "    name=\"Ftrl\",\n",
    "    l2_shrinkage_regularization_strength=0.0,\n",
    "    beta=0.0,\n",
    "    **kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KON64IUpXrW5"
   },
   "source": [
    "###**SGD Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQLhEF-wXw-j"
   },
   "source": [
    "Stochastic gradient descent(SGD) optimization algorithm in contrast performs a parameter update for each training example as given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRpLzthpX24z"
   },
   "source": [
    "SGD performs redundant computations for bigger datasets, as it recomputes gradients for the same example before each parameter update. It performs frequent updates with a high variance that cause the objective function to fluctuate heavily as as shown in below image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EY_x2uNX1cY"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pxZ-jVVX6Yo"
   },
   "source": [
    "You can call the SGD optimizer using below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3JOnDF6X7Dt"
   },
   "outputs": [],
   "source": [
    "tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "momentum=0.0,\n",
    "nesterov=False, \n",
    "name=\"SGD\", \n",
    "**kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbnOmHAHX9Qo"
   },
   "source": [
    "Now for starter you can implement a standalone example like this to see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AE8-wSbYAqL"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "var = tf.Variable(1.0)\n",
    "loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
    "step_count = opt.minimize(loss, [var]).numpy()\n",
    "## Step is `- learning_rate * grad` \n",
    "var.numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD68lYDPkZTw"
   },
   "source": [
    "# **Related Articles --**\n",
    "\n",
    "> * [Optimizers in Tensorflow Keras](https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/)\n",
    "> * [Loss functions in Tensorflow Keras](https://analyticsindiamag.com/ultimate-guide-to-loss-functions-in-tensorflow-keras-api-with-python-implementation/)\n",
    "> * [Deep Learning Frameworks](https://analyticsindiamag.com/deep-learning-frameworks/)\n",
    "> * [Types of Activation Functions](https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/)\n",
    "> * [Maths for Deep Learning](https://analyticsindiamag.com/beginners-guide-neural-network-math-python/)\n",
    "> * [Deep Learning Using Tensorflow Keras](https://analyticsindiamag.com/deep-learning-using-tensorflow-keras/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPV2/tjoDchZl6zFD4GioFa",
   "collapsed_sections": [],
   "name": "2_Optimizers_in_Tensorflow Keras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
