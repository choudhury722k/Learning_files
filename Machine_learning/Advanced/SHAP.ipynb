{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNQ3bDX_bxFI"
   },
   "source": [
    "# **SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib49kTKzemzi"
   },
   "source": [
    "SHAP is the acronym for SHapley Additive exPlanations derived originally from Shapley values introduced by Lloyd Shapley as a solution concept for cooperative game theory in 1951. SHAP works well with any kind of machine learning or deep learning model. ‘TreeExplainer’ is a fast and accurate algorithm used in all kinds of tree-based models such as random forests, xgboost, lightgbm, and decision trees. ‘DeepExplainer’ is an approximate algorithm used in deep neural networks. ‘KernelExplainer’ algorithm is, in general, applicable to any machine learning regression model. SHAP prefers different visualizations to demonstrate the feature importance and the way features contributed in predictions. In this article we discuss various SHAP visualization techniques with a decision tree classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCm5bQEpeoQQ"
   },
   "source": [
    "To read about it more, please refer [this](https://analyticsindiamag.com/hands-on-guide-to-interpret-machine-learning-with-shap/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPEGJUR-euB5"
   },
   "source": [
    "## **Code Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrNt_KfR20Us"
   },
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MQ31gCSe-nh"
   },
   "source": [
    "To install the open source library in python environment,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy sklearn statsmodels --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JIVEAL18rNd"
   },
   "outputs": [],
   "source": [
    "!python -m pip install shap graphviz --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaor82XcfA8X"
   },
   "source": [
    "Import necessary libraries and an in-built dataset for SHAP analysis. Here ‘Breast Cancer Data’ from sklearn datasets is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2aW_nGU2zki"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap \n",
    "import graphviz\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGdN3IB_bwD6"
   },
   "source": [
    "### Load data and train a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oB1sko3u2o0C"
   },
   "outputs": [],
   "source": [
    "# load the famous breast cancer data from sklearn inbuilt datasets\n",
    "# a supervised binary classification problem\n",
    "data = load_breast_cancer()\n",
    "# define predictors as pandas dataframe\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "# define target as pandas series\n",
    "y = pd.Series(data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBXV_oAafIEn"
   },
   "source": [
    "Save predictors and targets as in the variables X and y respectively. Then split the dataset into train and test sets in 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1VXmvDPHn3Z"
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# sample first few rows and few columns\n",
    "X_train.iloc[:5, :12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U47ESbAPfKPA"
   },
   "source": [
    "Construct a decision tree classifier model and train it with the training set of data. To know how the model classifies given data, we can use graphviz tree visualizer to plot the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlxxgirjIvVv"
   },
   "outputs": [],
   "source": [
    "# develop a decision tree model\n",
    "model = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "#model = RandomForestClassifier(random_state=1, max_depth=5, n_estimators=50)\n",
    "\n",
    "# train the model data\n",
    "model.fit(X_train, y_train)\n",
    "# Visualize how model classified the entire data\n",
    "tree_graph = export_graphviz(model, out_file=None, feature_names = data['feature_names'], rounded=True, filled=True)\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q1ijkElbcyN"
   },
   "source": [
    "### SHAP Force Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGw3futRfPq6"
   },
   "source": [
    "Develop a tree-based SHAP explainer and calculate the shap values. Shap values are arrays of a length corresponding to the number of classes in target. Here the problem is binary classification, and thus shap values have two arrays corresponding to either class.\n",
    "\n",
    "Shap values are floating-point numbers corresponding to data in each row corresponding to each feature. Shap value represents the contribution of that particular data point in predicting the outputs. If the shap value is much closer to zero, we can say that the data point contributes very little to predictions. If the shap value is a strong positive or strong negative value, we can say that the data point greatly contributes to predicting the positive or negative class.\n",
    "\n",
    "Force plots are suitable for row-wise SHAP analysis. It takes in a single row and shows in a rank order how each of the features contributed to the prediction. Wider a feature’s block, more the contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8tMlL2GO_P3"
   },
   "outputs": [],
   "source": [
    "# Initialize JavaScript visualizations in notebook environment\n",
    "shap.initjs()\n",
    "# Define a tree explainer for the built model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "# obtain shap values for the test data\n",
    "# we will use these shap values in all of our visualizations hereafter\n",
    "shap_values = explainer.shap_values(X_test.iloc[0])\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1hvNMebdrYx"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UCrJVjy1JAB"
   },
   "source": [
    "### SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUc49Dk2fThU"
   },
   "source": [
    "Summary plots are easy-to-read visualizations which bring the whole data to a single plot. All of the features are listed in y-axis in the rank order, the top one being the most contributor to the predictions and the bottom one being the least or zero-contributor. Shap values are provided in the x-axis. As we discussed already, a value of zero represents no contribution whereas contributions increase as the shap value moves away from zero. Each circular dot in the plot represents a single data point. Color of the dot denotes the value of that corresponding feature. It can be observed that the feature ‘worst perimeter’ contributes greatly to the model’s prediction with low values deciding one class and higher values deciding the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a07Wsu_exPM2"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_dy1Q4HfXOv"
   },
   "source": [
    "Summary plot can also be visualized as a bar plot for quick reading with minimum details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHRZe1pWxfs-"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], X_test, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4QCjpwE1NBC"
   },
   "source": [
    "### SHAP Dependence Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slMBgVAMfbSD"
   },
   "source": [
    "Dependence plots can be of great use while analyzing feature importance and doing feature selection. It makes one-versus-one plot against two features by plotting shap values of one feature and coloring the dots with respect to another interactive feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCXJuMTPyUtx"
   },
   "outputs": [],
   "source": [
    "# we use whole of X data from more points on plot\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap.dependence_plot('worst perimeter', shap_values[1], X, interaction_index='worst concave points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw8R7U0yffxH"
   },
   "source": [
    "If the interactive feature is not provided by the user, SHAP determines a suitable feature on its own and uses that as the interactive feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vzqGhKc1Sf7"
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('worst concave points' , shap_values[1], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzos8b5G2ln-"
   },
   "source": [
    "### SHAP Decision Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW9XXPPEfjHw"
   },
   "source": [
    "Finally we discuss the decision plot. As like the summary plot, it gives an overall picture of contribution to prediction. From bottom to top of the decision plot, shap values are cumulatively added to the base value of the model in determining the output values. It can be observed that certain strings colored in blue resulted in final class value 0 and the remaining strings colored in red resulted in final class value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y1JIvpBziM3"
   },
   "outputs": [],
   "source": [
    "shap.decision_plot(explainer.expected_value[1], shap_values[1], X)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOp9i5ZBUWH7/KeTD2ndFk+",
   "collapsed_sections": [],
   "name": "1_SHAP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
