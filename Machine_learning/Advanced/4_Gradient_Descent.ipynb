{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7E7GUm4BF6u"
   },
   "source": [
    "# **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3ulgA5gCfqy"
   },
   "source": [
    "##**What are Optimizers?**\n",
    "\n",
    "Optimizers are the ones that are used to reduce the loss in the model or to reduce the error rate made by deep learning models. The less the error rate better will be the performance of the model. There are several different types of optimizers that are used while compiling the models. Some of them include gradient descent, stochastic gradient descent, adam, etc. All these are used to optimize the performance of the model. They are commonly defined after defining the model structure. Refer to the below code to understand more about defining these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh-_NptzCo7c"
   },
   "source": [
    "##**What is Gradient Descent? How does it work?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjITyfQ7Ct43"
   },
   "source": [
    "It is the most preferred optimizer that is used to optimize a deep learning model. It uses optimization algorithms to reduce the error and find the minimum values for a function. Gradient descent makes use of derivatives to reach the minima of a function. Also, there are steps that are taken to reach the minimum point which is set by defining the learning rate. It decides how many steps to take to reach the minima. If we define a big value to the learning rate we may exceed the minima of the function whereas if we define it to be very small then it would consume much time to reach the target. There can be chances that gradient descent will miss out on the target if the learning rate is very high. \n",
    "\n",
    "The role of derivatives in optimization algorithms is to decide whether to increase or decrease the weights resulting in increasing or decreasing the loss function or cost function. We cannot train a neural network without defining the optimizer and loss functions. They are the mandatory parameters that need to be set while compiling a deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ln1yNYgDjp1"
   },
   "source": [
    "## **How to implement Gradient Descent in python?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlv9bOZ0DtCl"
   },
   "source": [
    "Now we will see how gradient descent can be implemented in python. We will start by defining the required library first that would be used for numerical calculation and for plotting the graphs. Refer to the below code for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZwZRz0sAXwzX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Now we will define a function f as a quadratic function and function to compute its gradient. Refer to the below code for the same.\n",
    "def function(x,a): \n",
    "    f = a[2]*x*x + a[1]*x + a[0] \n",
    "    return f\n",
    "def grad(x,a): \n",
    "    g = 2*a[2]*x + a[1]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tib6eDzAFnqL"
   },
   "source": [
    "Now we will plot this function before we compute its minima. Use the below code to do the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUNRXobfFoUL"
   },
   "outputs": [],
   "source": [
    "x = np.array([-3,-2,-1,0,1,2,3,4,5,6])\n",
    "a = np.array([-3, -2, 3]) \n",
    "f = function(x,a)\n",
    "plt.scatter(x,f)\n",
    "plt.plot(x,f)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('f(X)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2PpFxo_F3K8"
   },
   "source": [
    "We have values on the X-axis and f(x) on the y-axis. Now letâ€™s define how to use gradient descent to find the minimum. Use the below code for the same. We will first define the starting point, learning rate, and the parameter to stop it like iterations or if the value does not change then it should stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sDR_bD2F36o"
   },
   "outputs": [],
   "source": [
    "x = 8 \n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "change = 1e-5\n",
    "\n",
    "max_iteration = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxJ5aM1kF-7H"
   },
   "source": [
    "We have defined X_series the variable to check how the value of x is getting changed. Then in the loop, we have defined the function f at any point(x, a) followed by computing its gradient and then getting the changed values of x which gets computed by subtracting the original value of x from the product of the learning rate and gradient. Then we will define the condition to stop the loop by making use of maximum iteration and change that was previously defined. At last, we are plotting the values. Refer to the below code for the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnRPzTwBGEW9"
   },
   "outputs": [],
   "source": [
    "series = [x]\n",
    "iterations = 1\n",
    "while True:\n",
    "    f = function(x,a)\n",
    "    g = grad(x,a)\n",
    "    new_x = x - lr * g\n",
    "    if np.sum(abs(new_x - x)) < change:\n",
    "            break\n",
    "            \n",
    "            if iterations > max_iteration:\n",
    "              break\n",
    "    if iterations % (max_iteration/10) == 0:\n",
    "            plt.scatter(x, f, marker='*')\n",
    "            plt.plot(x, f)\n",
    "            plt.xlabel('X')\n",
    "            plt.ylabel('f(X)')\n",
    "    iterations += 1\n",
    "    x = new_x\n",
    "    series = np.concatenate((series,[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPV4BAPFGiuv"
   },
   "source": [
    "Now let us see the minimum value of X after iterations. We will check this by printing the min value of the series we defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpoChAsaGkN6"
   },
   "outputs": [],
   "source": [
    "print(series.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6ePtIsvA9T4"
   },
   "source": [
    "Please check the article [here](https://analyticsindiamag.com/gradient-descent-everything-you-need-to-know-with-implementation-in-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agZjsPeZBFEY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMycOZhUCn/QVQK11oBKrmo",
   "collapsed_sections": [],
   "name": "4_Gradient_Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
