{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r0iMPDYi65n"
   },
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPX9Ka7ckVDA"
   },
   "source": [
    "Data cleaning is one of the most hectic and time-consuming tasks in Data Science. There is no easy template that facilitates the cleaning of data as each data set is unique in its own way consisting of noises that need to be carefully filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV2_pAK-kanW"
   },
   "source": [
    "Exploratory Data Analysis or EDA is the first and foremost of all tasks that a dataset goes through. EDA lets us understand the data and thus helping us to prepare it for the upcoming tasks.\n",
    "\n",
    "Some of the key steps in EDA are identifying the features, a number of observations, checking for null values or empty cells etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUl9fUekke0c"
   },
   "source": [
    "## **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPwr56CZXcrQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_set = pd.read_excel(\"EDA_Data_Train.xlsx\")\n",
    "test_set = pd.read_excel(\"EDA_Data_Test.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkZ7LUQhovz7"
   },
   "source": [
    "We now have two data frames, one consisting of the data to be trained and the other for predicting the target value which in this case is the price of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ORY1V6CzvLq"
   },
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K2s6EpXzxHa"
   },
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwoDLOexo8Ka"
   },
   "source": [
    "## **Identifying the number of features or columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrSn1TDyo68c"
   },
   "outputs": [],
   "source": [
    "#checking the number of features in the Datasets\n",
    "print(\"\\n\\nNumber of features in the datasets :\\n\",'#' * 40)\n",
    "print(\"\\nTraining Set : \\n\",'-' * 20, len(training_set.columns))\n",
    "print(\"\\nTest Set : \\n\",'-' * 20,len(test_set.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKAXd7YxpCP0"
   },
   "source": [
    "## **Identifying the features or columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozWBKOS3pACL"
   },
   "outputs": [],
   "source": [
    "#checking the features in the Datasets\n",
    "print(\"\\n\\nFeatures in the datasets :\\n\",'#' * 40)\n",
    "print(\"\\nTraining Set : \\n\",'-' * 20, list(training_set.columns))\n",
    "print(\"\\nTest Set : \\n\",'-' * 20,list(test_set.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J4-3goBs-uT"
   },
   "source": [
    "## **Identifying the data types of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aj97_bIms-Rx"
   },
   "outputs": [],
   "source": [
    "#checking the data types of features\n",
    "print(\"\\n\\nDatatypes of features in the datasets :\\n\",'#' * 40)\n",
    "print(\"\\nTraining Set : \\n\",'-' * 20,\"\\n\", training_set.dtypes)\n",
    "print(\"\\nTest Set : \\n\",'-' * 20,\"\\n\",test_set.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viAbcjAWtHI8"
   },
   "source": [
    "## **Identifying the number of observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJJ4g_mntGPx"
   },
   "outputs": [],
   "source": [
    "#checking the number of rows\n",
    "print(\"\\n\\nNumber of observations in the datasets :\\n\",'#' * 40)\n",
    "print(\"\\nTraining Set : \\n\",'-' * 20,len(training_set))\n",
    "print(\"\\nTest Set : \\n\",'-' * 20,len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksppCbULtNSv"
   },
   "source": [
    "## **Checking if the dataset has empty cells or samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2bH6TBPtLn5"
   },
   "outputs": [],
   "source": [
    "#checking for NaNs or empty cells\n",
    "print(\"\\n\\nEmpty cells or Nans in the datasets :\\n\",'#' * 40)\n",
    "print(\"\\nTraining Set : \\n\",'-' * 20,training_set.isnull().values.any())\n",
    "print(\"\\nTest Set : \\n\",'-' * 20,test_set.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07SNcb3HtStW"
   },
   "source": [
    "## **Identifying the number of empty cells by features or columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLwqdlNItSBA"
   },
   "outputs": [],
   "source": [
    "#checking for NaNs or empty cells by features\n",
    "print(\"\\n\\nNumber of empty cells or Nans in the datasets :\\n\",'#' * 40)\n",
    "print(\"\\nTraining Set : \\n\",'-' * 20,\"\\n\", training_set.isnull().sum())\n",
    "print(\"\\nTest Set : \\n\",'-' * 20,\"\\n\",test_set.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQWLj57yuzUI"
   },
   "source": [
    "## **Exploring Categorical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JPfBpDku2VO"
   },
   "source": [
    "The below code block explores the categorical features, identifying the unique categories in both the test and training_sets combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmVpgMULtbE7"
   },
   "outputs": [],
   "source": [
    "#combining training set and test set data\n",
    "all_brands = list(training_set.Name) + list(test_set.Name)\n",
    "all_locations = list(training_set.Location) + list(test_set.Location)\n",
    "all_fuel_types = list(training_set.Fuel_Type) + list(test_set.Fuel_Type)\n",
    "all_transmissions = list(training_set.Transmission) + list(test_set.Transmission)\n",
    "all_owner_types = list(training_set.Owner_Type) + list(test_set.Owner_Type)\n",
    "\n",
    "print(\"\\nNumber Of Unique Values In Name : \\n \", len(set(all_brands)))\n",
    "#print(\"\\nThe Unique Values In Name : \\n \", set(all_brands))\n",
    "\n",
    "print(\"\\nNumber Of Unique Values In Location : \\n \", len(set(all_locations)))\n",
    "print(\"\\nThe Unique Values In Location : \\n \", set(all_locations) )\n",
    "\n",
    "print(\"\\nNumber Of Unique Values In Fuel_Type : \\n \", len(set(all_fuel_types)))\n",
    "print(\"\\nThe Unique Values In Fuel_Type : \\n \", set(all_fuel_types) )\n",
    "\n",
    "print(\"\\nNumber Of Unique Values In Transmission : \\n \", len(set(all_transmissions)))\n",
    "print(\"\\nThe Unique Values In Transmission : \\n \", set(all_transmissions) )\n",
    "\n",
    "print(\"\\nNumber Of Unique Values In Owner_Type : \\n \", len(set(all_owner_types)))\n",
    "print(\"\\nThe Unique Values In Owner_Type : \\n \" ,set(all_owner_types) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooNKuYrLu60o"
   },
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L2cbQgSvIZ9"
   },
   "source": [
    "In this stage, we will remove unwanted data or noises from the data set to prepare it for the data preprocessing stage. The goal is to clean the data in such a way that all data can be successfully converted into a numerical type in the preprocessing stage.\n",
    "\n",
    "By performing Exploratory data analysis, we found out that the majority of the features in the data set are objects. These features contain multiple strings of data in which most of them are useless or insignificant for a predictive model. We will traverse through each of those features cleaning one by one for both the training set and the test_set given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37qSEL2zvS5E"
   },
   "source": [
    "### **Feature/Column : Name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghZ3_M01vWmQ"
   },
   "source": [
    "We will start with the column “Name”. By going through the data, we can see that each cell in the column consists of multiple words that provide insights about both the brand and the model of the car. We can thus simplify the dataset by splitting this feature into two different features Brand and Model. We will then replace the “Name ” feature with the 2 derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0n-aat3u6cQ"
   },
   "outputs": [],
   "source": [
    "#\"\"\"Splitting name into 2 features, brand and model\"\"\"\n",
    "\n",
    "#Training Set\n",
    "names = list(training_set.Name)\n",
    "brand = []\n",
    "model = []\n",
    "for i in range(len(names)):\n",
    "   try:\n",
    "       brand.append(names[i].split(\" \")[0].strip())\n",
    "       try:\n",
    "           model.append(\" \".join(names[i].split(\" \")[1:]).strip())\n",
    "       except:\n",
    "           pass\n",
    "   except:\n",
    "       print(\"ERR ! - \", names[i], \"@\" , i)\n",
    "training_set[\"Brand\"] =  brand\n",
    "training_set[\"Model\"] = model\n",
    "training_set.drop(labels = ['Name'], axis = 1, inplace = True)\n",
    "\n",
    "#Test Set\n",
    "names = list(test_set.Name)\n",
    "brand = []\n",
    "model = []\n",
    "for i in range(len(names)):\n",
    "   try:\n",
    "       brand.append(names[i].split(\" \")[0].strip())\n",
    "       try:\n",
    "           model.append(\" \".join(names[i].split(\" \")[1:]).strip())\n",
    "       except:\n",
    "           pass\n",
    "   except:\n",
    "       print(\"ERR ! - \", names[i], \"@\" , i)\n",
    "test_set[\"Brand\"] =  brand\n",
    "test_set[\"Model\"] = model\n",
    "test_set.drop(labels = ['Name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIBKd43uvcDw"
   },
   "source": [
    "### **Feature/Column : Mileage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaQRNWbswouZ"
   },
   "source": [
    "In the given dataset, you will find that each of the values in the column ‘Mileage’ , the unit is also appended to the value. The unit makes no sense to the machines or the model. So we will remove it and convert the feature to float type with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bkw50kwrvaCX"
   },
   "outputs": [],
   "source": [
    "#\"\"\" Removing the  texts and converting to integer''\"\"\"\n",
    "\n",
    "# Training Set\n",
    "import numpy as np\n",
    "\n",
    "mileage = list(training_set.Mileage)\n",
    "for i in range(len(mileage)):\n",
    "   try :\n",
    "       mileage[i] = float(mileage[i].split(\" \")[0].strip())\n",
    "   except:\n",
    "       mileage[i] = np.nan\n",
    "training_set['Mileage'] = mileage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC8QKltpwyVF"
   },
   "source": [
    "Repeat the above code block for the test set by replacing all training_set with test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBkYr9HVwrOv"
   },
   "outputs": [],
   "source": [
    "#\"\"\" Removing the  texts and converting to integer''\"\"\"\n",
    "\n",
    "# Test Set\n",
    "import numpy as np\n",
    "\n",
    "mileage = list(test_set.Mileage)\n",
    "for i in range(len(mileage)):\n",
    "   try :\n",
    "       mileage[i] = float(mileage[i].split(\" \")[0].strip())\n",
    "   except:\n",
    "       mileage[i] = np.nan\n",
    "test_set['Mileage'] = mileage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDgmaTXRw40b"
   },
   "source": [
    "### **Feature/Column : Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH-7HgTVw7na"
   },
   "source": [
    "Similar to the column Mileage, Engine column also has the units in its values. We will remove the units and will convert the feature to int type as we can see all the values are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIFDz5X_w2QR"
   },
   "outputs": [],
   "source": [
    "#\"\"\" Removing the  texts and converting to integer''\"\"\"\n",
    "\n",
    "# Training Set\n",
    "engine = list(training_set.Engine)\n",
    "for i in range(len(engine)):\n",
    "   try :\n",
    "       engine[i] = int(engine[i].split(\" \")[0].strip())\n",
    "   except:\n",
    "       engine[i] = np.nan\n",
    "training_set['Engine'] = engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUdjgMfDw_lp"
   },
   "source": [
    "Repeat the above code block for the test set by replacing all training_set with test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b247isq4w-Bf"
   },
   "outputs": [],
   "source": [
    "#\"\"\" Removing the  texts and converting to integer''\"\"\"\n",
    "\n",
    "# Test Set\n",
    "engine = list(test_set.Engine)\n",
    "for i in range(len(engine)):\n",
    "   try :\n",
    "       engine[i] = int(engine[i].split(\" \")[0].strip())\n",
    "   except:\n",
    "       engine[i] = np.nan\n",
    "test_set['Engine'] = engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj0I-I0ixGwp"
   },
   "source": [
    "### **Feature/Column : Power**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMcE6Mc_zAWB"
   },
   "source": [
    "We will do the same for the feature Power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFl2mxTixFK5"
   },
   "outputs": [],
   "source": [
    "#\"\"\" Removing the  texts and converting to integer\"\"\"\n",
    "\n",
    "# Training Set\n",
    "power = list(training_set.Power)\n",
    "for i in range(len(power)):\n",
    "   try :\n",
    "       power[i] = float(power[i].split(\" \")[0].strip())\n",
    "   except:\n",
    "       power[i] = np.nan\n",
    "training_set['Power'] = power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e9j-PO4zEvr"
   },
   "source": [
    "Repeat the above code block for the test set by replacing all training_set with test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE84ZXoBzDk7"
   },
   "outputs": [],
   "source": [
    "#\"\"\" Removing the  texts and converting to integer\"\"\"\n",
    "\n",
    "# Test Set\n",
    "power = list(test_set.Power)\n",
    "for i in range(len(power)):\n",
    "   try :\n",
    "       power[i] = float(power[i].split(\" \")[0].strip())\n",
    "   except:\n",
    "       power[i] = np.nan\n",
    "test_set['Power'] = power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0DE3MO-zMK0"
   },
   "source": [
    "### **Feature/Column : New_Price**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6X-Q9c9zQdC"
   },
   "source": [
    "Since this feature has a huge number of null values compared to the entire dataset, we will choose to remove the feature itself from the datasets. (It is also possible to fill the nulls with zeros or unit values to check its significance in predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HHIQFlczKd2"
   },
   "outputs": [],
   "source": [
    "training_set.drop(labels = ['New_Price'], axis = 1, inplace = True)\n",
    "test_set.drop(labels = ['New_Price'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWyDrGdUzVRu"
   },
   "source": [
    "### **Reordering The Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqtTzHZ0zZsp"
   },
   "source": [
    "We have now cleaned the dataset, Lets reorder the columns and have a look at the new and cleaner dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGrBllJ9zTN_"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Re-ordering the columns\n",
    "training_set = training_set[['Brand', 'Model', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission',\n",
    "      'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats', 'Price']]\n",
    "test_set = test_set[['Brand', 'Model', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission',\n",
    "      'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttXGx9X7zpQQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heCLdjo8zaJn"
   },
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3eGQA3rzm0-"
   },
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F-iIcaCz2U4"
   },
   "source": [
    "Read more about it [here](https://analyticsindiamag.com/tutorial-get-started-with-exploratory-data-analysis-and-data-preprocessing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eNNkjNN0Jcw"
   },
   "source": [
    "#**Pandas Visual Analysis – Way To Speed-Up Data Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3EKLhl818kX"
   },
   "source": [
    "Being an important step in analyzing what data is all about Exploratory Data Analysis generally takes a lot of time because we need to write code for analyzing and visualizing data. What if we can automate this process of visualizing and analyzing data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13x6NqSK2Ff_"
   },
   "source": [
    "Pandas Visual Analysis is an open-source python library which is used to visually analyze the data and that too in just a single line of code. It creates a user interface that can be used to create different plots and graphs taking different attributes. It supports a large variety of graphs and plots, also all the graphs are created using Plotly so that they are highly interactive, visually appealing, and easily downloadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfva3RdQ2KP3"
   },
   "outputs": [],
   "source": [
    "!pip install pandas-visual-analysis --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YbhaaM2aUu"
   },
   "source": [
    "Read more about it, [here](https://analyticsindiamag.com/hands-on-guide-to-pandas-visual-analysis-way-to-speed-up-data-visualization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cLThAUt2aM9"
   },
   "source": [
    "## **Importing Required Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq3ua4h03CPj"
   },
   "source": [
    "For data analysis, we will be importing pandas visual analysis and we will import pandas for loading the dataset we will use. Other than this we will import seaborn to load a dataset defined in seaborn named tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUWAaKfEz9VO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_visual_analysis import VisualAnalysis\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EgZLY413HM8"
   },
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-57CHwXg3mXY"
   },
   "source": [
    "We will explore pandas visual analysis using dataset which we will load from seaborn named tips is a dataset of a restaurant data which contains attributes like ‘total bill’, ‘tip’, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6Xcdwd33FLP"
   },
   "outputs": [],
   "source": [
    "df1= sns.load_dataset('tips')\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkRJB40M4xIN"
   },
   "source": [
    "## **Visual Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPGUe8dJ44DB"
   },
   "source": [
    "This is the final step that will load our data in the form of a Graphical User Interface where we have a variety of graphs and plots defined and we can select different attributes to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp5sQcZl3uwE"
   },
   "outputs": [],
   "source": [
    "VisualAnalysis(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vkvcIuV78Uu"
   },
   "source": [
    "## **Graphical User Interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p5ynzB_8AVC"
   },
   "source": [
    "Here you can see that we have created an interface with different sections to analyze and visualize the dataset we are working on. It is a multivariate dataset still pandas visual analysis created it so easily and effortlessly. Let us see what are the different sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bi8u6Bm8Im8"
   },
   "source": [
    "**1. Statistical Analysis**\n",
    "\n",
    "The first section helps us analyze the statistical properties, we can analyze different metrics like mean, quartiles, median, etc. for all the numerical attributes.\n",
    "Statistical analysis\n",
    "\n",
    "**2. Distribution using Scatter Plot**\n",
    "\n",
    "Using this,  we can analyze the distribution and relationship between two attributes using a scatter plot.\n",
    "Scatter Plot\n",
    "\n",
    "**3. Distribution Using Histogram**\n",
    "\n",
    "In this way, we will analyze the distribution of an attribute using the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGuNN1S88jhs"
   },
   "source": [
    "Read more about it, [here](https://analyticsindiamag.com/hands-on-guide-to-pandas-visual-analysis-way-to-speed-up-data-visualization/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_Exploratory_Data_Analysis_Python.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
